├── LICENSE
├── Makefile           <- Makefile with commands like `make data` or `make train`
├── README.md          <- The top-level README for developers using this project.
├── data
│   ├── cpp            <- C++ code and executables, derived from VDG utils/ngrams/ (https://bitbucket.org/robvanderg/utils/src/master/ngrams/)
│   │   ├── load_ngrams     <- executable to load VdG ngrams
│   │   └── loadfrombin.cc  <- modification of `utils/ngrams/main.cc - compile in same place to produce load_ngrams
│   ├── external       <- Data from third party sources, that is scowl (http://wordlist.aspell.net/) 2020.12.07 for lexicon construction and ngrams and embeddings from VdG (http://www.itu.dk/people/robv/data/monoise/)
│   ├── hpc            <- Output directory for HPC jobs, transferred to local with scp
│   │   └── cv         <- Train and test for 5-fold cross-validation
│   ├── interim        <- Intermediate data that has been transformed (output of load_ngrams to give unigrams and bigrams for twitter and wikipedia)
│   ├── processed      <- The final, canonical data sets for modeling.
│   │   ├── combined.txt     <- concatenation of 2021 train and dev set
│   │   └── lexicon.txt      <- optimal lexicon to use (american 50, variants 1, no contractions)
│   ├── raw            <- The original, immutable data dump. Contains 2015 and 2021 train, dev, test.
│   └── test           <- Example lexica to test lexicon construction
│
├── models             <- Trained and serialized models, model predictions, or model summaries
│
├── notebooks          <- Jupyter notebooks. Naming convention is a number (for ordering),
│                         and a short `-` delimited description, e.g.
│                         `1.0-jqp-initial-data-exploration`.
│
├── reports            <- Generated analysis as HTML, PDF, LaTeX, etc.
│   └── figures        <- Generated graphics and figures to be used in reporting
│
├── requirements.txt   <- The requirements file for reproducing the analysis environment, e.g.
│                         generated with `pip freeze > requirements.txt`
│
├── setup.cfg & pyproject.toml      <- Make this project pip installable with `pip install -e`
│
├── Makefile           <- Coordination of scripts for data processing pipeline
│
├── src                <- Source code for use in this project.
│   └── lexnorm        <- Top level python package (all packages are modules)
│       ├── __init__.py    <- Makes src a Python package
│       │
│       ├── data           <- Package for data handling and generation
│       │
│       ├── generate_extract     <- Package for candidate generation and feature extraction
│       │
│       ├── models         <- Package for model training and candidate prediction using those models
│       │
│       ├── evaluation     <- Package for evaluation
│       │
│       └── visualization  <- Package for exploratory and results oriented visualizations
│
└──tests               <- Pytest unit tests