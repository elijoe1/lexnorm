[24/10/2022]
- Conducted a cursory overview of the resources I had already, to create some initial links to investigate.
[25/10/2022]
- Created research document and started reading resources. Came up with some provisional methods (edit distance, CRFs), features (context, in lexicon), preprocessing (removal of NER, following the annotation guidelines), and post-processing (spellchecking).
[2/11/2022]
- Continued working on research document, reading two useful theses and deciding that one of the classifiers will probably be a CRF, and random forests could be used for both classification and normalisation (through ranking normalisation candidates).
[05/11/2022]
- Did some further reading while doing NLP practical 2. Perhaps will use simple feedforward neural network for one of the classifiers, maybe implemented from scratch (as opposed to Hannah's use of a logistic regression classifier, with only one feature (word embeddings)). Learnt more about 'structured prediction' which predicts the whole structure (sentence) as a whole rather than individual words, and therefore can optimise over whole predicted sentence.
[08/11/2022]
- Further research, reading all relevant 2015 entries. Production of tentative plan for whole project. Models include CRF and random forest, and evaluation includes error reduction rate, precision, and recall.
[23/11/2022]
- More research, especially about decision trees and random forests. Found more resources to look at and looked at quite a lot of wikipedia pages. Think will do random forest and CRF, initially with scikit and default parameters (with cross validation) and then extension can implement random forest from scratch, and can do nested cross validation to produce ensemble of models with different hyper parameters. 
[09/01/2023]
- Reading over previous research. Still think random forest, CRF + theoretical selection for candidate step, and perhaps a single model for candidate generation based on theoretical insights. Ranger could be used for random forests - investigate. Found MFR baseline and ERR evaluation scripts already provided in shared task.
[10/01/2023]
- More research.
[11/01/2023]
- More research and prep (all in one note). Standardised repository structure, made sure automatic formatting and checking implemented with black, added definitions.py to define path to root directory.
[14/01/2023] Started notebook for exploration of datasets, added a script to concatenate given train and dev set.
[16/01/2023] Spent ages figuring out how to fix imports - found best way is to create editable installation of module with pip, using setup tools to configure. Makefile for data processing added and refactoring done.
[19/01/2023] Continue data exploration - almost done here. Looked at rt tokens, wrote functions for contingency and correlation of condition with if normalised per token, investigated for alphanumeric and proper noun tagged.
[20/01/2023] Added github action to run pytest on push. Added tests folder and set up pytest in local machine, updated config files. Investigate correlation with entity tagging, wrote lexicon evaluation function. Start tracking data folder as needed in tests. Added tests and standard docstrings for functions written so far, extracted some notebook code for package functions. Now have written make_train, lexicon modules in data, annotation in models, and condition_normalisation in evaluation. And corresponding tests.
[21-22/01/2023] Banged my head against the wall trying to find appropriate list of internet slang interjections - really inconsistent annotation, tends so include a lot of things that should be normalised e.g. ppl. Ended up pretty much giving up. On the bright side, found the optimal lexicon under current conditions - excluding abbreviations subcategory which is surprising but kind of makes sense.
[24/01/2023] Started writing modules for candidate generation - monoise has surprisingly few. Should add more from other sources, and investigate performance of each module in similar way to monoise/others. Maybe bang out a quick decision tree over everything first though.
[26/01/2023] Wrote initial version of all candidate generation modules.
[30/01/2023] Logic improvements and fixing tests. Candidate generation is extremely slow.
[01/02/2023] Implemented feature extraction (coupled with generation modules - could separate later if necessary. Parallelised and configured HPC so can run generation/extraction on there as intolerably slow otherwise. This works!
[02/02/2023] Pretty much finalise existing feature extraction code
[09/02/2023] Fixed main statements to be used with HPC so cover all of datasets, altered the candidates code to include gold token for all candidates for reference.
[10/02/2023] Implement random forest model with scikit learn, along with training and predicting functions, as well as functions to format candidates data frame correctly for doing so. Balanced is definitely the best option for class weights. Getting About 46 ERR - not fantastic, but certainly a step in the right direction! Improve feature extraction to copy monoise then push ahead!
[16/02/2023] Big refactor to make code structure make more sense and remove ad-hoc code. Works as before.
[17/02/2023] Fixes to feature extraction and candidate generation to make work as expected.
[21/02/2023] Banged my head against the wall hugely about allowing non-lowercase predictions from word embedding and spellcheck modules. Pretty large decrease in performance when this is done, even when adding features to detect these types of candidates - 14 and 33 ERR respectively. Non-lowercase includes things like chinese/arabic alphabets, and numbers - the former will never be correct, the latter could be correct but is very unlikely to be. Other capitalised candidates include proper names and the like, which since there are so many may complicate matters. I therefore have decided to only allow lowercase candidates only to be produced for the time being. I also stopped the spellcheck module from producing non-eligible candidates. With this and the updated feature extraction, achieving ERR of about 50 - at last a slight improvement. In Jupyter figured out how to get corresponding gold from tok_id, and get list of raw, gold pairs that weren't dealt with by candidate generation - can use this to check effect of not generating uppercase candidates. Next steps: add more features e.g. numeric, n-gram, add more trees to forest and investigate other parameters... For evaluation, ablation tests for candidates, features - latter easy, former slightly more complicated.
[22/02/2023] Figured out how to load ngrams from van der Goot - using some c++ and some python, put into makefile for reproducibility. Calculating probability from here pretty easy, although likely pretty slow too. Have to decide how to handle bigrams at beginning/end of sentences. Should be 6 ngram features in total - wiki and twitter unigrams, and bigrams on both sides of candidate (with the words on either side being the original tokens! May have to include this in candidates_from_tweets.
[23-24/02/2023] Fixing n-gram processing.
[01/03/2023] Split predicting into two to allow to drop to LAI if probability of best choice below some threshold based on experiments in notebook, allowing for pretty consistent classifier performance with random forest, which is highly variable based on bagging for each tree - a key part of why previous tests were so variable was that each time I generated candidates data frame in different order because of multiprocessing. 
[05/03/2023] Implemented calculation of recall, precision, F1 and planned out the next two weeks - feeling a little less stressed about the whole thing now. Recall seems like the biggest weakness of the model - have an investigate.
[06/03/2023] Extended qualitative analysis for normalisations, F1, recall, precision for normalisations, non-generated normalisations, analysis of predicted probabilities from model, ablation testing for candidate generation modules, top-N recall, saving of predictions in correct format. Also pretty good idea of what I have to do next.
[07/03/2023] Refactored everything to take loaded data instead of loading in function - means cross validation can generate correct data and pass into relevant functions rather than saving to temporary files. Got higher than MFR for the first time by removing max_depth - but side effect of predicting pretty much only 1 or 0 prob - just luck of which prob 1 selected!
[08/03/2023] Read some relevant things about the 'probabilities' output by RFClassifier - essentially just values for decision function (>0.5 as doing with threshold). Motivating use of logreg - second method! Inherently predicting probs of binary classes. Changed candidate ranking evaluation to deal with variable ranks of tied candidates by using joint ranks - not ideal but crucially doesn't affect top 1 metrics very much as low average number of top ranked candidates (becomes much higher for top 2 and beyond).
[09/03/2023] Implemented K-fold candidate generation, feature extraction, training, prediction, evaluation, and ran HPC to get dataframes that can now use to search for hyperparameters and so on. Next step is to evaluate on concatenated test folds, implement feature ablation testing, and implemented log reg model.