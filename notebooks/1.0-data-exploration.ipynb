{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from lexnorm.data import normEval\n",
    "from lexnorm.data import baseline\n",
    "from lexnorm.definitions import DATA_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "train_raw, train_norm = normEval.loadNormData(os.path.join(DATA_PATH, 'raw/train.norm'))\n",
    "test_raw, test_norm = normEval.loadNormData(os.path.join(DATA_PATH, 'raw/test.norm'))\n",
    "dev_raw, dev_norm = normEval.loadNormData(os.path.join(DATA_PATH, 'raw/dev.norm'))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of tweets: 4917\n",
      "Total number of normed tweets: 4917\n",
      "Unchanged from 2015 dataset.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total number of tweets: {len(train_raw) + len(test_raw) + len(dev_raw)}\")\n",
    "print(f\"Total number of normed tweets: {len(train_norm) + len(test_norm) + len(dev_norm)}\")\n",
    "print(\"Unchanged from 2015 dataset.\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of test set: 1967\n",
      "Unchanged from 2015 dataset. Keep split so can compare with both 2015 and 2021 entries.\n",
      "Size of train set: 2360\n",
      "As described in 2021 task paper.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Size of test set: {len(test_raw)}\")\n",
    "print(\"Unchanged from 2015 dataset. Keep split so can compare with both 2015 and 2021 entries.\")\n",
    "print(f\"Size of train set: {len(train_raw)}\")\n",
    "print(\"As described in 2021 task paper.\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "full_raw = train_raw + dev_raw + test_raw\n",
    "full_norm = train_norm + dev_norm + test_norm"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No length mismatch (as expected).\n"
     ]
    }
   ],
   "source": [
    "for tweet_raw, tweet_norm in zip(full_raw, full_norm):\n",
    "    if len(tweet_raw) != len(tweet_norm):\n",
    "        print(\"Length mismatch!\")\n",
    "print(\"No length mismatch (as expected).\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN\n",
      "Number of raw tokens: 35216\n",
      "Number of normed tokens: 35598\n",
      "Number of 1 to n normalisation raw tokens: 307\n",
      "Percentage of 1 to n: 0.87\n",
      "Number of n to 1 normalisation raw tokens: 13\n",
      "Percentage of n to 1: 0.04\n",
      "Number of normalised raw tokens: 2666\n",
      "Percentage normalised: 7.570422535211268\n",
      "DEV\n",
      "Number of raw tokens: 9169\n",
      "Number of normed tokens: 9282\n",
      "Number of 1 to n normalisation raw tokens: 98\n",
      "Percentage of 1 to n: 1.07\n",
      "Number of n to 1 normalisation raw tokens: 1\n",
      "Percentage of n to 1: 0.01\n",
      "Number of normalised raw tokens: 633\n",
      "Percentage normalised: 6.903697240702367\n",
      "TEST\n",
      "Number of raw tokens: 29421\n",
      "Number of normed tokens: 29738\n",
      "Number of 1 to n normalisation raw tokens: 262\n",
      "Percentage of 1 to n: 0.89\n",
      "Number of n to 1 normalisation raw tokens: 17\n",
      "Percentage of n to 1: 0.06\n",
      "Number of normalised raw tokens: 2324\n",
      "Percentage normalised: 7.899119676421604\n",
      "ALL\n",
      "Number of raw tokens: 73806\n",
      "Number of normed tokens: 74618\n",
      "Number of 1 to n normalisation raw tokens: 667\n",
      "Percentage of 1 to n: 0.90\n",
      "Number of n to 1 normalisation raw tokens: 31\n",
      "Percentage of n to 1: 0.04\n",
      "Number of normalised raw tokens: 5623\n",
      "Percentage normalised: 7.618621792266212\n"
     ]
    }
   ],
   "source": [
    "for name, collection in [(\"TRAIN\", zip(train_raw, train_norm)), (\"DEV\", zip(dev_raw, dev_norm)), (\"TEST\", zip(test_raw, test_norm)), (\"ALL\", zip(full_raw, full_norm))]:\n",
    "    print(name)\n",
    "    one_to_n_count = 0\n",
    "    n_to_one_count = 0\n",
    "    raw_count = 0\n",
    "    norm_count = 0\n",
    "    raw_normalised_count = 0\n",
    "    for tweet_raw, tweet_norm in collection:\n",
    "        for token_raw, token_norm in zip(tweet_raw, tweet_norm):\n",
    "            raw_count += 1\n",
    "            norm_count += len(token_norm.split(\" \"))\n",
    "            if not token_norm:\n",
    "                n_to_one_count += 1\n",
    "            if len(token_norm.split(\" \")) > 1:\n",
    "                one_to_n_count += 1\n",
    "            if token_norm != token_raw:\n",
    "                raw_normalised_count += 1\n",
    "    print(f\"Number of raw tokens: {raw_count}\")\n",
    "    print(f\"Number of normed tokens: {norm_count}\")\n",
    "    print(f\"Number of 1 to n normalisation raw tokens: {one_to_n_count}\")\n",
    "    print(f\"Percentage of 1 to n: {one_to_n_count * 100 / raw_count:.2f}\")\n",
    "    print(f\"Number of n to 1 normalisation raw tokens: {n_to_one_count}\")\n",
    "    print(f\"Percentage of n to 1: {n_to_one_count * 100 / raw_count:.2f}\")\n",
    "    print(f\"Number of normalised raw tokens: {raw_normalised_count}\")\n",
    "    print(f\"Percentage normalised: {raw_normalised_count * 100 / raw_count}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For some reason, the stats in the 2021 task paper are on the train set only (correct in that case).\n",
      "Apart from percentage change - unclear how this is calculated anyway.\n",
      "Note the 1 to n and n to 1 counts are counting the number of raw tokens involved in the respective normalisations.\n",
      "So a 5 to 1 normalisation would produce a count of 4 (number of tokens merged into first token) for 1 to n, and a 1 to 5 normalisation would produce a count of 1 for n to 1.\n",
      "Note as no capitalisation correction for 2015 dataset, in 2021 dataset version EVERYTHING (RAW AND GOLD) IS LOWER CASE\n",
      "2015 task paper has different statistics, one reason for this being lack of capitalisation consideration in 2021\n"
     ]
    }
   ],
   "source": [
    "print(\"For some reason, the stats in the 2021 task paper are on the train set only (correct in that case).\")\n",
    "print(\"Apart from percentage change - unclear how this is calculated anyway.\")\n",
    "print(\"Note the 1 to n and n to 1 counts are counting the number of raw tokens involved in the respective normalisations.\")\n",
    "print(\"So a 5 to 1 normalisation would produce a count of 4 (number of tokens merged into first token) for 1 to n, and a 1 to 5 normalisation would produce a count of 1 for n to 1.\")\n",
    "print(\"Note as no capitalisation correction for 2015 dataset, in 2021 dataset version EVERYTHING (RAW AND GOLD) IS LOWER CASE\")\n",
    "print(\"2015 task paper has different statistics, one reason for this being lack of capitalisation consideration in 2021\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "import json\n",
    "f_train = open(os.path.join(DATA_PATH, \"raw/2015/train_data.json\"))\n",
    "f_test = open(os.path.join(DATA_PATH, \"raw/2015/test_truth.json\"))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "fif_data = json.load(f_train)\n",
    "fif_data += json.load(f_test)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29 raw differences, 1036 norm only differences\n",
      "Most common norm differences: [(('laughing out loud', 'lol'), 465), (('oh my god', 'omg'), 100), (('laughing my ass off', 'lmao'), 96), ((\"i don't know\", 'idk'), 63), (('gonna', 'going to'), 46), (('what the fuck', 'wtf'), 45), (('shaking my head', 'smh'), 40), (('wanna', 'want to'), 33), (('laughing my fucking ass off', 'lmfao'), 26), ((\"i don't care\", 'idc'), 22)]\n",
      "Differences in 2015, 2021 raw due to username anonymization\n",
      "Differences in 2015, 2021 gold due to leaving interjections alone e.g. lol, lmfao, ctfu and normalising gonna and wanna\n",
      "Hence make sure models do too to ensure good performance on 2021 set (maybe dict lookup, hard coding?)\n",
      "Could evaluate on both datasets to compare with submissions from both tasks\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "raw_diff = 0\n",
    "norm_diff = 0\n",
    "norm = Counter()\n",
    "for fif, twe_raw, twe_norm in zip(fif_data, full_raw, full_norm):\n",
    "    fif_raw = [x.lower() for x in fif[\"input\"]]\n",
    "    fif_norm = [x.lower() for x in fif[\"output\"]]\n",
    "    if fif_raw != twe_raw:\n",
    "        raw_diff += 1\n",
    "    elif fif_norm != twe_norm:\n",
    "        norm_diff += 1\n",
    "        norm.update((x, y) for x, y in zip(fif_norm, twe_norm) if x != y)\n",
    "print(f\"{raw_diff} raw differences, {norm_diff} norm only differences\")\n",
    "print(f\"Most common norm differences: {norm.most_common(10)}\")\n",
    "print(\"Differences in 2015, 2021 raw due to username anonymization\")\n",
    "print(\"Differences in 2015, 2021 gold due to leaving interjections alone e.g. lol, lmfao, ctfu and normalising gonna and wanna\")\n",
    "print(\"Hence make sure models do too to ensure good performance on 2021 set (maybe dict lookup, hard coding?)\")\n",
    "print(\"Could evaluate on both datasets to compare with submissions from both tasks\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common normalisation pairs: [(('u', 'you'), 562), (('im', \"i'm\"), 334), (('dont', \"don't\"), 149), (('nigga', 'nigger'), 117), (('niggas', 'niggers'), 93), (('n', 'and'), 89), (('pls', 'please'), 68), (('lil', 'little'), 62), (('ur', 'your'), 54), (('thats', \"that's\"), 54)]\n",
      "Most common normalised raw words: [('u', 569), ('im', 336), ('dont', 149), ('nigga', 117), ('niggas', 94), ('n', 93), ('ur', 74), ('pls', 68), ('lil', 62), ('thats', 54)]\n",
      "Remember this is including the test set - can't use all of this for the normalisation dictionary!\n"
     ]
    }
   ],
   "source": [
    "normalised_pairs = Counter()\n",
    "non_standard_tokens = Counter()\n",
    "\n",
    "for tweet_raw, tweet_norm in zip(full_raw, full_norm):\n",
    "    for token_raw, token_norm in zip(tweet_raw, tweet_norm):\n",
    "        if token_raw != token_norm:\n",
    "            normalised_pairs.update([(token_raw, token_norm)])\n",
    "            non_standard_tokens.update([token_raw])\n",
    "\n",
    "print(f\"Most common normalisation pairs: {normalised_pairs.most_common(10)}\")\n",
    "print(f\"Most common normalised raw words: {non_standard_tokens.most_common(10)}\")\n",
    "print(\"Remember this is including the test set - can't use all of this for the normalisation dictionary!\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline acc.(LAI): 92.10\n",
      "Accuracy:           97.23\n",
      "ERR:                64.93\n",
      "As in 2021 paper. For some reason not using dev for training - could fix. Notice the difference in accuracy and ERR.\n"
     ]
    }
   ],
   "source": [
    "normEval.evaluate(test_raw, test_norm, baseline.mfr(train_raw, train_norm, test_raw))\n",
    "print(\"As in 2021 paper. For some reason not using dev for training - could fix. Notice the difference in accuracy and ERR.\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "american_70 = open(os.path.join(DATA_PATH, \"interim/american-70.txt\"))\n",
    "words = set()\n",
    "for line in american_70:\n",
    "    words.add(line.strip().lower())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent raw tokens not in lexicon: 17.770912933907812\n",
      "Percent normed tokens not in lexicon: 13.020621629677803\n",
      "Phi coefficient between in lexicon and normalisation: 0.31\n",
      "This means there is a moderate relationship between them.\n",
      "Most common un-normalised raw alphanumeric tokens in lexicon: [('rt', 1503), ('i', 1102), ('the', 1036), ('to', 888), ('a', 814), ('and', 714), ('you', 577), ('in', 536), ('is', 529), ('for', 522), ('me', 457), ('my', 450), ('on', 440), ('of', 428), ('it', 357), ('with', 321), ('that', 313), ('this', 312), ('so', 290), ('be', 269)]\n",
      "Not super helpful except note domain specific 'rt' left alone.\n",
      "Most common normalised raw alphanumeric tokens in lexicon: [('u', 569), ('nigga', 117), ('niggas', 94), ('n', 93), ('ur', 74), ('rt', 49), ('gonna', 46), ('r', 44), ('bout', 37), ('wanna', 33), ('d', 31), ('yo', 30), ('cause', 29), ('b', 29), ('dat', 28), ('bc', 28), ('tho', 27), ('ya', 27), ('wit', 26), ('cant', 25)]\n",
      "Annotation guidelines for the n-word! May want to remove single letter words from lexicon (apart from v. common ones), and filter out double letter acronyms with an internet acronyms list. Note wanna normalised in 2021, inconsistent rt normalisation.\n",
      "This is important as lookup is used to determine if generated candidate is valid/used as feature for candidate selection.\n",
      "Most common un-normalised raw alphanumeric tokens not in lexicon: [('lol', 469), ('haha', 127), ('omg', 101), ('lmao', 96), ('2', 65), ('exo', 63), ('bae', 63), ('idk', 63), ('hahaha', 62), ('niall', 59), ('1', 57), ('5', 46), ('wtf', 45), ('smh', 40), ('zayn', 37), ('luhan', 35), ('2014', 35), ('3', 34), ('xd', 31), ('croke', 29)]\n",
      "Pretty much all interjections and some common names (one direction - time specific). May want to expand lexicon in these areas - have to generate as candidates to get correct!. Hard code what to leave alone?\n",
      "Most common normalised raw alphanumeric tokens not in lexicon: [('im', 336), ('dont', 149), ('pls', 68), ('lil', 62), ('thats', 54), ('aint', 53), ('bruh', 49), ('ima', 43), ('ppl', 43), ('cuz', 38), ('yall', 36), ('didnt', 31), ('gon', 29), ('goin', 28), ('tryna', 28), ('fav', 27), ('ive', 27), ('imma', 26), ('2', 25), ('fuckin', 24)]\n",
      "A lot of missing apostrophes - think about for candidate generation.\n",
      "A good lexicon (high correlation between in lexicon and normalised) will be good for checking validity of generated candidates - not suggesting candidates that wouldn't be considered normalised, not rejecting ones that would be (obviously on individual word basis). Size offers tradeoff between former and latter.\n",
      "WANT: to either expand lexicon or hard code to reduce un-normalised non-lexical, so that non-lexical->normalised. to reduce lexicon to reduce normalised lexical, so that lexical->non-normalised. Of course there will always be OOV words not requiring normalisation, e.g. unknown/nobel named entities, and IV words requiring normalisation, e.g. where misspelling of other words etc.\n",
      "Most common raw tokens not in lexicon: [('lol', 469), ('im', 336), ('dont', 149), ('haha', 127), ('omg', 101), ('lmao', 96), ('2', 90), ('pls', 68), ('exo', 63), ('bae', 63)]\n",
      "Most common norm tokens not in lexicon: [('lol', 469), ('haha', 127), ('omg', 101), ('lmao', 96), ('2', 66), ('exo', 63), ('bae', 63), ('idk', 63), ('hahaha', 62), ('niall', 59)]\n",
      "40831 2076 9575 3541\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "raw_non_lexical = Counter()\n",
    "norm_non_lexical = Counter()\n",
    "changed_non_lexical = Counter()\n",
    "unchanged_non_lexical = Counter()\n",
    "changed_lexical = Counter()\n",
    "unchanged_lexical = Counter()\n",
    "\n",
    "for twe_raw, twe_norm in zip(full_raw, full_norm):\n",
    "    for token in twe_raw:\n",
    "        if token.isalnum() and token not in words:\n",
    "            raw_non_lexical.update([token])\n",
    "    for norm in twe_norm:\n",
    "        for token in norm.split():\n",
    "            if token.isalnum() and token not in words:\n",
    "                norm_non_lexical.update([token])\n",
    "    for raw, norm in zip(twe_raw, twe_norm):\n",
    "        if raw.isalnum():\n",
    "            if raw in words:\n",
    "                if raw == norm:\n",
    "                    unchanged_lexical.update([raw])\n",
    "                else:\n",
    "                    changed_lexical.update([raw])\n",
    "            elif raw not in words:\n",
    "                if raw == norm:\n",
    "                    unchanged_non_lexical.update([raw])\n",
    "                else:\n",
    "                    changed_non_lexical.update([raw])\n",
    "print(f\"Percent raw tokens not in lexicon: {sum(raw_non_lexical.values())*100/raw_count}\")\n",
    "print(f\"Percent normed tokens not in lexicon: {sum(norm_non_lexical.values())*100/raw_count}\")\n",
    "a = sum(unchanged_lexical.values())\n",
    "b = sum(changed_lexical.values())\n",
    "c = sum(unchanged_non_lexical.values())\n",
    "d = sum(changed_non_lexical.values())\n",
    "phi = (a * d - b * c) / math.sqrt((a+b)*(b+d)*(a+c)*(c+d))\n",
    "print(f\"Phi coefficient between in lexicon and normalisation: {phi:.2f}\")\n",
    "print(\"This means there is a moderate relationship between them.\")\n",
    "print(f\"Most common un-normalised raw alphanumeric tokens in lexicon: {unchanged_lexical.most_common(20)}\")\n",
    "print(\"Not super helpful except note domain specific 'rt' left alone.\")\n",
    "print(f\"Most common normalised raw alphanumeric tokens in lexicon: {changed_lexical.most_common(20)}\")\n",
    "print(\"Annotation guidelines for the n-word! May want to remove single letter words from lexicon (apart from v. common ones), and filter out double letter acronyms with an internet acronyms list. Note wanna normalised in 2021, inconsistent rt normalisation.\")\n",
    "print(\"This is important as lookup is used to determine if generated candidate is valid/used as feature for candidate selection.\")\n",
    "print(f\"Most common un-normalised raw alphanumeric tokens not in lexicon: {unchanged_non_lexical.most_common(20)}\")\n",
    "print(\"Pretty much all interjections and some common names (one direction - time specific). May want to expand lexicon in these areas - have to generate as candidates to get correct!. Hard code what to leave alone?\")\n",
    "print(f\"Most common normalised raw alphanumeric tokens not in lexicon: {changed_non_lexical.most_common(20)}\")\n",
    "print(\"A lot of missing apostrophes - think about for candidate generation.\")\n",
    "print(\"A good lexicon (high correlation between in lexicon and normalised) will be good for checking validity of generated candidates - not suggesting candidates that wouldn't be considered normalised, not rejecting ones that would be (obviously on individual word basis). Size offers tradeoff between former and latter.\")\n",
    "print(\"WANT: to either expand lexicon or hard code to reduce un-normalised non-lexical, so that non-lexical->normalised. to reduce lexicon to reduce normalised lexical, so that lexical->non-normalised. Of course there will always be OOV words not requiring normalisation, e.g. unknown/nobel named entities, and IV words requiring normalisation, e.g. where misspelling of other words etc.\")\n",
    "print(f\"Most common raw tokens not in lexicon: {raw_non_lexical.most_common(10)}\")\n",
    "print(f\"Most common norm tokens not in lexicon: {norm_non_lexical.most_common(10)}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "outputs": [],
   "source": [
    "def if_normed(raw_list, norm_list):\n",
    "    resp = []\n",
    "    for raw_tweet, norm_tweet in zip(raw_list, norm_list):\n",
    "        resp_tweet = []\n",
    "        for raw_tok, norm_tok in zip(raw_tweet, norm_tweet):\n",
    "            if raw_tok == norm_tok:\n",
    "                resp_tweet.append((raw_tok, \"\"))\n",
    "            else:\n",
    "                resp_tweet.append((raw_tok, norm_tok))\n",
    "        resp.append(resp_tweet)\n",
    "    return resp\n",
    "\n",
    "full_if_normed = if_normed(full_raw, full_norm)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "outputs": [],
   "source": [
    "def norm_condition(raw_list, norm_list, condition, pair=False, norm=False):\n",
    "    \"\"\"\n",
    "    Return counter for every combination of normalised and satisfying condition given a list of tuples of tokens and normalisations if done\n",
    "    \"\"\"\n",
    "    p_normed = Counter()\n",
    "    p_unnormed = Counter()\n",
    "    n_normed = Counter()\n",
    "    n_unnormed = Counter()\n",
    "    for tweet_raw, tweet_norm in zip(raw_list, norm_list):\n",
    "        for raw, normed in zip(tweet_raw, tweet_norm):\n",
    "            tok = normed if norm else raw\n",
    "            to_update = (tok, normed) if pair else tok\n",
    "            if raw != normed and condition(tok):\n",
    "                p_normed.update([to_update])\n",
    "            elif raw == normed and condition(tok):\n",
    "                p_unnormed.update([to_update])\n",
    "            elif raw != normed and not condition(tok):\n",
    "                n_normed.update([to_update])\n",
    "            else:\n",
    "                n_unnormed.update([to_update])\n",
    "    return p_unnormed, p_normed, n_unnormed, n_normed\n",
    "\n",
    "def correlation_with_norm(p_unnormed, p_normed, n_unnormed, n_normed):\n",
    "    \"\"\"\n",
    "    Calculate matthew's correlation coefficient between normalisation and the condition\n",
    "    \"\"\"\n",
    "    a = sum(p_unnormed.values())\n",
    "    b = sum(p_normed.values())\n",
    "    c = sum(n_unnormed.values())\n",
    "    d = sum(n_normed.values())\n",
    "    phi = (a * d - b * c) / math.sqrt((a+b)*(b+d)*(a+c)*(c+d))\n",
    "    return phi\n",
    "\n",
    "# a, b, c, d = norm_condition([[tok for tok in tweet if tok[0].isalnum()] for tweet in full_if_normed], lambda x: x in words)\n",
    "# correlation_with_norm(a, b, c, d)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Only 6 non-alphanumeric raw tokens are normalised: [((\"could've\", 'could have'), 2), ((\"good'o\", 'good'), 1), ((\"should've\", 'should have'), 1), ((\"ca'nt\", \"can't\"), 1), ((\"do'nt\", \"don't\"), 1)]. These all contain apostrophes, which are included as not invalidating tokens for normalisation. However, could've and should've should not have been normalised under 2015 annotation rule 9. This rule also makes sense as expanding could change the formality of the text. ca'nt and do'nt have the apostrophe in the wrong place. Perhaps the contraction module could handle moving apostrophes as well as inserting to get a IV word. Modules should only be able to add candidates, not remove them. Unclear what is happening in good'o but perhaps could hypothesise drop/split before/after the apostrophe? The alphanumeric filter conveniently also filters out domain specific entities.\n"
     ]
    }
   ],
   "source": [
    "_, b, _, _= norm_condition(full_raw, full_norm, lambda x: not x.isalnum(), True, False)\n",
    "print(f\"Only {sum(b.values())} non-alphanumeric raw tokens are normalised: {b.most_common()}. These all contain apostrophes, which are included as not invalidating tokens for normalisation. However, could've and should've should not have been normalised under 2015 annotation rule 9. This rule also makes sense as expanding could change the formality of the text. ca'nt and do'nt have the apostrophe in the wrong place. Perhaps the contraction module could handle moving apostrophes as well as inserting to get a IV word. Modules should only be able to add candidates, not remove them. Unclear what is happening in good'o but perhaps could hypothesise drop/split before/after the apostrophe? The alphanumeric filter conveniently also filters out domain specific entities.\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 0 raw tokens containing whitespace.\n"
     ]
    }
   ],
   "source": [
    "a, b, _, _ = norm_condition(full_raw, full_norm, lambda x: any(char.isspace() for char in x), True)\n",
    "print(f\"There are {sum((a+b).values())} raw tokens containing whitespace.\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1552 rt tokens total, with 1503 left alone. rt at the start of a tweet is a domain specific entity and thus should be disregarded as per rule 3. In the middle of a tweet, this is more debatable. There are 1416 rt tokens at the start of tweets, which are all left alone. Middle rts (130) are inconsistently annotated (46 normalised), but if they are normalised, this is to retweet. From observation looks like domain-specific rts are always followed by a @mention. 42/46 normalised middle rts are not followed by @, 61/84 unnormalised middle rts are. There are 6 rts at the end of tweets, and they are split down the middle in terms of annotation. So perhaps rule could be: if at start of tweet or followed by @mention, ignore.\n"
     ]
    }
   ],
   "source": [
    "a, b, _, _ = norm_condition(full_raw, full_norm, lambda x: x == \"rt\", True)\n",
    "print(f\"There are {sum((a+b).values())} rt tokens total, with {sum(a.values())} left alone. rt at the start of a tweet is a domain specific entity and thus should be disregarded as per rule 3. In the middle of a tweet, this is more debatable. There are {sum(1 if tweet[0] == 'rt' else 0 for tweet in full_raw)} rt tokens at the start of tweets, which are all left alone. Middle rts (130) are inconsistently annotated (46 normalised), but if they are normalised, this is to retweet. From observation looks like domain-specific rts are always followed by a @mention. 42/46 normalised middle rts are not followed by @, 61/84 unnormalised middle rts are. There are 6 rts at the end of tweets, and they are split down the middle in terms of annotation. So perhaps rule could be: if at start of tweet or followed by @mention, ignore.\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "outputs": [],
   "source": [
    "# print([tweet[tweet.index('rt'):tweet.index('rt')+2] for tweet in full_raw if 'rt' in tweet and len(tweet[tweet.index('rt'):tweet.index('rt')+2]) == 2 and tweet[tweet.index('rt')+1][0] != '@'])\n",
    "\n",
    "# count = 0\n",
    "# for tweet in full_if_normed:\n",
    "#     for i, tok in enumerate(tweet):\n",
    "#         if tok[0] == 'rt' and 0 < i < len(tweet)-1 and not tok[1] and tweet[i+1][0][0] != '@':\n",
    "#             print(tweet[i+1])\n",
    "#             count += 1\n",
    "# print(count)\n",
    "\n",
    "# [[tweet[i:i+2] for i, tok in enumerate(tweet) if tok == 'rt' and len(tweet[i:i+2]) < 2] for tweet in full_raw if 'rt' in tweet]\n",
    "\n",
    "# No starting rts normalised\n",
    "# [[tok for i, tok in enumerate(tweet) if tok[0] == 'rt' and tok[1] and i == 0] for tweet in full_if_normed if len([tok for i, tok in enumerate(tweet) if tok[0] == 'rt' and tok[1] and i==0])]\n",
    "\n",
    "# Middle rts inconsistently normalised\n",
    "# [[tok for i, tok in enumerate(tweet) if tok[0] == 'rt' and 0 < i < len(tweet)-1] for tweet in full_if_normed if len([tok for i, tok in enumerate(tweet) if tok[0] == 'rt' and 0 < i < len(tweet)-1])])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "outputs": [],
   "source": [
    "spacy_full_raw = []\n",
    "for tweet in full_raw:\n",
    "    spacy_full_raw.append(nlp(\" \".join(tweet)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "outputs": [],
   "source": [
    "# for tweet in full_if_normed:\n",
    "#     for tok, norm in tweet:\n",
    "#         dok = nlp(tok)[0]\n",
    "#         type = dok.pos_\n",
    "#         if type == \"PROPN\":\n",
    "#             print(tok, norm)\n",
    "\n",
    "a, b, c, d = norm_condition(full_raw, full_norm, lambda x: nlp(x)[0].pos_ == \"PROPN\", True, False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ozil', 'bie', 'haha', 'avedon', 'pshh', 'hahaha', 'bria', 'exo', 'chanyeol', 'sehun', 'lol', 'grecia', 'hana', 'jeongguk', 'dremel', 'keelan', 'amo', 'investissement', 'heheheh', 'galeris', 'salmann', 'khann', 'croke', 'mtfs', 'pss', 'havard', 'nordtveit', 'yaya', 'shakira', 'calle', 'wtf', 'xo', 'gervais', 'peppa', 'kylie', 'sungjong', 'seongyeol', 'myungsoo', 'hai', 'kcu', 'oppa', 'kuru', 'manis', 'moscato', 'mq', 'doto', 'shourie', 'ji', 'badboy', 'ballerino', 'lmao', 'marwan', 'asmar', 'lonergan', 'kna', 'jga', 'enda', 'poch', 'kalau', 'hisap', 'inte', 'babbar', 'pacquiao', 'zedd', 'kamar', 'bae', 'alcon', 'chmp', 'simbrinz', 'kanye', 'zimuvuta', 'gwamba', 'fasta', 'pouncey', 'annabeth', 'kloss', 'enduro', 'fenwick', 'tbh', 'benim', 'kamran', 'apna', 'uma', 'toma', 'ng', 'jaya', 'haahaha', 'hahagaha', 'lantak', 'rasheeda', 'jinka', 'hu', 'parha', 'hyn', 'mathata', 'ke', 'annalynne', 'mccord', 'catz', 'gazzetta', 'psg', 'hahah', 'jfb', 'juseyo', 'bgt', 'matty', 'scaramouche', 'maomao', 'cansas', 'civico', 'jajaja', 'wifey', 'xolo', 'lfc', 'jobby', 'cuzz', 'idc', 'nuneaton', 'wimminz', 'kerrang', 'tabaruis', 'arigatou', 'gozaimasu', 'zuccarello', 'gionta', 'cba', 'chav', 'suho', 'atleti', 'kurang', 'kakak', 'serius', 'hehe', 'dlineman', 'wrs', 'modell', 'uvp', 'ligid', 'dubb', 'chica', 'mclaren', 'ronaldo', 'borno', 'camo', 'sayang', 'amb', 'ruffalo', 'botol', 'wallingford', 'directon', 'dawg', 'bmi', 'interp', 'nigg', 'sheeran', 'fanclub', 'wmf', 'aktiengese', 'jong', 'hyun', 'molaa', 'esta', 'basta', 'maganda', 'busan', 'ibnlivechinese', 'stadionie', 'newsfeed', 'rosberg', 'abang', 'haan', 'hmu', 'lolz', 'marriner', 'janick', 'jarryd', 'smtown', 'andalucia', 'hala', 'nada', 'tmh', 'ermenegildo', 'zegna', 'hahahahha', 'hahahaha', 'illa', 'kagepro', 'baekhyun', 'cek', 'caxias', 'wala', 'tota', 'avril', 'fredo', 'x69', 'thot', 'cewek', 'loh', 'pheobe', 'uefa', 'foun', 'elin', 'durk', 'nunu', 'hasina', 'henny', 'amritha', 'macbook', 'nahin', 'parmesean', 'ftw', 'narendra', 'joaquim', 'soubhik', 'stefani', 'frfr', 'hiii', 'yiyo', 'sarante', 'homenaje', 'huhuhuhuhuh', 'encanta', 'nao', 'vato', 'sushmita', 'balgo', 'magandang', 'yun', 'arie', 'cabron', 'rana', 'samaha', 'rakim', 'shugga', 'facetime', 'ogba', 'hahahahahaha', 'kitso', 'voir', 'finca', 'rey', 'ryu', 'bkn', 'mulia', 'pitbull', 'uae', 'naku', 'zyewa', 'brixham', 'cassillas', 'priyanka', 'fabulosa', 'looch', 'neikeiishav', 'oxy', 'shizz', 'ibalik', 'axel', 'pbb', 'be2003', 'kpop', 'yan', 'junon', 'suga', 'baee', 'greyni', 'tanpa', 'syal', 'sleepytime', 'andreina', 'puchong', 'waqa', 'fij', 'dungey', 'mi870621', 'ughh', 'vai', 'kah', 'michal', 'tweep', 'kai', 'awwww', 'ahaha', 'greyson', 'gortat', 'cooyyyy', 'liddell', 'jai', 'jes', 'blanchett', 'jank', 'efron', 'sctv', 'cabe', 'takuma', 'eptv', 'donizildo', 'persie', 'jz', 'abi', 'loooooool', 'ayato', 'woh', 'khaa', 'chavvy', 'xabi', 'alonso', 'jameela', 'jamil', 'krillin', 'lucu', 'kak', 'niam', 'ptdr', 'jassume', 'collab', 'umbridge', 'mbc', 'bigyan', 'ako', 'cockadoodledo', 'indi', 'vinz', 'sinabi', 'balotelli', 'paz', 'ngl', 'hea', 'ang', 'leamington', 'iggy', 'ashlynn', 'libre', 'ohmygod', 'nuala', 'tierna', 'kaam', 'neco', 'jeti', 'kzn', 'kawan', 'tangina', 'mnl', 'durat', 'ffn', 'shab', 'xxxx', 'ciye', 'pvd', 'laro', 'tayo', 'von', 'trapp', 'lololol', 'malik', 'aku', 'mau', 'woah', 'krispy', 'mep', 'vauxhall', 'vectra', 'signum', 'almarhum', 'lautner', 'dapat', 'hakuna', 'forney', 'tysm', 'jia', 'fei', 'siapa', 'kita', 'vyd', 'thang', 'pagkikita', 'muy', 'jt', 'nana', 'bamm', 'afia', 'kamera', 'jalah', 'porscha', 'chaiyya', 'avicii', 'nicky', 'bsjfnskf', 'hahahahahahahahah', 'hee', 'nicki', 'hgdsfs', 'soberano', 'isra', 'eduard', 'ee', 'remy', 'siya', 'oomfs', 'nino', 'naeem', 'kyungsoo', 'slikaj', 'ustima', 'sao', 'addis', 'abeba', 'cambiame', 'dia', 'jin', 'sooyoung', 'cinta', 'juwita', 'fx', 'sudirman', 'rell', 'ruga', 'sportseoul', 'perfeito', 'summerbee', 'senja', 'mblooo', 'khris', 'mansbridge', 'knowlton', 'msh', 'swifty', 'molson', 'sami', 'zayne', 'juni', 'kasih', 'gratia', 'gycj', 'defo', 'cabela', 'kacang', 'jutek', 'tenshi', 'assista', 'jbjb', 'sajc', 'haja', 'jen', 'forwood', 'gyllenhaal', 'darko', 'lorde', 'halina', 'merda', 'jogo', 'mais', 'chapstick', 'strom', 'jorok', 'wahh', 'nahhh', 'wyd', 'salah', 'jihadis', 'mata', 'jaden', 'triston', 'mundo', 'huffington', 'jang', 'shaquille', 'hahahahahhahaha', 'dmvan', 'akxndknd', 'ambien', 'kenia', 'nonton', 'wkwk', 'kangen', 'gelo', 'patt', 'derrito', 'snd', 'biggy', 'hahahahhaa', 'joakim', 'muaythai', 'garie', 'jetski', 'seunggi', 'andhra', 'telangana', 'lewl', 'horor', 'vittoria', 'hahahahahha', 'hhahah', 'lakas', 'maka', 'manusia', 'ramah', 'suh', 'matosevic', 'chai', 'pj', 'terkoneksi', 'braai', 'ade', 'gua', 'tak', 'maksudnya', 'ginebra', 'puso', 'shon', 'kyung', 'amorsito', 'donjazzy', 'bello', 'idr', 'crewe', 'cucamonga', 'dbhdjddn', 'jorg', 'dietzel', 'inuyasha', 'tgk', 'busuk', 'negara', 'hahahha', 'zouis', 'scooby', 'mewah', 'sian', 'lah', 'bayne', 'jago', 'sepakbolanya', 'arkandayiz', 'kimmis', 'waab', 'kym', 'uko', 'mga', 'topo', 'aappu', 'kannukku', 'ohoi', 'mei', 'thundercat', 'spongebob', 'gue', 'kenal', 'jackman', 'kuala', 'alomo', 'tendulkar', 'mmmmm', 'wizkid', 'decima', 'etienne', 'francey', 'logitech', 'playsta', 'nikkaz', 'herro', 'ugg', 'ibu', 'karebosi', 'smc', 'herndon', 'aache', 'rahe', 'jgn', 'ngert', 'dmu', 'dli', 'greenlee', 'eiichiro', 'sensei', 'haftada', 'bitchin', 'fuckboy', 'dapatkan', 'hingga', 'setiap', 'hari', 'diss', 'calo', 'economia', 'mami', 'dillashaw', 'bharta', 'deepak', 'chorasia', 'paida', 'meri', 'bhej', 'bhai', 'mufasa', 'chahta', 'lim', 'jn', 'muahaha', 'sehari', 'meimei', 'desabafa', 'ele', 'omeruo', 'ewnjap', 'kbs', 'aishwarya', 'eun', 'sunod', 'xbox', 'esquisito', 'nisso', 'gleesh', 'lmfaoooo', 'belo', 'mbeya', 'namo', 'keren', 'putangina', 'hahahsahahaha', 'vut', 'kahkahkah', 'colombiana', 'yeshh', 'kd', 'djokovic', 'pasukan', 'puyat', 'medtronic', 'besh', 'hunnid', 'dollas', 'pagi', 'lolol', 'fifa', 'cracknell', 'tila', 'manam', 'nagarjuna', 'garu', 'kesha', 'akademiks', 'avirex', 'brookfield', 'liro', 'lirobel', 'keibler', 'pobre', 'bindi', 'sigo', 'ndume', 'liat', 'ibn', 'battuta', 'termurah', 'nusantara', 'dll', 'mackay', 'ela', 'abdel', 'fattah', 'beequnique', 'mgmt', 'hernan', 'wmc', 'letso', 'kgm', 'tottenham', 'whl', 'geylang', 'durimi', 'jumahat', 'ano', 'diretso', 'ulit', 'tas', 'edie', 'thibaut', 'botwe', 'nbd', 'owa', 'ndaluma', 'leeeee', 'lamiel', 'simbora', 'xuerui', 'sakelah', 'kuy', 'muka', 'pegang', 'maklumlah', 'noteven', 'floorball', 'tiwa', 'mino', 'aon', 'donzell', 'rinatauveli', 'hyland', 'x47', 'shriya', 'mkhaba', 'nizothini', 'kengoku', 'nyanyi', 'contagieux', 'emmerich', 'upsc', 'chaat', 'boro', 'ots', 'welp', 'bikin', 'fmi', 'chiraq', 'explotar', 'walang', 'isco', 'mmmm', 'gojira', 'rela', 'sukarelawan', 'nkansah', 'yooara', 'hellup', 'chayse', 'wizz', 'auro', 'dnp', 'tarrus', 'tluk', 'tobi', 'jaylen', 'kakashi', 'charlet', 'akwa', 'cano', 'cac', 'kuya', 'farah', 'jessy', 'unnie', 'eyy', 'hahahh', 'ibs', 'photoshop', 'pouya', 'kejriwal', 'tihar', 'uff', 'jolie', 'kee', 'bitlocker', 'hyuk', 'buti', 'sina', 'ferb', 'unli', 'altaf', 'citi', 'gozaimashta', 'farhan', 'nizami', 'blono', 'gotti', 'kmsl', 'eijk', 'chong', 'birmz', 'fulham', 'congis', 'segundon', 'agarro', 'khar', 'naah', 'bergeron', 'ikr', 'tondo', 'xoxo', 'chibok', 'aksi', 'cuzzo', 'sagt', 'noch', 'lmaooooo', 'anc', 'bandanas', 'sica', 'yoona', 'karan', 'nalala', 'selo', 'hita', 'eamon', 'byrne', 'daly', 'stoltz', 'iya', 'ciel', 'ek', 'okc', 'brah', 'totp', 'quan', 'katt', 'zaza', 'sharmas', 'dabas', 'janu', 'soo', 'errm', 'yoohoo', 'awa', 'kouhai', 'looooh', 'uw', 'tatlong', 'araw', 'nalang', 'yung', 'kyoya', 'jelena', 'akshay', 'kumar', 'alli', 'dann', 'gespannt', 'karangan', 'migos', 'carshow', 'khloe', 'tesco', 'burkina', 'kuota', 'tiap', 'hella', 'comel', 'bhahahaha', 'heffernan', 'makarova', 'stfd', 'fil', 'arani', 'suegra', 'dere', 'xan', 'patama', 'titillandus', 'ahjumma', 'jokowi', 'kasus', 'korupsi', 'transjakarta', 'satnite', 'dealema', 'kamis', 'wohooo', 'otw', 'zamana', 'agad', 'jajajajajaja', 'kwara', 'sila', 'terserah', 'sumatera', 'mxm', 'wma', 'siang', 'bachata', 'naranasan', 'sobrang', 'galactica', 'merica', 'cemburu', 'tiiddaakkkkk', 'heropanti', 'fdfs', 'hoya', 'macaron', 'bosan', 'higashi', 'bianco', 'italico', 'noida', 'rbsb6', 'ballo', 'hahaaha', 'manne', 'loe', 'blancas', 'koosuwan', 'lfsci', 'misha', 'kath', 'polaright', 'kpa', 'smbd', 'kms', 'misurata', 'omegle', 'papelbon', 'theechaco', 'ahan', 'conyo', 'basara', 'charissa', 'bernabe', 'dl', 'prabowo', 'acl', 'bex', 'agbuya', 'bareng', 'kwarta', 'uan', 'acommo', 'momento', 'fue', 'tyc', 'hafeez', 'wali', 'jamhooriyat', 'jali', 'dhandli', 'leiam', 'osu', 'ciroc', 'ngelatih', 'temukan', 'tsk', 'sabia', 'nlr', 'kohli', 'cheon', 'chanbar', 'buuren', 'satu', 'berkacalah', 'wahai', 'sukanya', 'podia', 'bikn', 'stuntin', 'konti', 'pag', 'asc', 'nomnomnomnom', 'xfa', 'fse', 'hyung', 'nakakasad', 'umikot', 'ucf', 'griezmann', 'juventus', 'smokeshow', 'kennett', 'ilmi', 'ancelotti', 'dorma', 'peninggi', 'badan', 'penggemuk', 'alami', 'senhora', 'everton', 'hahahahahah', 'gehd', 'flameyeol', 'onitsha', 'chinelo', 'huma', 'neng', 'shinseki', 'digvijay', 'sanjay', 'wip', 'boyband', 'marikina', 'sau', 'lahat', 'kal', 'sunetrac', 'niecy', 'mds', 'yuh', 'jfc', 'kawaii', 'haahahahha', 'macama', 'terimakassih', 'akon', 'xaragdax', 'xezee', 'cagt', 'baisan', 'niya', 'shettima', 'kardashia', 'atd', 'paramore', 'ume', 'psd', 'x8', 'imran', 'nda', 'fami', 'arro', 'ainda', 'niyata', 'hahahahahahhahaha', 'makapag', 'ipon', 'jskcosnxoancoamxis', 'mcilroy', 'suju', 'spiderman', 'itq', 'needa', 'bargin', 'ndi', 'zomwe', 'jb', 'apa', 'shaah', 'tarra', 'lirik', 'terjemahan', 'carcillo', 'jabroni', 'yovani', 'glisan', 'halaaa', 'awww', 'margiela', 'walz', 'fiba', 'ariana', 'ekxh', 'realy', 'yeoja', 'dulu', 'kf', 'bongkaran', 'perghhhh', 'laut', 'aguero', 'correa', 'gooch', 'dumont', 'lundqvist', 'pao', 'salam', 'spsl', 'beamer', 'inam', 'haq', 'daum', 'naeun', 'janoski', 'msl', 'ecf', 'kilian', 'kaggs', 'meron', 'kang', 'pahhheerraamm', 'bintang', 'rjr', 'jagah', 'karta', 'teben', 'xoxoxxo', 'astaghfirullah', 'farrukh', 'shehar', 'sayeng', 'vuldemurt', 'lakala', 'nene', 'pegar', 'algo', 'jhene', 'aina', 'jheeze', 'pg24', 'dwade', 'anais', 'bok', 'abdullah', 'sudah', 'melampau', 'kot', 'hahha', 'uhmmm', 'viv', 'kenn', 'jaylin', 'shabu', 'wenger', 'a2', 'nizamabad', 'haddon', 'mmm', 'kunjani', 'kule', 'svu', 'sakho', 'yosh', 'bana', 'acura', 'oem', 'esn', 'imei', 'ohne', 'izak', 'sharjah', 'feroz', 'kaygeo', 'baho', 'teh', 'cust', 'chicharito', 'tek', 'salut', 'masso', 'dari', 'kapala', 'sante', 'avenida', 'ppc', 'hawalli', 'itv', 'loveholic', 'cabello', 'sampe', 'hiiii', 'dubstep', 'gabbana', 'jsy', 'timberlake', 'beckham', 'athar', 'shaq', 'uggh', 'eww', 'riaz', 'khalil', 'arnaiz', 'yao', 'ros', 'pon', 'miuna', 'waynerooney', 'fabio', 'capello', 'tbf', 't6', 'branford', 'kaos', 'gopro', 'mountsby', 'gwyneth', 'paltrow', 'makalahlwe', 'pouliot', '125th', 'sawtell', 'ewa', 'bangtan', 'ambi', 'inori', 'tsugumi', 'tonyselznick', 'ifop', 'eelv', 'fdg', 'eataly', 'duramax', 'zelo', 'exs', 'nengok', 'klaroline', 'killr', 'junie', 'teamchill', 'broham', 'senayan', 'pochey', 'zardari', 'machar', 'madhu', 'kiran', 'orali', 'mazkirah', 'kematian', 'slavkov', 'brna', 'kesip', 'hibs', 'sasuke', 'itachi', 'chu', 'ucsb', 'sistah', 'lallana', 'pilih', 'ketimbang', 'arcv', 'ayy', 'skiperooo', 'marlton', 'subhanallah', 'alhamdulillah', 'thibodeau', 'messi', 'hoodrat', 'chatnewale', 'chacha', 'sikhao', 'merde', 'laa', 'juseyoo', 'janji', 'hrg', 'oppar', 'bhushan', 'hahahhahahahah', 'katanya', 'selatan', 'lga', 'khisiyani', 'billi', 'khambha', 'noche', 'alia', 'fauzi', 'chaiwaala', 'kylee', 'jaron', 'malioboro', 'jonny', 'kanna', 'hajiya', 'fome', 'mayne', 'arryn', 'yaoi', 'tashay', 'lapierre', 'uoeno', 'moxley', 'flocka', 'gwan', 'dto', 'jaddhah', 'naahh', 'cerco', 'xyz', 'ddr', 'paddington', 'kettl', 'sabang', 'stapleford', 'indica', 'awolowo', 'ikoyi', 'llegue', 'jasa', 'promosi', 'kira', 'rm50', 'maen', 'vlado', 'lkt', 'soixante', 'cbm', 'jommymateo', 'zaroorat', 'bandeira', 'irlanda', 'ayrton', 'venta', 'henslee', 'mandem', 'cuando', 'dijo', 'tarik', 'gonul', 'dilara', 'gigante', 'ewwwwwwww', 'mentiras', 'alezia', 'cheeki', 'rafiki', 'ilocos', 'didja', 'iyan', 'ogufe', 'quero', 'buri', 'josimar', 'neymar', 'tignan', 'nyo', 'labelle', 'aretha', 'yee', 'morc', 'margan', 'calvillo', 'krib', 'chodot', 'ohmygosh', 'mrt', 'kafi', 'det', 'kiya', 'omaigod', 'ikutan', 'manami', 'huy', 'heckard', 'violinby', 'caren', 'atitoda', 'poksa', 'dwg', 'charice', 'pempengco', 'haiyan', 'willingham', 'kung', 'hunny', 'haaaaaabibiiiiiiii', 'accs', 'haresh', 'zuka', 'ein', 'positiv', 'sheehan', 'nigz', 'trm', 'ikram', 'gosu', 'abhishek', 'lsdhaljd', 'euc', 'barkada', 'chiang', 'fic', 'marguerita', 'charleville', 'mdrr', 'bild', 'airasia', 'bri', 'hahahahah', 'zaxbys', 'malhotra', 'shraddha', 'aawww', 'gago', 'mirabella', 'cassy', 'mcw', 'hk', 'amalina', 'jamali', 'olx', 'krum', 'cutasfuck', 'jesper', 'ibiaku', 'nwaniba', 'jadeyyyy', 'kepal', 'kole', 'shih', 'et38', 'gba', 'meriam', 'brockville', 'herrington', 'xandy', 'benihanna', 'tejada', 'tanak', 'kacau', 'sepatah', 'dani', 'masi', 'ketinggalan', 'posole', 'ambani', 'ariston', 'berbahan', 'kayu', 'passa', 'x49', 'tangerang', 'kolon', 'instyle', 'juga', 'pevita', 'queenova', 'ita', 'passato', 'belanja', 'hrc', 'femi', 'gajapati', 'ruang', 'langsung', 'ziam', 'hmmm', 'mddrrr', 'msk', 'tfais', 'lollol', 'lourd', 'hornby', 'dcc', 'masyadong', 'papansin', 'wahalagrl', 'cantona', 'kungfu', 'kesmoot', 'namaste', 'bagaimana', 'dengan', 'rakan', 'dihl', 'ahha', 'dubbel', 'brouwerij', 'waha', 'smos', 'illmi', 'ifunny', 'idolo', 'bdna', 'kro', 'kyuhyun', 'jaegi', 'fammy', 'gomora', 'robala', 'kgotso', 'mckibben', 'mascato', 'howniall', 'ctf', 'roseville', 'ari', 'bermudez', 'talm', 'ayo', 'holla', 'lawan', 'mls', 'ngerjakno', 'peria', 'orionid', 'goodwood', 'kangeeeen', 'embem', 'cepetan', 'inframe', 'euxton', 'buckshaw', 'lihpar', 'kombi', 'juye', 'chehre', 'nhi', 'paso', 'spitzauer', 'nialler', 'baloch', 'ooooo', 'keshia', 'chante', 'pendejas', 'familia', 'demis', 'sharma', 'dulhania', 'hotline', 'abey', 'bigaada', 'nueva', 'hijo', 'puta', 'sehunee', 'gabisa', 'ajee', 'alsina', 'spammy', 'porta', 'giugno', 'xm', 'smfd', 'ggc', 'ciara', 'borris', 'diaw', 'mishon', 'yabadabadoo', 'akeh', 'demetria', 'existem', 'mahal', 'patra', 'teenebelle', 'aerosmith', 'mtho', 'tommee', 'jerman', 'abriram', 'hafi', 'kaayo', 'mahesh', 'bhatt', 'lhh', 'bvs', 'kitv', 'shardha', 'tourbus', 'jigg', 'emansipasi', 'evo', 'mbps', 'makarov', 'marburg', 'kaykay', 'turu', 'krafty', 'zico', 'cumplen', 'boca', 'saad', 'ddh', 'holton', 'takoma', 'doujin', 'khamis', 'tapelle', 'lml', 'lahm', 'batho', 'pdhl', 'gby', 'madhubala', 'ftfy', 'jeil', 'kkkkkkk', 'jongdae', 'pergi', 'keli', 'kryfko', 'baca', 'selengkapnya', 'lukkoy', 'semangat', 'hannan', 'faithfull', 'baat', 'kar', 'moshi', 'panera', 'juve', 'lampard', 'vallarta', 'olczyk', 'ponis', 'raku', 'chitoge', 'kkkk', 'ghirardelli', 'puedo', 'esto', 'frente', 'kaya', 'palang', 'cjr', 'gitaris', 'scarlett', 'mourinho', 'espinosa', 'usgs', 'h2h', 'stroman', 'serran', 'barragan', 'kiseop', 'gunn', 'cdi', 'hihi', 'hur', 'darvish', 'houghton', 'hardaway', 'mornight', 'zurich', 'jodorowsky', 'borkson', 'keonna', 'masarap', 'lysm', 'boney', 'oliveira', 'jelen', 'superliga', 'm3', 'khilafat', 'zindabad', 'iwaju', 'bomerang', 'remjx', 'lloyds', 'rihanna', 'rewatch', 'manutd', 'jangan', 'pacsun', 'shitbag', 'fanwar', 'egan', 'culina', 'linley', 'lovren', 'mahashivratri', 'sant', 'shivaji', 'wowowowow', 'bernabeu', 'johansson', 'yeh', 'odisha', 'efctv', 'kikkin', 'naww', 'naz', 'spoelstra', 'kilala', 'pinaka', 'wam', 'habloo', 'dara', 'hehehehe', 'mulgrew', 'entrevista', 'noviembre', 'llorando', 'sitc', 'keyaad', 'fnc', 'cuatro', 'koko', 'karim', 'benzema', 'mallshow', 'daayan', 'nahi', 'badli', 'kalpataru', 'das', 'mis', 'gustan', 'kulitan', 'xchange', 'noona', 'tamiya', 'amoo', 'derulo', 'rizka', 'pensi', 'cibitung', 'x21', 'yaay', 'virat', 'jellal', 'erza', 'smoothy', 'schwab', 'convo', 'kosta', 'boda', 'bany', 'genteng', 'kecil', 'pulau', 'budax', 'permpuan', 'farhi', 'sohai', 'sappi', 'yebke', 'hussman', 'ehsan', 'ahmadi', 'flatty', 'efx', 'bukti', 'kalo', 'oji', 'coochie', 'kenner', 'jodido', 'epl', 'sehman', 'witte', 'neutrogena', 'deje', 'crea', 'cauchi', 'tpke', 'snp', 'zigg', 'zagg', 'chara', 'setlist', 'darklordvoldemort666', 'jamhorayat', 'sorat', 'kellan', 'okaycan', 'zhang', 'csr', 'hafa', 'nna', 'infographic', 'kamo', 'cultura', 'nee', 'personaliti', 'makita', 'felicita', 'thuj', 'nung', 'karylle', 'spero', 'tutto', 'scocchia', 'mcinnis', 'mantap', 'necesito', 'haria', 'feliz', 'gbutang', 'maayo', 'kok', 'suropati', 'svip', 'isnin', 'elvina', 'idolos', 'insha', 'aboul', 'pakcik', 'besplatan', 'igricu', 'fanchat', 'muuch', 'flipante', 'toze', 'viejita', 'paco', 'vou', 'engolida', 'monstro', 'schumi', 'sowell', 'pasok', 'banyo', 'pagmamahal', 'mlk', 'vox', 'yiah', 'sera', 'krik', 'luka', 'kala', 'donatan', 'candyman', 'susunod', 'lelang', 'actualizo', 'maldita', 'hatetfarag', 'mumken', 'helter', 'apki', 'belakang', 'haaibo', 'sheringham', 'solskjaer', 'skrapz', 'lucco', 'kieran', 'zlatan', 'wasan', 'xc', 'ebc', 'diddy', 'heyyyy', 'jae', 'edd', 'soz', 'jaejoong', 'krabs']\n",
      "Correlation of PROPN and normalisation -0.10. Very low - probably due to bad POS performance thanks to lowercasing and other irregularities of the text - sort of a chicken and egg issue with downstream processing. A lot these could probably be found by expanding the lexicon instead.\n"
     ]
    }
   ],
   "source": [
    "# print(f\"{sum(b.values())}/{sum(d.values())+sum(b.values())}\")\n",
    "print([k[0] for k in a.keys() if k[0] not in words and k[0].isalnum()])\n",
    "print(f\"Correlation of PROPN and normalisation {correlation_with_norm(a, b, c, d):.2f}. Very low - probably due to bad POS performance thanks to lowercasing and other irregularities of the text - sort of a chicken and egg issue with downstream processing. A lot these could probably be found by expanding the lexicon instead.\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
