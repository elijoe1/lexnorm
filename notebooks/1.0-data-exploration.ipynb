{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from lexnorm.data import normEval\n",
    "from lexnorm.data import baseline\n",
    "from lexnorm.definitions import DATA_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "train_raw, train_norm = normEval.loadNormData(os.path.join(DATA_PATH, 'raw/train.norm'))\n",
    "test_raw, test_norm = normEval.loadNormData(os.path.join(DATA_PATH, 'raw/test.norm'))\n",
    "dev_raw, dev_norm = normEval.loadNormData(os.path.join(DATA_PATH, 'raw/dev.norm'))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of tweets: 4917\n",
      "Total number of normed tweets: 4917\n",
      "Unchanged from 2015 dataset.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total number of tweets: {len(train_raw) + len(test_raw) + len(dev_raw)}\")\n",
    "print(f\"Total number of normed tweets: {len(train_norm) + len(test_norm) + len(dev_norm)}\")\n",
    "print(\"Unchanged from 2015 dataset.\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of test set: 1967\n",
      "Unchanged from 2015 dataset. Keep split so can compare with both 2015 and 2021 entries.\n",
      "Size of train set: 2360\n",
      "As described in 2021 task paper.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Size of test set: {len(test_raw)}\")\n",
    "print(\"Unchanged from 2015 dataset. Keep split so can compare with both 2015 and 2021 entries.\")\n",
    "print(f\"Size of train set: {len(train_raw)}\")\n",
    "print(\"As described in 2021 task paper.\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "full_raw = train_raw + dev_raw + test_raw\n",
    "full_norm = train_norm + dev_norm + test_norm"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No length mismatch (as expected).\n"
     ]
    }
   ],
   "source": [
    "for tweet_raw, tweet_norm in zip(full_raw, full_norm):\n",
    "    if len(tweet_raw) != len(tweet_norm):\n",
    "        print(\"Length mismatch!\")\n",
    "print(\"No length mismatch (as expected).\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN\n",
      "Number of raw tokens: 35216\n",
      "Number of normed tokens: 35598\n",
      "Number of 1 to n normalisation raw tokens: 307\n",
      "Percentage of 1 to n: 0.87\n",
      "Number of n to 1 normalisation raw tokens: 13\n",
      "Percentage of n to 1: 0.04\n",
      "Number of normalised raw tokens: 2666\n",
      "Percentage normalised: 7.570422535211268\n",
      "DEV\n",
      "Number of raw tokens: 9169\n",
      "Number of normed tokens: 9282\n",
      "Number of 1 to n normalisation raw tokens: 98\n",
      "Percentage of 1 to n: 1.07\n",
      "Number of n to 1 normalisation raw tokens: 1\n",
      "Percentage of n to 1: 0.01\n",
      "Number of normalised raw tokens: 633\n",
      "Percentage normalised: 6.903697240702367\n",
      "TEST\n",
      "Number of raw tokens: 29421\n",
      "Number of normed tokens: 29738\n",
      "Number of 1 to n normalisation raw tokens: 262\n",
      "Percentage of 1 to n: 0.89\n",
      "Number of n to 1 normalisation raw tokens: 17\n",
      "Percentage of n to 1: 0.06\n",
      "Number of normalised raw tokens: 2324\n",
      "Percentage normalised: 7.899119676421604\n",
      "ALL\n",
      "Number of raw tokens: 73806\n",
      "Number of normed tokens: 74618\n",
      "Number of 1 to n normalisation raw tokens: 667\n",
      "Percentage of 1 to n: 0.90\n",
      "Number of n to 1 normalisation raw tokens: 31\n",
      "Percentage of n to 1: 0.04\n",
      "Number of normalised raw tokens: 5623\n",
      "Percentage normalised: 7.618621792266212\n"
     ]
    }
   ],
   "source": [
    "for name, collection in [(\"TRAIN\", zip(train_raw, train_norm)), (\"DEV\", zip(dev_raw, dev_norm)), (\"TEST\", zip(test_raw, test_norm)), (\"ALL\", zip(full_raw, full_norm))]:\n",
    "    print(name)\n",
    "    one_to_n_count = 0\n",
    "    n_to_one_count = 0\n",
    "    raw_count = 0\n",
    "    norm_count = 0\n",
    "    raw_normalised_count = 0\n",
    "    for tweet_raw, tweet_norm in collection:\n",
    "        for token_raw, token_norm in zip(tweet_raw, tweet_norm):\n",
    "            raw_count += 1\n",
    "            norm_count += len(token_norm.split(\" \"))\n",
    "            if not token_norm:\n",
    "                n_to_one_count += 1\n",
    "            if len(token_norm.split(\" \")) > 1:\n",
    "                one_to_n_count += 1\n",
    "            if token_norm != token_raw:\n",
    "                raw_normalised_count += 1\n",
    "    print(f\"Number of raw tokens: {raw_count}\")\n",
    "    print(f\"Number of normed tokens: {norm_count}\")\n",
    "    print(f\"Number of 1 to n normalisation raw tokens: {one_to_n_count}\")\n",
    "    print(f\"Percentage of 1 to n: {one_to_n_count * 100 / raw_count:.2f}\")\n",
    "    print(f\"Number of n to 1 normalisation raw tokens: {n_to_one_count}\")\n",
    "    print(f\"Percentage of n to 1: {n_to_one_count * 100 / raw_count:.2f}\")\n",
    "    print(f\"Number of normalised raw tokens: {raw_normalised_count}\")\n",
    "    print(f\"Percentage normalised: {raw_normalised_count * 100 / raw_count}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For some reason, the stats in the 2021 task paper are on the train set only (correct in that case).\n",
      "Apart from percentage change - unclear how this is calculated anyway.\n",
      "Note the 1 to n and n to 1 counts are counting the number of raw tokens involved in the respective normalisations.\n",
      "So a 5 to 1 normalisation would produce a count of 4 (number of tokens merged into first token) for 1 to n, and a 1 to 5 normalisation would produce a count of 1 for n to 1.\n",
      "Note as no capitalisation correction for 2015 dataset, in 2021 dataset version EVERYTHING (RAW AND GOLD) IS LOWER CASE\n",
      "2015 task paper has different statistics, one reason for this being lack of capitalisation consideration in 2021\n"
     ]
    }
   ],
   "source": [
    "print(\"For some reason, the stats in the 2021 task paper are on the train set only (correct in that case).\")\n",
    "print(\"Apart from percentage change - unclear how this is calculated anyway.\")\n",
    "print(\"Note the 1 to n and n to 1 counts are counting the number of raw tokens involved in the respective normalisations.\")\n",
    "print(\"So a 5 to 1 normalisation would produce a count of 4 (number of tokens merged into first token) for 1 to n, and a 1 to 5 normalisation would produce a count of 1 for n to 1.\")\n",
    "print(\"Note as no capitalisation correction for 2015 dataset, in 2021 dataset version EVERYTHING (RAW AND GOLD) IS LOWER CASE\")\n",
    "print(\"2015 task paper has different statistics, one reason for this being lack of capitalisation consideration in 2021\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "import json\n",
    "f_train = open(os.path.join(DATA_PATH, \"raw/2015/train_data.json\"))\n",
    "f_test = open(os.path.join(DATA_PATH, \"raw/2015/test_truth.json\"))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "fif_data = json.load(f_train)\n",
    "fif_data += json.load(f_test)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29 raw differences, 1036 norm only differences\n",
      "Most common norm differences: [(('laughing out loud', 'lol'), 465), (('oh my god', 'omg'), 100), (('laughing my ass off', 'lmao'), 96), ((\"i don't know\", 'idk'), 63), (('gonna', 'going to'), 46), (('what the fuck', 'wtf'), 45), (('shaking my head', 'smh'), 40), (('wanna', 'want to'), 33), (('laughing my fucking ass off', 'lmfao'), 26), ((\"i don't care\", 'idc'), 22)]\n",
      "Differences in 2015, 2021 raw due to username anonymization\n",
      "Differences in 2015, 2021 gold due to leaving interjections alone e.g. lol, lmfao, ctfu and normalising gonna and wanna\n",
      "Hence make sure models do too to ensure good performance on 2021 set (maybe dict lookup, hard coding?)\n",
      "Could evaluate on both datasets to compare with submissions from both tasks\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "raw_diff = 0\n",
    "norm_diff = 0\n",
    "norm = Counter()\n",
    "for fif, twe_raw, twe_norm in zip(fif_data, full_raw, full_norm):\n",
    "    fif_raw = [x.lower() for x in fif[\"input\"]]\n",
    "    fif_norm = [x.lower() for x in fif[\"output\"]]\n",
    "    if fif_raw != twe_raw:\n",
    "        raw_diff += 1\n",
    "    elif fif_norm != twe_norm:\n",
    "        norm_diff += 1\n",
    "        norm.update((x, y) for x, y in zip(fif_norm, twe_norm) if x != y)\n",
    "print(f\"{raw_diff} raw differences, {norm_diff} norm only differences\")\n",
    "print(f\"Most common norm differences: {norm.most_common(10)}\")\n",
    "print(\"Differences in 2015, 2021 raw due to username anonymization\")\n",
    "print(\"Differences in 2015, 2021 gold due to leaving interjections alone e.g. lol, lmfao, ctfu and normalising gonna and wanna\")\n",
    "print(\"Hence make sure models do too to ensure good performance on 2021 set (maybe dict lookup, hard coding?)\")\n",
    "print(\"Could evaluate on both datasets to compare with submissions from both tasks\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common normalisation pairs: [(('u', 'you'), 562), (('im', \"i'm\"), 334), (('dont', \"don't\"), 149), (('nigga', 'nigger'), 117), (('niggas', 'niggers'), 93), (('n', 'and'), 89), (('pls', 'please'), 68), (('lil', 'little'), 62), (('ur', 'your'), 54), (('thats', \"that's\"), 54)]\n",
      "Most common normalised raw words: [('u', 569), ('im', 336), ('dont', 149), ('nigga', 117), ('niggas', 94), ('n', 93), ('ur', 74), ('pls', 68), ('lil', 62), ('thats', 54)]\n",
      "Remember this is including the test set - can't use all of this for the normalisation dictionary!\n"
     ]
    }
   ],
   "source": [
    "normalised_pairs = Counter()\n",
    "non_standard_tokens = Counter()\n",
    "\n",
    "for tweet_raw, tweet_norm in zip(full_raw, full_norm):\n",
    "    for token_raw, token_norm in zip(tweet_raw, tweet_norm):\n",
    "        if token_raw != token_norm:\n",
    "            normalised_pairs.update([(token_raw, token_norm)])\n",
    "            non_standard_tokens.update([token_raw])\n",
    "\n",
    "print(f\"Most common normalisation pairs: {normalised_pairs.most_common(10)}\")\n",
    "print(f\"Most common normalised raw words: {non_standard_tokens.most_common(10)}\")\n",
    "print(\"Remember this is including the test set - can't use all of this for the normalisation dictionary!\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline acc.(LAI): 92.10\n",
      "Accuracy:           97.23\n",
      "ERR:                64.93\n",
      "As in 2021 paper. For some reason not using dev for training - could fix.\n"
     ]
    }
   ],
   "source": [
    "normEval.evaluate(test_raw, test_norm, baseline.mfr(train_raw, train_norm, test_raw))\n",
    "print(\"As in 2021 paper. For some reason not using dev for training - could fix.\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [],
   "source": [
    "american_70 = open(os.path.join(DATA_PATH, \"interim/american-70.txt\"))\n",
    "words = set()\n",
    "for line in american_70:\n",
    "    words.add(line.strip().lower())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phi coefficient between in lexicon and normalisation: 0.31\n",
      "This means there is a moderate relationship between them.\n",
      "Most common un-normalised raw alphanumeric tokens in lexicon: [('rt', 1503), ('i', 1102), ('the', 1036), ('to', 888), ('a', 814), ('and', 714), ('you', 577), ('in', 536), ('is', 529), ('for', 522)]\n",
      "Not super helpful except note domain specific 'rt' left alone.\n",
      "Most common normalised raw alphanumeric tokens in lexicon: [('u', 569), ('nigga', 117), ('niggas', 94), ('n', 93), ('ur', 74), ('rt', 49), ('gonna', 46), ('r', 44), ('bout', 37), ('wanna', 33)]\n",
      "Annotation guidelines for the n-word! May want to remove single letter, double letter words from lexicon (apart from v. common ones). Note wanna normalised.\n",
      "This is important as lookup is used to determine if generated candidate is valid/used as feature for candidate selection.\n",
      "Most common un-normalised raw alphanumeric tokens not in lexicon: [('lol', 469), ('haha', 127), ('omg', 101), ('lmao', 96), ('2', 65), ('exo', 63), ('bae', 63), ('idk', 63), ('hahaha', 62), ('niall', 59)]\n",
      "Pretty much all interjections and some common names (one direction - time specific). May want to expand lexicon in these areas. Hard code what to leave alone?\n",
      "Most common normalised raw alphanumeric tokens not in lexicon: [('im', 336), ('dont', 149), ('pls', 68), ('lil', 62), ('thats', 54), ('aint', 53), ('bruh', 49), ('ima', 43), ('ppl', 43), ('cuz', 38)]\n",
      "A lot of missing apostrophes - think about for candidate generation.\n",
      "A good lexicon (high correlation between in lexicon and normalised) will be good for checking validity of generated candidates - not suggesting candidates that wouldn't be considered normalised, not rejecting ones that would be (obviously on individual word basis). Size offers tradeoff between former and latter.\n",
      "WANT: to either expand lexicon or hard code to reduce un-normalised non-lexical, so that non-lexical->normalised. to reduce lexicon to reduce normalised lexical, so that lexical->non-normalised\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "raw_non_lexical = Counter()\n",
    "norm_non_lexical = Counter()\n",
    "changed_non_lexical = Counter()\n",
    "unchanged_non_lexical = Counter()\n",
    "changed_lexical = Counter()\n",
    "unchanged_lexical = Counter()\n",
    "lex_raw_count = 0\n",
    "\n",
    "# TODO: percent normalised tokens in lexicon, percent raw tokens in lexicon\n",
    "# TODO: percent non-alphanumeric raw tokens normalised (should be 0), percent alphanumeric raw tokens normalised. Extend to other annotation guidelines\n",
    "\n",
    "for twe_raw, twe_norm in zip(full_raw, full_norm):\n",
    "    for token in twe_raw:\n",
    "        if token.isalnum() and token not in words:\n",
    "            raw_non_lexical.update([token])\n",
    "    for norm in twe_norm:\n",
    "        for token in norm.split():\n",
    "            if token.isalnum() and token not in words:\n",
    "                norm_non_lexical.update([token])\n",
    "    for raw, norm in zip(twe_raw, twe_norm):\n",
    "        if raw.isalnum():\n",
    "            if raw in words:\n",
    "                if raw == norm:\n",
    "                    unchanged_lexical.update([raw])\n",
    "                else:\n",
    "                    changed_lexical.update([raw])\n",
    "            elif raw not in words:\n",
    "                if raw == norm:\n",
    "                    unchanged_non_lexical.update([raw])\n",
    "                else:\n",
    "                    changed_non_lexical.update([raw])\n",
    "a = sum(unchanged_lexical.values())\n",
    "b = sum(changed_lexical.values())\n",
    "c = sum(unchanged_non_lexical.values())\n",
    "d = sum(changed_non_lexical.values())\n",
    "phi = (a * d - b * c) / math.sqrt((a+b)*(b+d)*(a+c)*(c+d))\n",
    "print(f\"Phi coefficient between in lexicon and normalisation: {phi:.2f}\")\n",
    "print(\"This means there is a moderate relationship between them.\")\n",
    "print(f\"Most common un-normalised raw alphanumeric tokens in lexicon: {unchanged_lexical.most_common(10)}\")\n",
    "print(\"Not super helpful except note domain specific 'rt' left alone.\")\n",
    "print(f\"Most common normalised raw alphanumeric tokens in lexicon: {changed_lexical.most_common(10)}\")\n",
    "print(\"Annotation guidelines for the n-word! May want to remove single letter, double letter words from lexicon (apart from v. common ones). Note wanna normalised.\")\n",
    "print(\"This is important as lookup is used to determine if generated candidate is valid/used as feature for candidate selection.\")\n",
    "print(f\"Most common un-normalised raw alphanumeric tokens not in lexicon: {unchanged_non_lexical.most_common(10)}\")\n",
    "print(\"Pretty much all interjections and some common names (one direction - time specific). May want to expand lexicon in these areas. Hard code what to leave alone?\")\n",
    "print(f\"Most common normalised raw alphanumeric tokens not in lexicon: {changed_non_lexical.most_common(10)}\")\n",
    "print(\"A lot of missing apostrophes - think about for candidate generation.\")\n",
    "print(\"A good lexicon (high correlation between in lexicon and normalised) will be good for checking validity of generated candidates - not suggesting candidates that wouldn't be considered normalised, not rejecting ones that would be (obviously on individual word basis). Size offers tradeoff between former and latter.\")\n",
    "print(\"WANT: to either expand lexicon or hard code to reduce un-normalised non-lexical, so that non-lexical->normalised. to reduce lexicon to reduce normalised lexical, so that lexical->non-normalised\")"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
