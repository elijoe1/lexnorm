{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from lexnorm.data import normEval\n",
    "from lexnorm.data import baseline\n",
    "from lexnorm.definitions import DATA_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "train_raw, train_norm = normEval.loadNormData(os.path.join(DATA_PATH, 'raw/train.norm'))\n",
    "test_raw, test_norm = normEval.loadNormData(os.path.join(DATA_PATH, 'raw/test.norm'))\n",
    "dev_raw, dev_norm = normEval.loadNormData(os.path.join(DATA_PATH, 'raw/dev.norm'))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of tweets: 4917\n",
      "Total number of normed tweets: 4917\n",
      "Unchanged from 2015 dataset.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total number of tweets: {len(train_raw) + len(test_raw) + len(dev_raw)}\")\n",
    "print(f\"Total number of normed tweets: {len(train_norm) + len(test_norm) + len(dev_norm)}\")\n",
    "print(\"Unchanged from 2015 dataset.\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of test set: 1967\n",
      "Unchanged from 2015 dataset. Keep split so can compare with both 2015 and 2021 entries.\n",
      "Size of train set: 2360\n",
      "As described in 2021 task paper.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Size of test set: {len(test_raw)}\")\n",
    "print(\"Unchanged from 2015 dataset. Keep split so can compare with both 2015 and 2021 entries.\")\n",
    "print(f\"Size of train set: {len(train_raw)}\")\n",
    "print(\"As described in 2021 task paper.\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "full_raw = train_raw + dev_raw\n",
    "full_norm = train_norm + dev_norm"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No length mismatch (as expected).\n"
     ]
    }
   ],
   "source": [
    "for tweet_raw, tweet_norm in zip(full_raw, full_norm):\n",
    "    if len(tweet_raw) != len(tweet_norm):\n",
    "        print(\"Length mismatch!\")\n",
    "print(\"No length mismatch (as expected).\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN\n",
      "Number of raw tokens: 35216\n",
      "Number of normed tokens: 35598\n",
      "Number of 1 to n normalisation raw tokens: 307\n",
      "Percentage of 1 to n: 0.87\n",
      "Number of n to 1 normalisation raw tokens: 13\n",
      "Percentage of n to 1: 0.04\n",
      "Number of normalised raw tokens: 2666\n",
      "Percentage normalised: 7.570422535211268\n",
      "DEV\n",
      "Number of raw tokens: 9169\n",
      "Number of normed tokens: 9282\n",
      "Number of 1 to n normalisation raw tokens: 98\n",
      "Percentage of 1 to n: 1.07\n",
      "Number of n to 1 normalisation raw tokens: 1\n",
      "Percentage of n to 1: 0.01\n",
      "Number of normalised raw tokens: 633\n",
      "Percentage normalised: 6.903697240702367\n",
      "FULL\n",
      "Number of raw tokens: 44385\n",
      "Number of normed tokens: 44880\n",
      "Number of 1 to n normalisation raw tokens: 405\n",
      "Percentage of 1 to n: 0.91\n",
      "Number of n to 1 normalisation raw tokens: 14\n",
      "Percentage of n to 1: 0.03\n",
      "Number of normalised raw tokens: 3299\n",
      "Percentage normalised: 7.432691224512785\n"
     ]
    }
   ],
   "source": [
    "for name, collection in [(\"TRAIN\", zip(train_raw, train_norm)), (\"DEV\", zip(dev_raw, dev_norm)), (\"FULL\", zip(full_raw, full_norm))]:\n",
    "    print(name)\n",
    "    one_to_n_count = 0\n",
    "    n_to_one_count = 0\n",
    "    raw_count = 0\n",
    "    norm_count = 0\n",
    "    raw_normalised_count = 0\n",
    "    for tweet_raw, tweet_norm in collection:\n",
    "        for token_raw, token_norm in zip(tweet_raw, tweet_norm):\n",
    "            raw_count += 1\n",
    "            norm_count += len(token_norm.split(\" \"))\n",
    "            if not token_norm:\n",
    "                n_to_one_count += 1\n",
    "            if len(token_norm.split(\" \")) > 1:\n",
    "                one_to_n_count += 1\n",
    "            if token_norm != token_raw:\n",
    "                raw_normalised_count += 1\n",
    "    print(f\"Number of raw tokens: {raw_count}\")\n",
    "    print(f\"Number of normed tokens: {norm_count}\")\n",
    "    print(f\"Number of 1 to n normalisation raw tokens: {one_to_n_count}\")\n",
    "    print(f\"Percentage of 1 to n: {one_to_n_count * 100 / raw_count:.2f}\")\n",
    "    print(f\"Number of n to 1 normalisation raw tokens: {n_to_one_count}\")\n",
    "    print(f\"Percentage of n to 1: {n_to_one_count * 100 / raw_count:.2f}\")\n",
    "    print(f\"Number of normalised raw tokens: {raw_normalised_count}\")\n",
    "    print(f\"Percentage normalised: {raw_normalised_count * 100 / raw_count}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For some reason, the stats in the 2021 task paper are on the train set only (correct in that case).\n",
      "Apart from percentage normalised - unclear how this is calculated anyway.\n",
      "Note the 1 to n and n to 1 counts are counting the number of raw tokens involved in the respective normalisations.\n",
      "So a 5 to 1 normalisation would produce a count of 4 (number of tokens merged into first token) for 1 to n, and a 1 to 5 normalisation would produce a count of 1 for n to 1.\n",
      "Note as no capitalisation correction for 2015 dataset, in 2021 dataset version EVERYTHING (RAW AND GOLD) IS LOWER CASE\n",
      "2015 task paper has different statistics, one reason for this being lack of capitalisation consideration in 2021\n"
     ]
    }
   ],
   "source": [
    "print(\"For some reason, the stats in the 2021 task paper are on the train set only (correct in that case).\")\n",
    "print(\"Apart from percentage normalised - unclear how this is calculated anyway.\")\n",
    "print(\"Note the 1 to n and n to 1 counts are counting the number of raw tokens involved in the respective normalisations.\")\n",
    "print(\"So a 5 to 1 normalisation would produce a count of 4 (number of tokens merged into first token) for 1 to n, and a 1 to 5 normalisation would produce a count of 1 for n to 1.\")\n",
    "print(\"Note as no capitalisation correction for 2015 dataset, in 2021 dataset version EVERYTHING (RAW AND GOLD) IS LOWER CASE\")\n",
    "print(\"2015 task paper has different statistics, one reason for this being lack of capitalisation consideration in 2021\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "import json\n",
    "f_train = open(os.path.join(DATA_PATH, \"raw/2015/train_data.json\"))\n",
    "# f_test = open(os.path.join(DATA_PATH, \"raw/2015/test_truth.json\"))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "fif_data = json.load(f_train)\n",
    "# fif_data += json.load(f_test)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 raw differences, 606 norm only differences\n",
      "Most common norm differences: [(('laughing out loud', 'lol'), 271), (('oh my god', 'omg'), 66), (('laughing my ass off', 'lmao'), 51), ((\"i don't know\", 'idk'), 36), (('gonna', 'going to'), 29), (('what the fuck', 'wtf'), 26), (('shaking my head', 'smh'), 21), (('to be honest', 'tbh'), 16), ((\"i don't care\", 'idc'), 15), (('laughing my fucking ass off', 'lmfao'), 13)]\n",
      "Differences in 2015, 2021 raw due to username anonymization\n",
      "Differences in 2015, 2021 gold due to leaving interjections alone e.g. lol, lmfao, ctfu and normalising gonna and wanna\n",
      "Hence make sure models do too to ensure good performance on 2021 set (maybe dict lookup, hard coding?)\n",
      "Could evaluate on both datasets to compare with submissions from both tasks\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "raw_diff = 0\n",
    "norm_diff = 0\n",
    "norm = Counter()\n",
    "for fif, twe_raw, twe_norm in zip(fif_data, full_raw, full_norm):\n",
    "    fif_raw = [x.lower() for x in fif[\"input\"]]\n",
    "    fif_norm = [x.lower() for x in fif[\"output\"]]\n",
    "    if fif_raw != twe_raw:\n",
    "        raw_diff += 1\n",
    "    elif fif_norm != twe_norm:\n",
    "        norm_diff += 1\n",
    "        norm.update((x, y) for x, y in zip(fif_norm, twe_norm) if x != y)\n",
    "print(f\"{raw_diff} raw differences, {norm_diff} norm only differences\")\n",
    "print(f\"Most common norm differences: {norm.most_common(10)}\")\n",
    "print(\"Differences in 2015, 2021 raw due to username anonymization\")\n",
    "print(\"Differences in 2015, 2021 gold due to leaving interjections alone e.g. lol, lmfao, ctfu and normalising gonna and wanna\")\n",
    "print(\"Hence make sure models do too to ensure good performance on 2021 set (maybe dict lookup, hard coding?)\")\n",
    "print(\"Could evaluate on both datasets to compare with submissions from both tasks\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common normalisation pairs: [(('u', 'you'), 328), (('im', \"i'm\"), 181), (('dont', \"don't\"), 92), (('nigga', 'nigger'), 57), (('niggas', 'niggers'), 52), (('n', 'and'), 47), (('pls', 'please'), 43), (('lil', 'little'), 35), (('ur', 'your'), 33), (('thats', \"that's\"), 33)]\n",
      "Most common normalised raw words: [('u', 333), ('im', 182), ('dont', 92), ('nigga', 57), ('niggas', 52), ('n', 49), ('ur', 46), ('pls', 43), ('lil', 35), ('thats', 33)]\n",
      "Think about for candidate generation.\n"
     ]
    }
   ],
   "source": [
    "normalised_pairs = Counter()\n",
    "non_standard_tokens = Counter()\n",
    "\n",
    "for tweet_raw, tweet_norm in zip(full_raw, full_norm):\n",
    "    for token_raw, token_norm in zip(tweet_raw, tweet_norm):\n",
    "        if token_raw != token_norm:\n",
    "            normalised_pairs.update([(token_raw, token_norm)])\n",
    "            non_standard_tokens.update([token_raw])\n",
    "\n",
    "print(f\"Most common normalisation pairs: {normalised_pairs.most_common(10)}\")\n",
    "print(f\"Most common normalised raw words: {non_standard_tokens.most_common(10)}\")\n",
    "print(\"Think about for candidate generation.\")\n",
    "# print(\"Remember this is including the test set - can't use all of this for the normalisation dictionary!\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline acc.(LAI): 92.10\n",
      "Accuracy:           97.23\n",
      "ERR:                64.93\n",
      "As in 2021 paper. For some reason not using dev for training - could fix. Notice the difference in accuracy and ERR.\n"
     ]
    }
   ],
   "source": [
    "normEval.evaluate(test_raw, test_norm, baseline.mfr(train_raw, train_norm, test_raw))\n",
    "print(\"As in 2021 paper. For some reason not using dev for training - could fix. Notice the difference in accuracy and ERR.\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [],
   "source": [
    "american_70 = open(os.path.join(DATA_PATH, \"interim/american-70.txt\"))\n",
    "words = set()\n",
    "for line in american_70:\n",
    "    words.add(line.strip().lower())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCOWL AMERICAN 70\n",
      "Percent raw tokens not in lexicon: 17.72\n",
      "Percent normed tokens not in lexicon: 13.04\n",
      "Phi coefficient between in lexicon and normalisation: 0.31\n",
      "This means there is a moderate relationship between them.\n",
      "Most common un-normalised raw alphanumeric tokens in lexicon: [('rt', 921), ('i', 648), ('the', 631), ('to', 534), ('a', 479), ('and', 411), ('you', 340), ('in', 326), ('for', 320), ('is', 318), ('me', 281), ('my', 278), ('on', 271), ('of', 249), ('it', 203), ('with', 186), ('that', 185), ('this', 180), ('so', 180), ('be', 159)]\n",
      "Not super helpful except note domain specific 'rt' left alone.\n",
      "Most common normalised raw alphanumeric tokens in lexicon: [('u', 333), ('nigga', 57), ('niggas', 52), ('n', 49), ('ur', 46), ('gonna', 29), ('rt', 29), ('r', 24), ('d', 22), ('bout', 21), ('yo', 19), ('b', 18), ('wit', 17), ('tho', 17), ('cause', 16), ('dat', 16), ('bc', 16), ('ya', 16), ('da', 14), ('wanna', 12)]\n",
      "Annotation guidelines for the n-word! May want to remove single letter words from lexicon (apart from v. common ones), and filter out double letter acronyms with an internet acronyms list. Note wanna normalised in 2021, inconsistent rt normalisation.\n",
      "This is important as lookup is used to determine if generated candidate is valid/used as feature for candidate selection.\n",
      "Most common un-normalised raw alphanumeric tokens not in lexicon: [('lol', 272), ('haha', 81), ('omg', 67), ('lmao', 51), ('bae', 45), ('niall', 40), ('2', 40), ('hahaha', 36), ('idk', 36), ('1', 34), ('exo', 31), ('wtf', 26), ('5', 25), ('zayn', 23), ('2014', 23), ('smh', 21), ('3', 21), ('4', 20), ('xd', 16), ('tbh', 16)]\n",
      "Pretty much all interjections and some common names (one direction - time specific). May want to expand lexicon in these areas - have to generate as candidates to get correct!. Hard code what to leave alone?\n",
      "Most common normalised raw alphanumeric tokens not in lexicon: [('im', 182), ('dont', 92), ('pls', 43), ('lil', 35), ('thats', 33), ('bruh', 31), ('aint', 30), ('ima', 25), ('ppl', 25), ('yall', 23), ('cuz', 22), ('didnt', 19), ('fav', 19), ('gon', 18), ('tryna', 18), ('imma', 18), ('goin', 16), ('ive', 16), ('2', 14), ('favourite', 13)]\n",
      "A lot of missing apostrophes - think about for candidate generation.\n",
      "A good lexicon (high correlation between in lexicon and normalised) will be good for checking validity of generated candidates - not suggesting candidates that wouldn't be considered normalised, not rejecting ones that would be (obviously on individual word basis). Size offers tradeoff between former and latter.\n",
      "WANT: to either expand lexicon or hard code to reduce un-normalised non-lexical, so that non-lexical->normalised. to reduce lexicon to reduce normalised lexical, so that lexical->non-normalised. Of course there will always be OOV words not requiring normalisation, e.g. unknown/novel named entities, and IV words requiring normalisation, e.g. where misspelling of other words etc.\n",
      "Most common raw tokens not in lexicon: [('lol', 272), ('im', 182), ('dont', 92), ('haha', 81), ('omg', 67), ('2', 54), ('lmao', 51), ('bae', 45), ('pls', 43), ('niall', 40)]\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "raw_non_lexical = Counter()\n",
    "norm_non_lexical = Counter()\n",
    "changed_non_lexical = Counter()\n",
    "unchanged_non_lexical = Counter()\n",
    "changed_lexical = Counter()\n",
    "unchanged_lexical = Counter()\n",
    "\n",
    "for twe_raw, twe_norm in zip(full_raw, full_norm):\n",
    "    for token in twe_raw:\n",
    "        if token.isalnum() and token not in words:\n",
    "            raw_non_lexical.update([token])\n",
    "    for norm in twe_norm:\n",
    "        for token in norm.split():\n",
    "            if token.isalnum() and token not in words:\n",
    "                norm_non_lexical.update([token])\n",
    "    for raw, norm in zip(twe_raw, twe_norm):\n",
    "        if raw.isalnum():\n",
    "            if raw in words:\n",
    "                if raw == norm:\n",
    "                    unchanged_lexical.update([raw])\n",
    "                else:\n",
    "                    changed_lexical.update([raw])\n",
    "            elif raw not in words:\n",
    "                if raw == norm:\n",
    "                    unchanged_non_lexical.update([raw])\n",
    "                else:\n",
    "                    changed_non_lexical.update([raw])\n",
    "print(\"SCOWL AMERICAN 70\")\n",
    "print(f\"Percent raw tokens not in lexicon: {sum(raw_non_lexical.values())*100/raw_count:.2f}\")\n",
    "print(f\"Percent normed tokens not in lexicon: {sum(norm_non_lexical.values())*100/raw_count:.2f}\")\n",
    "a = sum(unchanged_lexical.values())\n",
    "b = sum(changed_lexical.values())\n",
    "c = sum(unchanged_non_lexical.values())\n",
    "d = sum(changed_non_lexical.values())\n",
    "phi = (a * d - b * c) / math.sqrt((a+b)*(b+d)*(a+c)*(c+d))\n",
    "print(f\"Phi coefficient between in lexicon and normalisation: {phi:.2f}\")\n",
    "print(\"This means there is a moderate relationship between them.\")\n",
    "print(f\"Most common un-normalised raw alphanumeric tokens in lexicon: {unchanged_lexical.most_common(20)}\")\n",
    "print(\"Not super helpful except note domain specific 'rt' left alone.\")\n",
    "print(f\"Most common normalised raw alphanumeric tokens in lexicon: {changed_lexical.most_common(20)}\")\n",
    "print(\"Annotation guidelines for the n-word! May want to remove single letter words from lexicon (apart from v. common ones), and filter out double letter acronyms with an internet acronyms list. Note wanna normalised in 2021, inconsistent rt normalisation.\")\n",
    "print(\"This is important as lookup is used to determine if generated candidate is valid/used as feature for candidate selection.\")\n",
    "print(f\"Most common un-normalised raw alphanumeric tokens not in lexicon: {unchanged_non_lexical.most_common(20)}\")\n",
    "print(\"Pretty much all interjections and some common names (one direction - time specific). May want to expand lexicon in these areas - have to generate as candidates to get correct!. Hard code what to leave alone?\")\n",
    "print(f\"Most common normalised raw alphanumeric tokens not in lexicon: {changed_non_lexical.most_common(20)}\")\n",
    "print(\"A lot of missing apostrophes - think about for candidate generation.\")\n",
    "print(\"A good lexicon (high correlation between in lexicon and normalised) will be good for checking validity of generated candidates - not suggesting candidates that wouldn't be considered normalised, not rejecting ones that would be (obviously on individual word basis). Size offers tradeoff between former and latter.\")\n",
    "print(\"WANT: to either expand lexicon or hard code to reduce un-normalised non-lexical, so that non-lexical->normalised. to reduce lexicon to reduce normalised lexical, so that lexical->non-normalised. Of course there will always be OOV words not requiring normalisation, e.g. unknown/novel named entities, and IV words requiring normalisation, e.g. where misspelling of other words etc.\")\n",
    "print(f\"Most common raw tokens not in lexicon: {raw_non_lexical.most_common(10)}\")\n",
    "# print(f\"Most common norm tokens not in lexicon: {norm_non_lexical.most_common(10)}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [],
   "source": [
    "# def if_normed(raw_list, norm_list):\n",
    "#     resp = []\n",
    "#     for raw_tweet, norm_tweet in zip(raw_list, norm_list):\n",
    "#         resp_tweet = []\n",
    "#         for raw_tok, norm_tok in zip(raw_tweet, norm_tweet):\n",
    "#             if raw_tok == norm_tok:\n",
    "#                 resp_tweet.append((raw_tok, \"\"))\n",
    "#             else:\n",
    "#                 resp_tweet.append((raw_tok, norm_tok))\n",
    "#         resp.append(resp_tweet)\n",
    "#     return resp\n",
    "#\n",
    "# full_if_normed = if_normed(full_raw, full_norm)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [],
   "source": [
    "def norm_condition(raw_list, norm_list, condition, pair=False, norm=False):\n",
    "    \"\"\"\n",
    "    Return counter for each quadrant of normalised and condition.\n",
    "    pair option returns the normalisation pair if True\n",
    "    norm option evaluates the condition on the normed rather than the raw token if True\n",
    "    \"\"\"\n",
    "    p_normed = Counter()\n",
    "    p_unnormed = Counter()\n",
    "    n_normed = Counter()\n",
    "    n_unnormed = Counter()\n",
    "    for tweet_raw, tweet_norm in zip(raw_list, norm_list):\n",
    "        for raw, normed in zip(tweet_raw, tweet_norm):\n",
    "            tok = normed if norm else raw\n",
    "            to_update = (tok, normed) if pair else tok\n",
    "            if raw != normed and condition(tok):\n",
    "                p_normed.update([to_update])\n",
    "            elif raw == normed and condition(tok):\n",
    "                p_unnormed.update([to_update])\n",
    "            elif raw != normed and not condition(tok):\n",
    "                n_normed.update([to_update])\n",
    "            else:\n",
    "                n_unnormed.update([to_update])\n",
    "    return p_unnormed, p_normed, n_unnormed, n_normed\n",
    "\n",
    "def correlation_with_norm(p_unnormed, p_normed, n_unnormed, n_normed):\n",
    "    \"\"\"\n",
    "    Calculate phi correlation coefficient between normalisation and a condition (two binary variables)\n",
    "    \"\"\"\n",
    "    a = sum(p_unnormed.values())\n",
    "    b = sum(p_normed.values())\n",
    "    c = sum(n_unnormed.values())\n",
    "    d = sum(n_normed.values())\n",
    "    phi = (a * d - b * c) / math.sqrt((a+b)*(b+d)*(a+c)*(c+d))\n",
    "    return phi\n",
    "\n",
    "# a, b, c, d = norm_condition([[tok for tok in tweet if tok[0].isalnum()] for tweet in full_if_normed], lambda x: x in words)\n",
    "# correlation_with_norm(a, b, c, d)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Only 4 non-alphanumeric raw tokens are normalised: [((\"good'o\", 'good'), 1), ((\"could've\", 'could have'), 1), ((\"should've\", 'should have'), 1), ((\"ca'nt\", \"can't\"), 1)]. These all contain apostrophes, which are included as not invalidating tokens for normalisation. However, could've and should've should not have been normalised under 2015 annotation rule 9. This rule also makes sense as expanding could change the formality of the text. ca'nt has the apostrophe in the wrong place. Perhaps the contraction module could handle moving apostrophes as well as inserting to get a IV word. Modules should only be able to add candidates, not remove them. Unclear what is happening in good'o but perhaps could hypothesise drop/split before/after the apostrophe? The alphanumeric filter conveniently also filters out domain specific entities (hashtags and mentions).\n"
     ]
    }
   ],
   "source": [
    "_, b, _, _= norm_condition(full_raw, full_norm, lambda x: not x.isalnum(), True, False)\n",
    "print(f\"Only {sum(b.values())} non-alphanumeric raw tokens are normalised: {b.most_common()}. These all contain apostrophes, which are included as not invalidating tokens for normalisation. However, could've and should've should not have been normalised under 2015 annotation rule 9. This rule also makes sense as expanding could change the formality of the text. ca'nt has the apostrophe in the wrong place. Perhaps the contraction module could handle moving apostrophes as well as inserting to get a IV word. Modules should only be able to add candidates, not remove them. Unclear what is happening in good'o but perhaps could hypothesise drop/split before/after the apostrophe? The alphanumeric filter conveniently also filters out domain specific entities (hashtags and mentions).\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 0 raw tokens containing whitespace.\n"
     ]
    }
   ],
   "source": [
    "a, b, _, _ = norm_condition(full_raw, full_norm, lambda x: any(char.isspace() for char in x), True)\n",
    "print(f\"There are {sum((a+b).values())} raw tokens containing whitespace.\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 950 rt tokens total, with 921 left alone. rt at the start of a tweet is a domain specific entity and thus should be disregarded as per rule 3. In the middle of a tweet, this is more debatable. There are 860 rt tokens at the start of tweets, which are all left alone. Middle rts (85) are inconsistently normalised (27 normalised), but if they are normalised, this is to retweet. From observation looks like domain-specific rts are always followed by a @mention. 24/27 normalised middle rts are not followed by @, 42/58 left alone middle rts are. There are 6 rts at the end of tweets, and they are inconsistently normalised. So perhaps rule could be: if at start of tweet or followed by @mention, ignore. Otherwise normalise to retweet.\n"
     ]
    }
   ],
   "source": [
    "a, b, _, _ = norm_condition(full_raw, full_norm, lambda x: x == \"rt\", True)\n",
    "print(f\"There are {sum((a+b).values())} rt tokens total, with {sum(a.values())} left alone. rt at the start of a tweet is a domain specific entity and thus should be disregarded as per rule 3. In the middle of a tweet, this is more debatable. There are {sum(1 if tweet[0] == 'rt' else 0 for tweet in full_raw)} rt tokens at the start of tweets, which are all left alone. Middle rts (85) are inconsistently normalised (27 normalised), but if they are normalised, this is to retweet. From observation looks like domain-specific rts are always followed by a @mention. 24/27 normalised middle rts are not followed by @, 42/58 left alone middle rts are. There are 6 rts at the end of tweets, and they are inconsistently normalised. So perhaps rule could be: if at start of tweet or followed by @mention, ignore. Otherwise normalise to retweet.\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "outputs": [],
   "source": [
    "# count = 0\n",
    "# for tweet_raw, tweet_norm in zip(full_raw, full_norm):\n",
    "#     for i, tok in enumerate(zip(tweet_raw, tweet_norm)):\n",
    "#         tok_raw, tok_norm = tok\n",
    "#         # if tok_raw == 'rt' and 0 < i < len(tweet_raw)-1 and tok_raw == tok_norm:\n",
    "#         if tok_raw == 'rt' and i == len(tweet_raw)-1:\n",
    "#             # print(tweet_raw[i+1])\n",
    "#             print(tok_norm)\n",
    "#             count += 1\n",
    "# print(count)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "outputs": [],
   "source": [
    "spacy_full_raw = []\n",
    "for tweet in full_raw:\n",
    "    spacy_full_raw.append(nlp(\" \".join(tweet)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "outputs": [],
   "source": [
    "# for tweet in full_if_normed:\n",
    "#     for tok, norm in tweet:\n",
    "#         dok = nlp(tok)[0]\n",
    "#         type = dok.pos_\n",
    "#         if type == \"PROPN\":\n",
    "#             print(tok, norm)\n",
    "\n",
    "a, b, c, d = norm_condition(full_raw, full_norm, lambda x: nlp(x)[0].pos_ == \"PROPN\", True, False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples of unnormalised words tagged as PROPN not in lexicon: ['ozil', 'bie', 'haha', 'avedon', 'pshh', 'hahaha', 'bria', 'exo', 'chanyeol', 'sehun']\n",
      "Correlation of PROPN and normalisation -0.09. Very low - probably due to bad POS performance thanks to lower casing and other irregularities of the text - sort of a chicken and egg issue with downstream processing. A lot these could probably be found by expanding the lexicon instead - looking at POS tagging may be a red herring\n"
     ]
    }
   ],
   "source": [
    "# print(f\"{sum(b.values())}/{sum(d.values())+sum(b.values())}\")\n",
    "print(f\"Examples of unnormalised words tagged as PROPN not in lexicon: {[k[0] for k in a.keys() if k[0] not in words and k[0].isalnum()][:10]}\")\n",
    "print(f\"Correlation of PROPN and normalisation: {correlation_with_norm(a, b, c, d):.2f}. Very low - probably due to bad POS performance thanks to lower casing and other irregularities of the text - sort of a chicken and egg issue with downstream processing. A lot these could probably be found by expanding the lexicon instead - looking at POS tagging may be a red herring\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "outputs": [],
   "source": [
    "a, b, c, d = norm_condition(full_raw, full_norm, lambda x: nlp(x)[0].ent_type_ in ['LOC', 'PERSON'], True, False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples of unnormalised words with these entity types not in lexicon: ['grecia', 'directionas', 'homie', 'jimin', 'simbrinz', 'lantak', 'kakak', 'dubb', 'hala', 'wala']\n",
      "Correlation of normalisation and these entity types: -0.00. Zero correlation, probably due to the same reasons as above.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Examples of unnormalised words with these entity types not in lexicon: {[k[0] for k in a.keys() if k[0] not in words and k[0].isalnum()][:10]}\")\n",
    "print(f\"Correlation of normalisation and these entity types: {correlation_with_norm(a, b, c, d):.2f}. Zero correlation, probably due to the same reasons as above.\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "outputs": [],
   "source": [
    "def alphanum(raw_list, norm_list):\n",
    "    raw_resp, norm_resp = [], []\n",
    "    for raw_tweet, norm_tweet in zip(raw_list, norm_list):\n",
    "        raw_resp_tweet, norm_resp_tweet = [], []\n",
    "        for raw_tok, norm_tok in zip(raw_tweet, norm_tweet):\n",
    "            if raw_tok.isalnum() and raw_tok != 'rt':\n",
    "                raw_resp_tweet.append(raw_tok)\n",
    "                norm_resp_tweet.append(norm_tok)\n",
    "        raw_resp.append(raw_resp_tweet)\n",
    "        norm_resp.append(norm_resp_tweet)\n",
    "    return raw_resp, norm_resp\n",
    "\n",
    "def lexicon_investigate(raw_list, norm_list, lex):\n",
    "    \"\"\"\n",
    "    Gives statistics for a given lexicon\n",
    "    \"\"\"\n",
    "    a, b, c, d = norm_condition(*alphanum(raw_list, norm_list), lambda x: x in lex, pair=True)\n",
    "    print(f\"Correlation of lexicon with normalisation: {correlation_with_norm(a, b, c, d):.2f}\")\n",
    "    print(f\"Most common un-normalised raw alphanumeric tokens in lexicon: {a.most_common(20)}\")\n",
    "    print(f\"Most common normalised raw alphanumeric tokens in lexicon: {b.most_common(20)}\")\n",
    "    print(f\"Most common un-normalised raw alphanumeric tokens not in lexicon: {c.most_common(20)}\")\n",
    "    print(f\"Most common normalised raw alphanumeric tokens not in lexicon: {d.most_common(20)}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation of lexicon with normalisation: 0.31\n",
      "Most common un-normalised raw alphanumeric tokens in lexicon: [(('i', 'i'), 648), (('the', 'the'), 631), (('to', 'to'), 534), (('a', 'a'), 479), (('and', 'and'), 411), (('you', 'you'), 340), (('in', 'in'), 326), (('for', 'for'), 320), (('is', 'is'), 318), (('me', 'me'), 281), (('my', 'my'), 278), (('on', 'on'), 271), (('of', 'of'), 249), (('it', 'it'), 203), (('with', 'with'), 186), (('that', 'that'), 185), (('this', 'this'), 180), (('so', 'so'), 180), (('be', 'be'), 159), (('like', 'like'), 156)]\n",
      "Most common normalised raw alphanumeric tokens in lexicon: [(('u', 'you'), 328), (('nigga', 'nigger'), 57), (('niggas', 'niggers'), 52), (('n', 'and'), 47), (('ur', 'your'), 33), (('gonna', 'going to'), 29), (('r', 'are'), 22), (('d', 'the'), 22), (('bout', 'about'), 21), (('b', 'be'), 17), (('wit', 'with'), 17), (('tho', 'though'), 17), (('cause', 'because'), 16), (('dat', 'that'), 16), (('bc', 'because'), 16), (('ya', 'you'), 15), (('da', 'the'), 13), (('wanna', 'want to'), 12), (('cant', \"can't\"), 12), (('congrats', 'congratulations'), 12)]\n",
      "Most common un-normalised raw alphanumeric tokens not in lexicon: [(('lol', 'lol'), 272), (('haha', 'haha'), 81), (('omg', 'omg'), 67), (('lmao', 'lmao'), 51), (('bae', 'bae'), 45), (('niall', 'niall'), 40), (('2', '2'), 40), (('hahaha', 'hahaha'), 36), (('idk', 'idk'), 36), (('1', '1'), 34), (('exo', 'exo'), 31), (('wtf', 'wtf'), 26), (('5', '5'), 25), (('zayn', 'zayn'), 23), (('2014', '2014'), 23), (('smh', 'smh'), 21), (('3', '3'), 21), (('4', '4'), 20), (('xd', 'xd'), 16), (('tbh', 'tbh'), 16)]\n",
      "Most common normalised raw alphanumeric tokens not in lexicon: [(('im', \"i'm\"), 181), (('dont', \"don't\"), 92), (('pls', 'please'), 43), (('lil', 'little'), 35), (('thats', \"that's\"), 33), (('bruh', 'brother'), 31), (('aint', \"ain't\"), 30), (('ima', \"i'm going to\"), 25), (('ppl', 'people'), 25), (('yall', \"y'all\"), 23), (('cuz', 'because'), 22), (('didnt', \"didn't\"), 19), (('gon', 'gonna'), 18), (('tryna', 'trying to'), 18), (('imma', \"i'm going to\"), 18), (('fav', 'favorite'), 16), (('goin', 'going'), 16), (('ive', \"i've\"), 16), (('2', 'to'), 14), (('favourite', 'favorite'), 13)]\n"
     ]
    }
   ],
   "source": [
    "lexicon_investigate(full_raw, full_norm, words)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation of lexicon with normalisation: 0.40\n",
      "Most common un-normalised raw alphanumeric tokens in lexicon: [(('i', 'i'), 648), (('the', 'the'), 631), (('to', 'to'), 534), (('a', 'a'), 479), (('and', 'and'), 411), (('you', 'you'), 340), (('in', 'in'), 326), (('for', 'for'), 320), (('is', 'is'), 318), (('me', 'me'), 281), (('my', 'my'), 278), (('on', 'on'), 271), (('of', 'of'), 249), (('it', 'it'), 203), (('with', 'with'), 186), (('that', 'that'), 185), (('this', 'this'), 180), (('so', 'so'), 180), (('be', 'be'), 159), (('like', 'like'), 156)]\n",
      "Most common normalised raw alphanumeric tokens in lexicon: [(('nigga', 'nigger'), 57), (('niggas', 'niggers'), 52), (('ur', 'your'), 33), (('gonna', 'going to'), 29), (('bout', 'about'), 21), (('wit', 'with'), 17), (('tho', 'though'), 17), (('cause', 'because'), 16), (('dat', 'that'), 16), (('bc', 'because'), 16), (('ya', 'you'), 15), (('da', 'the'), 13), (('wanna', 'want to'), 12), (('cant', \"can't\"), 12), (('congrats', 'congratulations'), 12), (('ur', \"you're\"), 12), (('yo', 'you'), 11), (('nd', 'and'), 11), (('pic', 'picture'), 11), (('dm', 'direct message'), 10)]\n",
      "Most common un-normalised raw alphanumeric tokens not in lexicon: [(('lol', 'lol'), 272), (('haha', 'haha'), 81), (('omg', 'omg'), 67), (('lmao', 'lmao'), 51), (('bae', 'bae'), 45), (('x', 'x'), 44), (('niall', 'niall'), 40), (('2', '2'), 40), (('hahaha', 'hahaha'), 36), (('idk', 'idk'), 36), (('1', '1'), 34), (('exo', 'exo'), 31), (('wtf', 'wtf'), 26), (('t', 't'), 25), (('5', '5'), 25), (('zayn', 'zayn'), 23), (('2014', '2014'), 23), (('smh', 'smh'), 21), (('c', 'c'), 21), (('b', 'b'), 21)]\n",
      "Most common normalised raw alphanumeric tokens not in lexicon: [(('u', 'you'), 328), (('im', \"i'm\"), 181), (('dont', \"don't\"), 92), (('n', 'and'), 47), (('pls', 'please'), 43), (('lil', 'little'), 35), (('thats', \"that's\"), 33), (('bruh', 'brother'), 31), (('aint', \"ain't\"), 30), (('ima', \"i'm going to\"), 25), (('ppl', 'people'), 25), (('yall', \"y'all\"), 23), (('cuz', 'because'), 22), (('r', 'are'), 22), (('d', 'the'), 22), (('didnt', \"didn't\"), 19), (('gon', 'gonna'), 18), (('tryna', 'trying to'), 18), (('imma', \"i'm going to\"), 18), (('b', 'be'), 17)]\n",
      "STRONG CORRELATION!\n",
      "So filter lexicon by removing all single letter words but a and i\n",
      "Filter tokens by ignoring some rts, alphanumeric/apostrophe only\n"
     ]
    }
   ],
   "source": [
    "lexicon_file = open(os.path.join(DATA_PATH, \"interim/american-70.txt\"))\n",
    "lexicon = set()\n",
    "for line in lexicon_file:\n",
    "    word = line.strip().lower()\n",
    "    if len(word) > 1 or word in ['a', 'i']:\n",
    "        lexicon.add(word)\n",
    "lexicon_investigate(full_raw, full_norm, lexicon)\n",
    "print(\"STRONG CORRELATION!\")\n",
    "print(\"So filter lexicon by removing all single letter words but a and i\")\n",
    "print(\"Filter tokens by ignoring some rts, alphanumeric/apostrophe only\")"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
