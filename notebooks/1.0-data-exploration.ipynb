{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from lexnorm.data import normEval\n",
    "from lexnorm.data import baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "train_raw, train_norm = normEval.loadNormData(os.path.abspath('../data/raw/train.norm'))\n",
    "test_raw, test_norm = normEval.loadNormData(os.path.abspath('../data/raw/test.norm'))\n",
    "dev_raw, dev_norm = normEval.loadNormData(os.path.abspath('../data/raw/dev.norm'))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of tweets: 4917\n",
      "Total number of normed tweets: 4917\n",
      "Unchanged from 2015 dataset.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total number of tweets: {len(train_raw) + len(test_raw) + len(dev_raw)}\")\n",
    "print(f\"Total number of normed tweets: {len(train_norm) + len(test_norm) + len(dev_norm)}\")\n",
    "print(\"Unchanged from 2015 dataset.\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of test set: 1967\n",
      "Unchanged from 2015 dataset. Keep split so can compare with both 2015 and 2021 entries.\n",
      "Size of train set: 2360\n",
      "As described in 2021 task paper.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Size of test set: {len(test_raw)}\")\n",
    "print(\"Unchanged from 2015 dataset. Keep split so can compare with both 2015 and 2021 entries.\")\n",
    "print(f\"Size of train set: {len(train_raw)}\")\n",
    "print(\"As described in 2021 task paper.\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "full_raw = train_raw + test_raw + dev_raw\n",
    "full_norm = train_norm + test_norm + dev_norm"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No length mismatch (as expected).\n"
     ]
    }
   ],
   "source": [
    "for tweet_raw, tweet_norm in zip(full_raw, full_norm):\n",
    "    if len(tweet_raw) != len(tweet_norm):\n",
    "        print(\"Length mismatch!\")\n",
    "print(\"No length mismatch (as expected).\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN\n",
      "Number of raw tokens: 35216\n",
      "Number of normed tokens: 35598\n",
      "Number of 1 to n normalisation raw tokens: 307\n",
      "Percentage of 1 to n: 0.87\n",
      "Number of n to 1 normalisation raw tokens: 13\n",
      "Percentage of n to 1: 0.04\n",
      "Number of normalised raw tokens: 2666\n",
      "Percentage normalised: 7.570422535211268\n",
      "DEV\n",
      "Number of raw tokens: 9169\n",
      "Number of normed tokens: 9282\n",
      "Number of 1 to n normalisation raw tokens: 98\n",
      "Percentage of 1 to n: 1.07\n",
      "Number of n to 1 normalisation raw tokens: 1\n",
      "Percentage of n to 1: 0.01\n",
      "Number of normalised raw tokens: 633\n",
      "Percentage normalised: 6.903697240702367\n",
      "TEST\n",
      "Number of raw tokens: 29421\n",
      "Number of normed tokens: 29738\n",
      "Number of 1 to n normalisation raw tokens: 262\n",
      "Percentage of 1 to n: 0.89\n",
      "Number of n to 1 normalisation raw tokens: 17\n",
      "Percentage of n to 1: 0.06\n",
      "Number of normalised raw tokens: 2324\n",
      "Percentage normalised: 7.899119676421604\n",
      "ALL\n",
      "Number of raw tokens: 73806\n",
      "Number of normed tokens: 74618\n",
      "Number of 1 to n normalisation raw tokens: 667\n",
      "Percentage of 1 to n: 0.90\n",
      "Number of n to 1 normalisation raw tokens: 31\n",
      "Percentage of n to 1: 0.04\n",
      "Number of normalised raw tokens: 5623\n",
      "Percentage normalised: 7.618621792266212\n"
     ]
    }
   ],
   "source": [
    "for name, collection in [(\"TRAIN\", zip(train_raw, train_norm)), (\"DEV\", zip(dev_raw, dev_norm)), (\"TEST\", zip(test_raw, test_norm)), (\"ALL\", zip(full_raw, full_norm))]:\n",
    "    print(name)\n",
    "    one_to_n_count = 0\n",
    "    n_to_one_count = 0\n",
    "    raw_count = 0\n",
    "    norm_count = 0\n",
    "    raw_normalised_count = 0\n",
    "    for tweet_raw, tweet_norm in collection:\n",
    "        for token_raw, token_norm in zip(tweet_raw, tweet_norm):\n",
    "            raw_count += 1\n",
    "            norm_count += len(token_norm.split(\" \"))\n",
    "            if not token_norm:\n",
    "                n_to_one_count += 1\n",
    "            if len(token_norm.split(\" \")) > 1:\n",
    "                one_to_n_count += 1\n",
    "            if token_norm != token_raw:\n",
    "                raw_normalised_count += 1\n",
    "    print(f\"Number of raw tokens: {raw_count}\")\n",
    "    print(f\"Number of normed tokens: {norm_count}\")\n",
    "    print(f\"Number of 1 to n normalisation raw tokens: {one_to_n_count}\")\n",
    "    print(f\"Percentage of 1 to n: {one_to_n_count * 100 / raw_count:.2f}\")\n",
    "    print(f\"Number of n to 1 normalisation raw tokens: {n_to_one_count}\")\n",
    "    print(f\"Percentage of n to 1: {n_to_one_count * 100 / raw_count:.2f}\")\n",
    "    print(f\"Number of normalised raw tokens: {raw_normalised_count}\")\n",
    "    print(f\"Percentage normalised: {raw_normalised_count * 100 / raw_count}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For some reason, the stats in the 2021 task paper are on the train set only (correct in that case).\n",
      "Apart from percentage change - unclear how this is calculated anyway.\n",
      "Note the 1 to n and n to 1 counts are counting the number of raw tokens involved in the respective normalisations.\n",
      "So a 5 to 1 normalisation would produce a count of 4 (number of tokens merged into first token) for 1 to n, and a 1 to 5 normalisation would produce a count of 1 for n to 1.\n",
      "Note as no capitalisation correction for 2015 dataset, in 2021 dataset version EVERYTHING (RAW AND GOLD) IS LOWER CASE\n",
      "2015 task paper has different statistics, one reason for this being lack of capitalisation consideration in 2021\n"
     ]
    }
   ],
   "source": [
    "print(\"For some reason, the stats in the 2021 task paper are on the train set only (correct in that case).\")\n",
    "print(\"Apart from percentage change - unclear how this is calculated anyway.\")\n",
    "print(\"Note the 1 to n and n to 1 counts are counting the number of raw tokens involved in the respective normalisations.\")\n",
    "print(\"So a 5 to 1 normalisation would produce a count of 4 (number of tokens merged into first token) for 1 to n, and a 1 to 5 normalisation would produce a count of 1 for n to 1.\")\n",
    "print(\"Note as no capitalisation correction for 2015 dataset, in 2021 dataset version EVERYTHING (RAW AND GOLD) IS LOWER CASE\")\n",
    "print(\"2015 task paper has different statistics, one reason for this being lack of capitalisation consideration in 2021\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "import json\n",
    "fif_train = open(os.path.abspath(\"../data/raw/2015/train_data.json\"))\n",
    "# fif_test = open(os.path.join(DATA_PATH, \"2015/test_truth.json\"))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "fif_data = json.load(fif_train)\n",
    "# fif_data += json.load(fif_test)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR ORIGINAL TRAIN SPLIT:\n",
      "16 raw differences, 477 norm only differences\n",
      "Most common norm differences: [('lol', 204), ('omg', 49), ('lmao', 38), ('idk', 28), ('going to', 22), ('smh', 19), ('wtf', 16), ('tbh', 14), ('idc', 14), ('omfg', 9)]\n",
      "Differences in 2015, 2021 raw due to username anonymization\n",
      "Differences in 2015, 2021 gold due to leaving interjections alone e.g. lol, lmfao, ctfu and normalising gonna and wanna\n",
      "Hence make sure models do too to ensure good performance on 2021 set (maybe dict lookup, hard coding?)\n",
      "Could evaluate on both datasets to compare with submissions from both tasks\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "raw_diff = 0\n",
    "norm_diff = 0\n",
    "norm = Counter()\n",
    "print(\"FOR ORIGINAL TRAIN SPLIT:\")\n",
    "for fif, twe_raw, twe_norm in zip(fif_data, train_raw, train_norm):\n",
    "    fif_raw = [x.lower() for x in fif[\"input\"]]\n",
    "    fif_norm = [x.lower() for x in fif[\"output\"]]\n",
    "    if fif_raw != twe_raw:\n",
    "        raw_diff += 1\n",
    "    elif fif_norm != twe_norm:\n",
    "        norm_diff += 1\n",
    "        norm.update(y for x, y in zip(fif_norm, twe_norm) if x != y)\n",
    "print(f\"{raw_diff} raw differences, {norm_diff} norm only differences\")\n",
    "print(f\"Most common norm differences: {norm.most_common(10)}\")\n",
    "print(\"Differences in 2015, 2021 raw due to username anonymization\")\n",
    "print(\"Differences in 2015, 2021 gold due to leaving interjections alone e.g. lol, lmfao, ctfu and normalising gonna and wanna\")\n",
    "print(\"Hence make sure models do too to ensure good performance on 2021 set (maybe dict lookup, hard coding?)\")\n",
    "print(\"Could evaluate on both datasets to compare with submissions from both tasks\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(('u', 'you'), 562), (('im', \"i'm\"), 334), (('dont', \"don't\"), 149), (('nigga', 'nigger'), 117), (('niggas', 'niggers'), 93), (('n', 'and'), 89), (('pls', 'please'), 68), (('lil', 'little'), 62), (('ur', 'your'), 54), (('thats', \"that's\"), 54)]\n",
      "[('u', 569), ('im', 336), ('dont', 149), ('nigga', 117), ('niggas', 94), ('n', 93), ('ur', 74), ('pls', 68), ('lil', 62), ('thats', 54)]\n",
      "Remember this is including the test set - can't use all of this for the normalisation dictionary!\n"
     ]
    }
   ],
   "source": [
    "normalised_pairs = Counter()\n",
    "non_standard_tokens = Counter()\n",
    "\n",
    "for tweet_raw, tweet_norm in zip(full_raw, full_norm):\n",
    "    for token_raw, token_norm in zip(tweet_raw, tweet_norm):\n",
    "        if token_raw != token_norm:\n",
    "            normalised_pairs.update([(token_raw, token_norm)])\n",
    "            non_standard_tokens.update([token_raw])\n",
    "\n",
    "print(normalised_pairs.most_common(10))\n",
    "print(non_standard_tokens.most_common(10))\n",
    "print(\"Remember this is including the test set - can't use all of this for the normalisation dictionary!\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline acc.(LAI): 92.10\n",
      "Accuracy:           97.23\n",
      "ERR:                64.93\n",
      "As in 2021 paper. For some reason not using dev for training - could fix.\n"
     ]
    }
   ],
   "source": [
    "normEval.evaluate(test_raw, test_norm, baseline.mfr(train_raw, train_norm, test_raw))\n",
    "print(\"As in 2021 paper. For some reason not using dev for training - could fix.\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "from lexnorm.data import make_train"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected str, bytes or os.PathLike object, not int",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[15], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mmake_train\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconcatenate\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Library/Mobile Documents/com~apple~CloudDocs/Documents/2/Diss/lexnorm/src/lexnorm/data/make_train.py:11\u001B[0m, in \u001B[0;36mconcatenate\u001B[0;34m(input1, input2, output)\u001B[0m\n\u001B[1;32m      7\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mconcatenate\u001B[39m(input1, input2, output):\n\u001B[1;32m      8\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m      9\u001B[0m \u001B[38;5;124;03m    concatenate two files in W-NUT 2021 format\u001B[39;00m\n\u001B[1;32m     10\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m---> 11\u001B[0m     train_raw, train_norm \u001B[38;5;241m=\u001B[39m loadNormData(\u001B[43mos\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpath\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mabspath\u001B[49m\u001B[43m(\u001B[49m\u001B[43minput1\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[1;32m     12\u001B[0m     dev_raw, dev_norm \u001B[38;5;241m=\u001B[39m loadNormData(os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mabspath(input2))\n\u001B[1;32m     13\u001B[0m     write(\n\u001B[1;32m     14\u001B[0m         train_raw \u001B[38;5;241m+\u001B[39m dev_raw,\n\u001B[1;32m     15\u001B[0m         train_norm \u001B[38;5;241m+\u001B[39m dev_norm,\n\u001B[1;32m     16\u001B[0m         os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mabspath(output)),\n\u001B[1;32m     17\u001B[0m     )\n",
      "File \u001B[0;32m/usr/local/Cellar/python@3.10/3.10.8/Frameworks/Python.framework/Versions/3.10/lib/python3.10/posixpath.py:378\u001B[0m, in \u001B[0;36mabspath\u001B[0;34m(path)\u001B[0m\n\u001B[1;32m    376\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mabspath\u001B[39m(path):\n\u001B[1;32m    377\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Return an absolute path.\"\"\"\u001B[39;00m\n\u001B[0;32m--> 378\u001B[0m     path \u001B[38;5;241m=\u001B[39m \u001B[43mos\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfspath\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    379\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m isabs(path):\n\u001B[1;32m    380\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(path, \u001B[38;5;28mbytes\u001B[39m):\n",
      "\u001B[0;31mTypeError\u001B[0m: expected str, bytes or os.PathLike object, not int"
     ]
    }
   ],
   "source": [
    "make_train.concatenate(2, 2, 2)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
