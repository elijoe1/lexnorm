{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "import lexnorm.models.normalise\n",
    "from lexnorm.definitions import DATA_PATH\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "train = pd.read_csv(os.path.join(DATA_PATH, \"hpc/train_annotated.txt\"), index_col=0)\n",
    "dev = pd.read_csv(os.path.join(DATA_PATH, \"hpc/dev_unannotated.txt\"), index_col=0)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "outputs": [
    {
     "data": {
      "text/plain": "      cosine_to_orig  embeddings_rank  from_clipping  from_original_token  \\\nan          0.377180              NaN            NaN                  NaN   \nca          0.255824              NaN            NaN                  NaN   \ncab         0.261057              NaN            NaN                  NaN   \ncam         0.299136              NaN            NaN                  NaN   \ncan         1.000000              NaN            1.0                  1.0   \n...              ...              ...            ...                  ...   \nlulu        0.401415              NaN            NaN                  NaN   \npol         0.230619              NaN            NaN                  NaN   \nsmh         0.896277              1.0            NaN                  NaN   \nsol         0.285978              NaN            NaN                  NaN   \nvol         0.235724              NaN            NaN                  NaN   \n\n      from_split  norms_seen  spellcheck_rank  in_lexicon  length  same_order  \\\nan           NaN         NaN              3.0         1.0       2         NaN   \nca           NaN         NaN              4.0         1.0       2         NaN   \ncab          NaN         NaN              1.0         1.0       3         NaN   \ncam          NaN         NaN              2.0         1.0       3         NaN   \ncan          NaN        72.0              NaN         1.0       3         1.0   \n...          ...         ...              ...         ...     ...         ...   \nlulu         NaN         NaN             14.0         1.0       4         NaN   \npol          NaN         NaN              9.0         1.0       3         NaN   \nsmh          NaN         NaN              NaN         NaN       3         NaN   \nsol          NaN         NaN              3.0         1.0       3         NaN   \nvol          NaN         NaN             12.0         NaN       3         NaN   \n\n      orig_norms_seen  orig_in_lexicon  orig_same_order  orig_length  process  \\\nan               72.0              1.0              1.0          3.0       35   \nca               72.0              1.0              1.0          3.0       35   \ncab              72.0              1.0              1.0          3.0       35   \ncam              72.0              1.0              1.0          3.0       35   \ncan              72.0              1.0              1.0          3.0       35   \n...               ...              ...              ...          ...      ...   \nlulu            205.0              1.0              1.0          3.0       32   \npol             205.0              1.0              1.0          3.0       32   \nsmh             205.0              1.0              1.0          3.0       32   \nsol             205.0              1.0              1.0          3.0       32   \nvol             205.0              1.0              1.0          3.0       32   \n\n      tweet  tok  \nan        0    2  \nca        0    2  \ncab       0    2  \ncam       0    2  \ncan       0    2  \n...     ...  ...  \nlulu      9  195  \npol       9  195  \nsmh       9  195  \nsol       9  195  \nvol       9  195  \n\n[727634 rows x 17 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>cosine_to_orig</th>\n      <th>embeddings_rank</th>\n      <th>from_clipping</th>\n      <th>from_original_token</th>\n      <th>from_split</th>\n      <th>norms_seen</th>\n      <th>spellcheck_rank</th>\n      <th>in_lexicon</th>\n      <th>length</th>\n      <th>same_order</th>\n      <th>orig_norms_seen</th>\n      <th>orig_in_lexicon</th>\n      <th>orig_same_order</th>\n      <th>orig_length</th>\n      <th>process</th>\n      <th>tweet</th>\n      <th>tok</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>an</th>\n      <td>0.377180</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>3.0</td>\n      <td>1.0</td>\n      <td>2</td>\n      <td>NaN</td>\n      <td>72.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>3.0</td>\n      <td>35</td>\n      <td>0</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>ca</th>\n      <td>0.255824</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>4.0</td>\n      <td>1.0</td>\n      <td>2</td>\n      <td>NaN</td>\n      <td>72.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>3.0</td>\n      <td>35</td>\n      <td>0</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>cab</th>\n      <td>0.261057</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>3</td>\n      <td>NaN</td>\n      <td>72.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>3.0</td>\n      <td>35</td>\n      <td>0</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>cam</th>\n      <td>0.299136</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>3</td>\n      <td>NaN</td>\n      <td>72.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>3.0</td>\n      <td>35</td>\n      <td>0</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>can</th>\n      <td>1.000000</td>\n      <td>NaN</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>NaN</td>\n      <td>72.0</td>\n      <td>NaN</td>\n      <td>1.0</td>\n      <td>3</td>\n      <td>1.0</td>\n      <td>72.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>3.0</td>\n      <td>35</td>\n      <td>0</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>lulu</th>\n      <td>0.401415</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>14.0</td>\n      <td>1.0</td>\n      <td>4</td>\n      <td>NaN</td>\n      <td>205.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>3.0</td>\n      <td>32</td>\n      <td>9</td>\n      <td>195</td>\n    </tr>\n    <tr>\n      <th>pol</th>\n      <td>0.230619</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>9.0</td>\n      <td>1.0</td>\n      <td>3</td>\n      <td>NaN</td>\n      <td>205.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>3.0</td>\n      <td>32</td>\n      <td>9</td>\n      <td>195</td>\n    </tr>\n    <tr>\n      <th>smh</th>\n      <td>0.896277</td>\n      <td>1.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>3</td>\n      <td>NaN</td>\n      <td>205.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>3.0</td>\n      <td>32</td>\n      <td>9</td>\n      <td>195</td>\n    </tr>\n    <tr>\n      <th>sol</th>\n      <td>0.285978</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>3.0</td>\n      <td>1.0</td>\n      <td>3</td>\n      <td>NaN</td>\n      <td>205.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>3.0</td>\n      <td>32</td>\n      <td>9</td>\n      <td>195</td>\n    </tr>\n    <tr>\n      <th>vol</th>\n      <td>0.235724</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>12.0</td>\n      <td>NaN</td>\n      <td>3</td>\n      <td>NaN</td>\n      <td>205.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>3.0</td>\n      <td>32</td>\n      <td>9</td>\n      <td>195</td>\n    </tr>\n  </tbody>\n</table>\n<p>727634 rows × 17 columns</p>\n</div>"
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "dev"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "train.fillna(0, inplace=True)\n",
    "dev.fillna(0, inplace=True)\n",
    "X_train = train.drop([\"correct\", \"process\", \"tweet\", \"tok\", \"gold\"], axis=1)\n",
    "X_dev = dev.drop([\"process\", \"tweet\", \"tok\"], axis=1)\n",
    "y_train = train[\"correct\"]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "feature_vals = X_train.values\n",
    "scaler = preprocessing.StandardScaler().fit(feature_vals)\n",
    "feature_vals = scaler.transform(feature_vals)\n",
    "X_train[:] = feature_vals\n",
    "\n",
    "feature_vals = X_dev.values\n",
    "scaler = preprocessing.StandardScaler().fit(feature_vals)\n",
    "feature_vals = scaler.transform(feature_vals)\n",
    "X_dev[:] = feature_vals"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear]....................................................................................................\n",
      "optimization finished, #iter = 1000\n",
      "\n",
      "WARNING: reaching max number of iterations\n",
      "Using -s 2 may be faster (also see FAQ)\n",
      "\n",
      "Objective value = -77126.360842\n",
      "nSV = 259470\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/elijoe/Library/Mobile Documents/com~apple~CloudDocs/Documents/2/Diss/lexnorm/.venv/lib/python3.10/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "svm = LinearSVC(class_weight=\"balanced\", verbose=1).fit(X_train, y_train.values.ravel())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'svm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[3], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mjoblib\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m dump, load\n\u001B[0;32m----> 2\u001B[0m dump(\u001B[43msvm\u001B[49m, os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(DATA_PATH, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m../models/svm.joblib\u001B[39m\u001B[38;5;124m\"\u001B[39m))\n\u001B[1;32m      3\u001B[0m test \u001B[38;5;241m=\u001B[39m load(os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(DATA_PATH, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m../models/svm.joblib\u001B[39m\u001B[38;5;124m\"\u001B[39m))\n\u001B[1;32m      4\u001B[0m test\u001B[38;5;241m.\u001B[39mcoef_\n",
      "\u001B[0;31mNameError\u001B[0m: name 'svm' is not defined"
     ]
    }
   ],
   "source": [
    "from joblib import dump, load\n",
    "dump(svm, os.path.join(DATA_PATH, \"../models/svm.joblib\"))\n",
    "test = load(os.path.join(DATA_PATH, \"../models/svm.joblib\"))\n",
    "test.coef_"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "outputs": [],
   "source": [
    "preds = svm.decision_function(X_dev)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "outputs": [
    {
     "data": {
      "text/plain": "0.9983700596728575"
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm.score(X_dev, y_dev)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "outputs": [
    {
     "data": {
      "text/plain": "          cosine_to_orig  embeddings_rank  from_clipping  from_original_token  \\\nnah             2.656120         3.877197      -2.593884            -0.094405   \noh              2.745444         2.535195      -2.593884            -0.094405   \nuh              2.624856         5.219199      -2.593884            -0.094405   \nye ah          -1.138580        -0.148809      -2.593884            -0.094405   \nye-ah          -1.138580        -0.148809      -2.593884            -0.094405   \n...                  ...              ...            ...                  ...   \nupward          0.569510        -0.148809       0.385522            -0.094405   \nupwardly        0.702616        -0.148809       0.385522            -0.094405   \nupwards         0.833669        -0.148809       0.385522            -0.094405   \nus              1.122110        -0.148809      -2.593884            -0.094405   \nyup            -0.101099        -0.148809      -2.593884            -0.094405   \n\n          from_split  norms_seen  spellcheck_rank  in_lexicon    length  \\\nnah        -0.046030   -0.048241        -0.268871   -4.348835 -1.763720   \noh         -0.046030   -0.048241        -0.268871    0.229947 -2.095137   \nuh         -0.046030   -0.048241        -0.268871    0.229947 -2.095137   \nye ah      21.724887   -0.048241         1.360023   -4.348835 -1.100884   \nye-ah      -0.046030   -0.048241         1.767247   -4.348835 -1.100884   \n...              ...         ...              ...         ...       ...   \nupward     -0.046030   -0.048241        -0.268871    0.229947 -0.769467   \nupwardly   -0.046030   -0.048241        -0.268871    0.229947 -0.106631   \nupwards    -0.046030   -0.048241        -0.268871    0.229947 -0.438049   \nus         -0.046030   -0.048241         2.988917    0.229947 -2.095137   \nyup        -0.046030   -0.048241         2.581694    0.229947 -1.763720   \n\n          same_order  orig_norms_seen  orig_in_lexicon  orig_same_order  \\\nnah        -3.155060        -1.057043         0.345895              0.0   \noh         -3.155060        -1.057043         0.345895              0.0   \nuh         -3.155060        -1.057043         0.345895              0.0   \nye ah       0.316951        -1.057043         0.345895              0.0   \nye-ah       0.316951        -1.057043         0.345895              0.0   \n...              ...              ...              ...              ...   \nupward      0.316951        -0.688526         0.345895              0.0   \nupwardly    0.316951        -0.688526         0.345895              0.0   \nupwards     0.316951        -0.688526         0.345895              0.0   \nus         -3.155060        -0.688526         0.345895              0.0   \nyup         0.316951        -0.688526         0.345895              0.0   \n\n          orig_length  \nnah          1.361154  \noh           1.361154  \nuh           1.361154  \nye ah        1.361154  \nye-ah        1.361154  \n...               ...  \nupward      -0.462068  \nupwardly    -0.462068  \nupwards     -0.462068  \nus          -0.462068  \nyup         -0.462068  \n\n[2988229 rows x 14 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>cosine_to_orig</th>\n      <th>embeddings_rank</th>\n      <th>from_clipping</th>\n      <th>from_original_token</th>\n      <th>from_split</th>\n      <th>norms_seen</th>\n      <th>spellcheck_rank</th>\n      <th>in_lexicon</th>\n      <th>length</th>\n      <th>same_order</th>\n      <th>orig_norms_seen</th>\n      <th>orig_in_lexicon</th>\n      <th>orig_same_order</th>\n      <th>orig_length</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>nah</th>\n      <td>2.656120</td>\n      <td>3.877197</td>\n      <td>-2.593884</td>\n      <td>-0.094405</td>\n      <td>-0.046030</td>\n      <td>-0.048241</td>\n      <td>-0.268871</td>\n      <td>-4.348835</td>\n      <td>-1.763720</td>\n      <td>-3.155060</td>\n      <td>-1.057043</td>\n      <td>0.345895</td>\n      <td>0.0</td>\n      <td>1.361154</td>\n    </tr>\n    <tr>\n      <th>oh</th>\n      <td>2.745444</td>\n      <td>2.535195</td>\n      <td>-2.593884</td>\n      <td>-0.094405</td>\n      <td>-0.046030</td>\n      <td>-0.048241</td>\n      <td>-0.268871</td>\n      <td>0.229947</td>\n      <td>-2.095137</td>\n      <td>-3.155060</td>\n      <td>-1.057043</td>\n      <td>0.345895</td>\n      <td>0.0</td>\n      <td>1.361154</td>\n    </tr>\n    <tr>\n      <th>uh</th>\n      <td>2.624856</td>\n      <td>5.219199</td>\n      <td>-2.593884</td>\n      <td>-0.094405</td>\n      <td>-0.046030</td>\n      <td>-0.048241</td>\n      <td>-0.268871</td>\n      <td>0.229947</td>\n      <td>-2.095137</td>\n      <td>-3.155060</td>\n      <td>-1.057043</td>\n      <td>0.345895</td>\n      <td>0.0</td>\n      <td>1.361154</td>\n    </tr>\n    <tr>\n      <th>ye ah</th>\n      <td>-1.138580</td>\n      <td>-0.148809</td>\n      <td>-2.593884</td>\n      <td>-0.094405</td>\n      <td>21.724887</td>\n      <td>-0.048241</td>\n      <td>1.360023</td>\n      <td>-4.348835</td>\n      <td>-1.100884</td>\n      <td>0.316951</td>\n      <td>-1.057043</td>\n      <td>0.345895</td>\n      <td>0.0</td>\n      <td>1.361154</td>\n    </tr>\n    <tr>\n      <th>ye-ah</th>\n      <td>-1.138580</td>\n      <td>-0.148809</td>\n      <td>-2.593884</td>\n      <td>-0.094405</td>\n      <td>-0.046030</td>\n      <td>-0.048241</td>\n      <td>1.767247</td>\n      <td>-4.348835</td>\n      <td>-1.100884</td>\n      <td>0.316951</td>\n      <td>-1.057043</td>\n      <td>0.345895</td>\n      <td>0.0</td>\n      <td>1.361154</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>upward</th>\n      <td>0.569510</td>\n      <td>-0.148809</td>\n      <td>0.385522</td>\n      <td>-0.094405</td>\n      <td>-0.046030</td>\n      <td>-0.048241</td>\n      <td>-0.268871</td>\n      <td>0.229947</td>\n      <td>-0.769467</td>\n      <td>0.316951</td>\n      <td>-0.688526</td>\n      <td>0.345895</td>\n      <td>0.0</td>\n      <td>-0.462068</td>\n    </tr>\n    <tr>\n      <th>upwardly</th>\n      <td>0.702616</td>\n      <td>-0.148809</td>\n      <td>0.385522</td>\n      <td>-0.094405</td>\n      <td>-0.046030</td>\n      <td>-0.048241</td>\n      <td>-0.268871</td>\n      <td>0.229947</td>\n      <td>-0.106631</td>\n      <td>0.316951</td>\n      <td>-0.688526</td>\n      <td>0.345895</td>\n      <td>0.0</td>\n      <td>-0.462068</td>\n    </tr>\n    <tr>\n      <th>upwards</th>\n      <td>0.833669</td>\n      <td>-0.148809</td>\n      <td>0.385522</td>\n      <td>-0.094405</td>\n      <td>-0.046030</td>\n      <td>-0.048241</td>\n      <td>-0.268871</td>\n      <td>0.229947</td>\n      <td>-0.438049</td>\n      <td>0.316951</td>\n      <td>-0.688526</td>\n      <td>0.345895</td>\n      <td>0.0</td>\n      <td>-0.462068</td>\n    </tr>\n    <tr>\n      <th>us</th>\n      <td>1.122110</td>\n      <td>-0.148809</td>\n      <td>-2.593884</td>\n      <td>-0.094405</td>\n      <td>-0.046030</td>\n      <td>-0.048241</td>\n      <td>2.988917</td>\n      <td>0.229947</td>\n      <td>-2.095137</td>\n      <td>-3.155060</td>\n      <td>-0.688526</td>\n      <td>0.345895</td>\n      <td>0.0</td>\n      <td>-0.462068</td>\n    </tr>\n    <tr>\n      <th>yup</th>\n      <td>-0.101099</td>\n      <td>-0.148809</td>\n      <td>-2.593884</td>\n      <td>-0.094405</td>\n      <td>-0.046030</td>\n      <td>-0.048241</td>\n      <td>2.581694</td>\n      <td>0.229947</td>\n      <td>-1.763720</td>\n      <td>0.316951</td>\n      <td>-0.688526</td>\n      <td>0.345895</td>\n      <td>0.0</td>\n      <td>-0.462068</td>\n    </tr>\n  </tbody>\n</table>\n<p>2988229 rows × 14 columns</p>\n</div>"
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "outputs": [
    {
     "data": {
      "text/plain": "         cosine_to_orig  embeddings_rank  from_clipping  from_original_token  \\\ncitch          0.756863              1.0            0.0                  0.0   \nco             0.240537              0.0            0.0                  0.0   \nhoe            0.741933              4.0            0.0                  0.0   \nlo             0.361795              0.0            0.0                  0.0   \nnigga          0.742659              3.0            0.0                  0.0   \n...                 ...              ...            ...                  ...   \nyowl's         0.000000              0.0            1.0                  0.0   \nyowled         0.283298              0.0            1.0                  0.0   \nyowling        0.225846              0.0            1.0                  0.0   \nyowls          0.204925              0.0            1.0                  0.0   \nyr             0.448676              0.0            0.0                  0.0   \n\n         from_split  norms_seen  spellcheck_rank  in_lexicon  length  \\\ncitch           0.0         0.0              0.0         0.0       5   \nco              0.0         0.0             14.0         0.0       2   \nhoe             0.0         0.0              0.0         1.0       3   \nlo              0.0         0.0             13.0         1.0       2   \nnigga           0.0         0.0              0.0         1.0       5   \n...             ...         ...              ...         ...     ...   \nyowl's          0.0         0.0              0.0         1.0       6   \nyowled          0.0         0.0              0.0         1.0       6   \nyowling         0.0         0.0              0.0         1.0       7   \nyowls           0.0         0.0              0.0         1.0       5   \nyr              0.0         0.0             12.0         0.0       2   \n\n         same_order  orig_norms_seen  orig_in_lexicon  orig_same_order  \\\ncitch           0.0              1.0              1.0              1.0   \nco              0.0              1.0              1.0              1.0   \nhoe             0.0              1.0              1.0              1.0   \nlo              0.0              1.0              1.0              1.0   \nnigga           0.0              1.0              1.0              1.0   \n...             ...              ...              ...              ...   \nyowl's          1.0              1.0              1.0              1.0   \nyowled          1.0              1.0              1.0              1.0   \nyowling         1.0              1.0              1.0              1.0   \nyowls           1.0              1.0              1.0              1.0   \nyr              0.0              1.0              1.0              1.0   \n\n         orig_length  process  tweet  tok     preds  \ncitch            2.0        0      0    4 -1.731161  \nco               2.0        0      0    4 -2.434154  \nhoe              2.0        0      0    4 -1.538175  \nlo               2.0        0      0    4 -1.583027  \nnigga            2.0        0      0    4 -1.389208  \n...              ...      ...    ...  ...       ...  \nyowl's           2.0        0      0    4 -1.555294  \nyowled           2.0        0      0    4 -1.327322  \nyowling          2.0        0      0    4 -1.390141  \nyowls            2.0        0      0    4 -1.373803  \nyr               2.0        0      0    4 -2.192546  \n\n[112 rows x 18 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>cosine_to_orig</th>\n      <th>embeddings_rank</th>\n      <th>from_clipping</th>\n      <th>from_original_token</th>\n      <th>from_split</th>\n      <th>norms_seen</th>\n      <th>spellcheck_rank</th>\n      <th>in_lexicon</th>\n      <th>length</th>\n      <th>same_order</th>\n      <th>orig_norms_seen</th>\n      <th>orig_in_lexicon</th>\n      <th>orig_same_order</th>\n      <th>orig_length</th>\n      <th>process</th>\n      <th>tweet</th>\n      <th>tok</th>\n      <th>preds</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>citch</th>\n      <td>0.756863</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>5</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>2.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>4</td>\n      <td>-1.731161</td>\n    </tr>\n    <tr>\n      <th>co</th>\n      <td>0.240537</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>14.0</td>\n      <td>0.0</td>\n      <td>2</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>2.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>4</td>\n      <td>-2.434154</td>\n    </tr>\n    <tr>\n      <th>hoe</th>\n      <td>0.741933</td>\n      <td>4.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>3</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>2.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>4</td>\n      <td>-1.538175</td>\n    </tr>\n    <tr>\n      <th>lo</th>\n      <td>0.361795</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>13.0</td>\n      <td>1.0</td>\n      <td>2</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>2.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>4</td>\n      <td>-1.583027</td>\n    </tr>\n    <tr>\n      <th>nigga</th>\n      <td>0.742659</td>\n      <td>3.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>5</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>2.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>4</td>\n      <td>-1.389208</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>yowl's</th>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>6</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>2.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>4</td>\n      <td>-1.555294</td>\n    </tr>\n    <tr>\n      <th>yowled</th>\n      <td>0.283298</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>6</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>2.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>4</td>\n      <td>-1.327322</td>\n    </tr>\n    <tr>\n      <th>yowling</th>\n      <td>0.225846</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>7</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>2.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>4</td>\n      <td>-1.390141</td>\n    </tr>\n    <tr>\n      <th>yowls</th>\n      <td>0.204925</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>5</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>2.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>4</td>\n      <td>-1.373803</td>\n    </tr>\n    <tr>\n      <th>yr</th>\n      <td>0.448676</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>12.0</td>\n      <td>0.0</td>\n      <td>2</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>2.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>4</td>\n      <td>-2.192546</td>\n    </tr>\n  </tbody>\n</table>\n<p>112 rows × 18 columns</p>\n</div>"
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev[\"preds\"] = preds\n",
    "dev.loc[(dev.process == 0) & (dev.tweet == 0) & (dev.tok == 4)]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "outputs": [
    {
     "data": {
      "text/plain": "['brother',\n 'get',\n 'out',\n 'yo',\n 'feelings',\n 'lol',\n 'manan',\n 'dund',\n 'xaragdax',\n 'ter',\n 'uuliin',\n 'oroid',\n 'minii',\n 'aav',\n 'xezee',\n 'neg',\n 'cagt',\n 'zogsoj',\n 'baisan',\n 'photo',\n 'by',\n 'why',\n 'dese',\n 'niggers',\n 'think',\n 'dey',\n 'doin',\n 'summn',\n \"it's\",\n 'about',\n 'more',\n 'than',\n 'number',\n 'brother',\n 'and',\n \"i'm\",\n 'not',\n 'talkn',\n 'about',\n 'statistics',\n \"i'm\",\n 'talkn',\n 'about',\n 'skill',\n 'level',\n 'yes',\n 'omg',\n 'i',\n \"don't\",\n 'want',\n 'him',\n 'feeling',\n 'unappreciated',\n 'or',\n 'stuff',\n 'like',\n 'that',\n 'lmao',\n 'the',\n 'whole',\n 'time',\n 'actually',\n 'we',\n 'need',\n 'to',\n 'start',\n 'our',\n 'own',\n 'team',\n \"y'all\",\n 'were',\n 'wilding',\n 'last',\n 'nigjt',\n 'yoh',\n 'niya',\n 'ja',\n 'saying',\n 'kau',\n 'aku',\n 'with',\n 'couple',\n 'is',\n 'actually',\n 'sweet',\n 'k',\n 'michelle',\n 'met',\n 'yo',\n 'match',\n 'that',\n 'fonk',\n 'rana',\n 'samaha',\n 'ignore',\n 'all',\n 'the',\n 'criticism',\n 'a',\n 'spotlight',\n 'can',\n 'never',\n 'see',\n 'the',\n 'shadows',\n 'anyways',\n \"you're\",\n 'flawless',\n 'boko',\n 'haram',\n 'shettima',\n 'warns',\n 'nigerian',\n 'leaders',\n 'over',\n 'politicization',\n 'of',\n 'chibok',\n 'abduction',\n 'and',\n 'in',\n 'that',\n 'moment',\n 'karma',\n 'realised',\n 'she',\n 'was',\n 'hella',\n 'gay',\n 'for',\n 'her',\n 'bestie',\n 'not',\n 'you',\n 'because',\n 'you',\n 'chillen',\n 'wit',\n 'me',\n 'kardashia',\n 'kardashian',\n 'died',\n 'because',\n 'her',\n 'ass',\n 'was',\n 'too',\n 'big',\n 'hahah',\n 'niggra',\n 'summation',\n 'classified',\n 'directory',\n 'high',\n 'grille',\n 'for',\n 'the',\n 'online',\n 'game',\n 'atd',\n 'lexx',\n 'little',\n 'cute',\n 'ass',\n 'woke',\n 'up',\n 'mad',\n 'as',\n 'hell',\n 'lol',\n 'green',\n 'day',\n 'paramore',\n 'yellowcard',\n 'fall',\n 'out',\n 'boy',\n 'is',\n 'that',\n 'minho',\n 'beside',\n 'taemin',\n 'omo',\n 'looks',\n 'like',\n 'him',\n '140529',\n 'smtown',\n 'artists',\n 'with',\n 'exo',\n 'photoset',\n 'flintandpyrite',\n 'ume',\n 'by',\n 'andrea',\n 'rangel',\n 'knits',\n 'for',\n 'spring',\n 'because',\n \"that's\",\n 'going to',\n 'happen',\n 'lol',\n 'need',\n 'to',\n 'revisit',\n 'their',\n 'thameslink',\n 'bid',\n 'the',\n 'moorgate',\n 'route',\n 'closed',\n '4',\n 'years',\n 'ago',\n 'icons',\n 'da',\n 'demi',\n 'sem',\n 'psd',\n 'no',\n 'the',\n 'x',\n 'factor',\n 'x8',\n 'you',\n 'dun',\n \"i'm\",\n 'da',\n 'king',\n 'of',\n 'sweg',\n 'haha',\n \"that's\",\n 'funny',\n 'but',\n 'not',\n 'every',\n 'girl',\n 'is',\n 'darlin',\n 'imran',\n 'khan',\n 'saying',\n 'really',\n 'true',\n 'about',\n 'pmln',\n 'and',\n \"you'll\",\n 'always',\n 'be',\n 'my',\n '5',\n 'heros',\n 'who',\n 'can',\n 'always',\n 'make',\n 'me',\n 'smyle',\n 'even',\n 'when',\n 'i',\n 'm',\n 'in',\n 'the',\n 'sadest',\n 'mood',\n 'ever',\n 'xxc',\n 'lol',\n 'what',\n 'he',\n 'trippin',\n 'on',\n 'xabi',\n 'alonso',\n 'just',\n 'achieved',\n 'stratospheric',\n 'heights',\n 'of',\n 'coolness',\n 'lmaoo',\n 'my',\n 'fault',\n 'yo',\n 'ima',\n 'let',\n 'you',\n 'get',\n 'your',\n 'shine',\n 'james',\n 'and',\n 'i',\n 'say',\n 'hi',\n 'with',\n 'a',\n 'dog',\n 'tenerezza',\n 'omg',\n 'hawt',\n 'like',\n 'omg',\n 'can',\n 'my',\n 'husband',\n 'please',\n 'look',\n 'like',\n 'you',\n 'or',\n 'can',\n 'i',\n 'have',\n 'your',\n 'babies',\n 'i',\n 'miss',\n 'going',\n 'to',\n 'mansion',\n 'elan',\n 'ery',\n 'wkeend',\n \"i'm\",\n 'finna',\n 'start',\n 'back',\n 'going',\n 'doe',\n 'facts',\n 'bro',\n 'this',\n 'babyface',\n \"ain't\",\n 'helping',\n 'nothing',\n 'lmao',\n 'can',\n 'i',\n 'have',\n 'him',\n 'yet',\n 'orrr',\n 'noo',\n 'ughh',\n '140524',\n 'hyoyeon',\n 'yuri',\n '2nd',\n 'day',\n 'kobe',\n 'by',\n 'rebecca',\n 'crowdfunding',\n 'roundup',\n 'a',\n 'late',\n 'spring',\n 'flowering',\n 'of',\n 'projects',\n 'every',\n 'week',\n 'tuaw',\n 'provides',\n 'readers',\n 'with',\n 'an',\n 'update',\n 'o',\n 'mayweather',\n 'needs',\n 'to',\n 'fight',\n 'paquiao',\n 'that',\n 'would',\n 'be',\n 'ammmazing',\n \"i'm\",\n 'actully',\n 'excited',\n 'to',\n 'see',\n 'what',\n 'it',\n 'will',\n 'look',\n 'like',\n 'so',\n 'its',\n 'happening',\n 'when',\n 'the',\n 'lawn',\n 'area',\n 'get',\n 'done',\n 'lol',\n 'who',\n 'bouta',\n 'come',\n 'way',\n 'out',\n 'there',\n 'come',\n 'get',\n 'me',\n 'bring',\n 'me',\n 'bck',\n \"we'll\",\n 'come',\n 'lol',\n 'i',\n 'forgot',\n 'you',\n 'moved',\n 'canal',\n 'then',\n 'lol',\n 'apple',\n 'forgets',\n 'to',\n 'renew',\n 'ssl',\n 'certificate',\n 'breaking',\n 'os',\n 'x',\n 'software',\n 'update',\n 'ik',\n 'id',\n 'rather',\n 'someone',\n 'unfollow',\n 'me',\n 'than',\n 'mute',\n 'me',\n 'like',\n 'what',\n 'ohhh',\n 'my',\n 'poor',\n 'zac',\n 'take',\n 'good',\n 'care',\n 'of',\n 'urself',\n 'kimberley',\n 'walsh',\n 'confirms',\n 'cheryl',\n \"cole's\",\n \"britain's\",\n 'got',\n 'talent',\n 'appearance',\n 'contactmusic',\n 'comkimberley',\n 'walsh',\n 'confir',\n 'exo',\n 'is',\n 'practising',\n 'for',\n 'concert',\n 'lol',\n 'you',\n 'still',\n 'in',\n 'school',\n 'shutup',\n 'he',\n 'wants',\n 'to',\n 'get',\n 'bigger',\n 'than',\n 'parth',\n 'lmfao',\n 'chanyeol',\n 'is',\n 'so',\n 'caring',\n 'omg',\n 'be',\n 'mine',\n 'please',\n 'nothin',\n 'just',\n 'chillin',\n 'imy',\n 'you',\n \"don't\",\n 'fuck',\n 'with',\n 'a',\n 'thug',\n 'nomore',\n 'lol',\n 'udah',\n 'lama',\n 'nda',\n 'how',\n 'about',\n 'it',\n 'how',\n 'about',\n 'that',\n 'lol',\n 'ik',\n 'i',\n 'just',\n 'took',\n 'them',\n 'real',\n 'quick',\n 'i',\n 'wasnt',\n 'thinkin',\n 'the',\n 'order',\n '1',\n 'daily',\n 'follower',\n '0',\n 'unfollowers',\n 'justunfollow',\n \"doesn't\",\n 'miss',\n 'a',\n 'trick',\n \"i'm\",\n 'going to',\n 'do',\n 'it',\n 'after',\n 'graduation',\n 'lol',\n 'second',\n 'time',\n 'this',\n 'shit',\n 'crashed',\n 'on',\n 'datpiff',\n 'so',\n 'we',\n 'on',\n 'soundcloud',\n 'with',\n 'it',\n 'genji',\n 'sports',\n 'pop',\n 'up',\n 'family',\n 'beach',\n 'tent',\n 'and',\n 'beach',\n 'sunshelter',\n 'genji',\n 'sports',\n 'pop',\n 'up',\n 'fami',\n 'titan',\n 'baseball',\n 'beats',\n 'brv',\n 'to',\n 'advance',\n 'to',\n 'county',\n 'championship',\n 'tri',\n 'tomorrow',\n '4',\n 'pm',\n 'jumpseat',\n 'radio',\n '018',\n 'combat',\n 'ready',\n 'firefighting',\n 'with',\n 'tony',\n 'kelleher',\n 'repost',\n 'i',\n 'added',\n 'a',\n 'video',\n 'to',\n 'a',\n 'playlist',\n 'plg',\n 'sensual',\n 'toribash',\n 'ep',\n '3',\n \"let's\",\n 'tango',\n 'wildfires',\n 'turkey',\n 'feather',\n 'fletching',\n 'for',\n 'archery',\n 'arrows',\n 'diy',\n 'pheasant',\n 'feathers',\n 'for',\n 'arro',\n 'where',\n 'the',\n 'mj',\n 'though',\n 'lol',\n '492',\n 'luke',\n 'hemmings',\n 'from',\n '5sos',\n 'i',\n 'love',\n 'you',\n 'so',\n 'much',\n 'please',\n 'follow',\n 'me',\n 'you',\n 'are',\n 'everything',\n 'to',\n 'me',\n 'please',\n 'dude',\n 'you',\n 'never',\n 'gave',\n 'me',\n 'the',\n 'kik',\n 'or',\n 'snapchat',\n \"you'll\",\n 'sea',\n 'lmfao',\n 'oookay',\n 'bean',\n 'eating',\n 'headass',\n 'morning',\n 'exercise',\n 'paakyat',\n 'ng',\n 'thunderbird',\n 'i',\n 'love',\n 'you',\n 'phil',\n 'of',\n 'the',\n 'future',\n 'i',\n 'love',\n 'everything',\n 'abt',\n 'you',\n 'nao',\n 'superei',\n 'ainda',\n 'this',\n 'is',\n 'for',\n 'niall',\n 'e',\n 'vcs',\n 'wara',\n 'man',\n 'talaga',\n 'forever',\n 'niyata',\n 'ano',\n 'una',\n 'mo',\n 'imortal',\n 'ka',\n 'hahahahahahhahaha',\n 'hot',\n 'music',\n 'mariah',\n 'carey',\n 'meteorite',\n 'q',\n 'meteorite',\n 'version',\n 'mariah',\n 'carey',\n 'has',\n 'ohh',\n 'i',\n 'feel',\n 'you',\n 'haha',\n 'so',\n 'is',\n 'writin',\n 'a',\n 'book',\n 'about',\n 'jay',\n 'z',\n 'and',\n 'beyonce',\n 'tellin',\n 'someone',\n \"else's\",\n 'secrets',\n 'for',\n 'money',\n 'shows',\n 'how',\n 'low',\n 'some',\n 'people',\n 'will',\n 'stoop',\n 'fast',\n 'forward',\n 'to',\n 'june',\n '9',\n 'please',\n 'para',\n 'makapag',\n 'ipon',\n 'na',\n 'hahahahahah',\n 'lol',\n 'thank',\n 'you',\n 'liam',\n 'for',\n 'asking',\n 'louis',\n 'what',\n 'flavour',\n 'harry',\n 'is',\n \"i'm\",\n 'so',\n 'jskcosnxoancoamxis',\n 'ahhhhh',\n 'salt',\n 'and',\n 'vinegar',\n \"don't\",\n 'take',\n 'it',\n 'personal',\n 'kid',\n 'caroline',\n 'wozniacki',\n 'breaks',\n 'silence',\n 'on',\n 'rory',\n 'mcilroy',\n 'breakup',\n '140525',\n 'boakwon',\n 'ig',\n 'update',\n 'with',\n 'exo',\n 'and',\n 'suju',\n 'siwon',\n 'donghae',\n 'more',\n 'and',\n 'forehead',\n 'ft',\n 'hyuk',\n 'thanks',\n 'for',\n 'that',\n 'spiderman',\n 'carnage',\n 'mask',\n '300rb',\n 'lom',\n 'ama',\n 'ongkir',\n 'ready',\n 'stock',\n 'runnin',\n 'with',\n 'who',\n 'do',\n 'you',\n 'think',\n 'is',\n 'faster',\n 'nicki',\n 'minaj',\n 'slick',\n 'went',\n 'from',\n 'brownskin',\n 'to',\n 'lightskin',\n 'over',\n 'the',\n 'years',\n 'if',\n 'you',\n \"don't\",\n 'fuck',\n 'wit',\n 'dunny',\n 'gage',\n 'then',\n 'fuck',\n 'ya',\n 'life',\n 'last',\n 'day',\n 'of',\n 'school',\n 'gonna',\n 'have',\n 'people',\n 'like',\n 'the',\n 'bad',\n 'pastor',\n 'impostors',\n 'so',\n 'loves',\n 'a',\n 'lottle',\n 'to',\n 'jesus',\n 'the',\n 'people',\n 'can',\n 'intercesion',\n 'for',\n 'he',\n 'for',\n 'the',\n 'soul',\n 'peole',\n 'wilshire',\n 'itq',\n 'hehe',\n 'stay',\n 'strong',\n '1',\n 'k',\n 'swim',\n 'sauna',\n 'post',\n 'work',\n 'now',\n 'beer',\n 'work',\n 'tomorrow',\n 'terserah',\n 'suho',\n 'gave',\n 'a',\n 'teddy',\n 'bear',\n 'and',\n 'took',\n 'a',\n 'selca',\n 'with',\n 'a',\n 'fan',\n 'tazz',\n 'needa',\n 'quit',\n 'it',\n 'wit',\n 'that',\n 'last',\n 'quote',\n 'lol',\n 'she',\n \"don't\",\n 'live',\n 'that',\n 'life',\n 'why',\n 'the',\n 'fuck',\n 'did',\n 'i',\n 'think',\n 'i',\n 'was',\n 'scheduled',\n 'today',\n 'am',\n 'i',\n 'on',\n 'crack',\n 'lmao',\n 'wut',\n 'my',\n 'top',\n '3',\n 'artists',\n 'girls',\n 'aloud',\n '15',\n 'christina',\n 'aguilera',\n '13',\n 'kylie',\n 'minogue',\n '12',\n 'how',\n 'he',\n 'top',\n '5',\n 'scorer',\n 'and',\n 'the',\n 'offense',\n \"ain't\",\n 'around',\n 'atleast',\n 'him',\n 'lol',\n 'that',\n \"don't\",\n 'make',\n 'sense',\n 'that',\n 'mean',\n 'he',\n 'jackin',\n 'then',\n 'lol',\n 'kompany',\n 'going',\n 'up',\n 'to',\n 'yaya',\n 'toure',\n 'after',\n 'he',\n 'found',\n 'out',\n 'he',\n 'wanted',\n 'to',\n 'leave',\n 'first',\n 'class',\n 'thing',\n 'see',\n 'wizkid',\n 'and',\n 'banky',\n 'w',\n 'having',\n 'fun',\n 'on',\n 'via',\n 'the',\n 'old',\n 'bargin',\n 'giant',\n 'would',\n 'make',\n 'a',\n 'good',\n 'entertainment',\n ...]"
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get top prediction for each raw token\n",
    "predictions = dev.sort_values(\"preds\", ascending=False).drop_duplicates([\"process\", \"tweet\", \"tok\"])\n",
    "pred_tokens = predictions.sort_values([\"process\", \"tweet\", \"tok\"]).index.tolist()\n",
    "pred_tokens"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "from lexnorm.data.normEval import loadNormData\n",
    "\n",
    "raw, norm = loadNormData(os.path.join(DATA_PATH, \"raw/dev.norm\"))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "from lexnorm.models.filtering import is_eligible\n",
    "\n",
    "pred_tokens_iter = iter(pred_tokens)\n",
    "\n",
    "pred_tweets = []\n",
    "\n",
    "for tweet in raw:\n",
    "    pred_tweet = []\n",
    "    for tok in tweet:\n",
    "        if is_eligible(tok):\n",
    "            pred_tweet.append(next(pred_tokens_iter))\n",
    "        else:\n",
    "            pred_tweet.append(tok)\n",
    "    pred_tweets.append(pred_tweet)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline acc.(LAI): 93.10\n",
      "Accuracy:           96.28\n",
      "ERR:                46.13\n"
     ]
    },
    {
     "data": {
      "text/plain": "(0.9309630275929763, 0.9628094666812084, 0.4612954186413895)"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from lexnorm.data.normEval import evaluate\n",
    "\n",
    "evaluate(raw, norm, pred_tweets)\n",
    "# pretty bad. Some reasons: not good enough feature extraction, using a linear svm when data might not be linearly seperable in our feature space (may need different kernel), set non-calculated/absent values to zero which can be picked up by a random forest but NOT by most models! May need to give highest possible value instead."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "outputs": [
    {
     "ename": "InvalidIndexError",
     "evalue": "(slice(None, None, None), slice(None, 2, None))",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "File \u001B[0;32m~/Library/Mobile Documents/com~apple~CloudDocs/Documents/2/Diss/lexnorm/.venv/lib/python3.10/site-packages/pandas/core/indexes/base.py:3802\u001B[0m, in \u001B[0;36mIndex.get_loc\u001B[0;34m(self, key, method, tolerance)\u001B[0m\n\u001B[1;32m   3801\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 3802\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_loc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcasted_key\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   3803\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m err:\n",
      "File \u001B[0;32m~/Library/Mobile Documents/com~apple~CloudDocs/Documents/2/Diss/lexnorm/.venv/lib/python3.10/site-packages/pandas/_libs/index.pyx:138\u001B[0m, in \u001B[0;36mpandas._libs.index.IndexEngine.get_loc\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32m~/Library/Mobile Documents/com~apple~CloudDocs/Documents/2/Diss/lexnorm/.venv/lib/python3.10/site-packages/pandas/_libs/index.pyx:144\u001B[0m, in \u001B[0;36mpandas._libs.index.IndexEngine.get_loc\u001B[0;34m()\u001B[0m\n",
      "\u001B[0;31mTypeError\u001B[0m: '(slice(None, None, None), slice(None, 2, None))' is an invalid key",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[0;31mInvalidIndexError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[157], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msklearn\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01minspection\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m DecisionBoundaryDisplay\n\u001B[0;32m----> 2\u001B[0m DecisionBoundaryDisplay\u001B[38;5;241m.\u001B[39mfrom_estimator(svm, \u001B[43mX_dev\u001B[49m\u001B[43m[\u001B[49m\u001B[43m:\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m:\u001B[49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m]\u001B[49m)\n",
      "File \u001B[0;32m~/Library/Mobile Documents/com~apple~CloudDocs/Documents/2/Diss/lexnorm/.venv/lib/python3.10/site-packages/pandas/core/frame.py:3807\u001B[0m, in \u001B[0;36mDataFrame.__getitem__\u001B[0;34m(self, key)\u001B[0m\n\u001B[1;32m   3805\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcolumns\u001B[38;5;241m.\u001B[39mnlevels \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[1;32m   3806\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_getitem_multilevel(key)\n\u001B[0;32m-> 3807\u001B[0m indexer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcolumns\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_loc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   3808\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_integer(indexer):\n\u001B[1;32m   3809\u001B[0m     indexer \u001B[38;5;241m=\u001B[39m [indexer]\n",
      "File \u001B[0;32m~/Library/Mobile Documents/com~apple~CloudDocs/Documents/2/Diss/lexnorm/.venv/lib/python3.10/site-packages/pandas/core/indexes/base.py:3809\u001B[0m, in \u001B[0;36mIndex.get_loc\u001B[0;34m(self, key, method, tolerance)\u001B[0m\n\u001B[1;32m   3804\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m(key) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01merr\u001B[39;00m\n\u001B[1;32m   3805\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m:\n\u001B[1;32m   3806\u001B[0m         \u001B[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001B[39;00m\n\u001B[1;32m   3807\u001B[0m         \u001B[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001B[39;00m\n\u001B[1;32m   3808\u001B[0m         \u001B[38;5;66;03m#  the TypeError.\u001B[39;00m\n\u001B[0;32m-> 3809\u001B[0m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_check_indexing_error\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   3810\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m\n\u001B[1;32m   3812\u001B[0m \u001B[38;5;66;03m# GH#42269\u001B[39;00m\n",
      "File \u001B[0;32m~/Library/Mobile Documents/com~apple~CloudDocs/Documents/2/Diss/lexnorm/.venv/lib/python3.10/site-packages/pandas/core/indexes/base.py:5925\u001B[0m, in \u001B[0;36mIndex._check_indexing_error\u001B[0;34m(self, key)\u001B[0m\n\u001B[1;32m   5921\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_check_indexing_error\u001B[39m(\u001B[38;5;28mself\u001B[39m, key):\n\u001B[1;32m   5922\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_scalar(key):\n\u001B[1;32m   5923\u001B[0m         \u001B[38;5;66;03m# if key is not a scalar, directly raise an error (the code below\u001B[39;00m\n\u001B[1;32m   5924\u001B[0m         \u001B[38;5;66;03m# would convert to numpy arrays and raise later any way) - GH29926\u001B[39;00m\n\u001B[0;32m-> 5925\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m InvalidIndexError(key)\n",
      "\u001B[0;31mInvalidIndexError\u001B[0m: (slice(None, None, None), slice(None, 2, None))"
     ]
    }
   ],
   "source": [
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "DecisionBoundaryDisplay.from_estimator(svm, X_dev[:, :2])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "from joblib import load\n",
    "import os\n",
    "from lexnorm.definitions import DATA_PATH\n",
    "rd_clf = load(os.path.join(DATA_PATH, \"../models/rf.joblib\"))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "outputs": [
    {
     "data": {
      "text/plain": "[DecisionTreeClassifier(max_features='sqrt', random_state=1608637542),\n DecisionTreeClassifier(max_features='sqrt', random_state=1273642419),\n DecisionTreeClassifier(max_features='sqrt', random_state=1935803228),\n DecisionTreeClassifier(max_features='sqrt', random_state=787846414),\n DecisionTreeClassifier(max_features='sqrt', random_state=996406378),\n DecisionTreeClassifier(max_features='sqrt', random_state=1201263687),\n DecisionTreeClassifier(max_features='sqrt', random_state=423734972),\n DecisionTreeClassifier(max_features='sqrt', random_state=415968276),\n DecisionTreeClassifier(max_features='sqrt', random_state=670094950),\n DecisionTreeClassifier(max_features='sqrt', random_state=1914837113),\n DecisionTreeClassifier(max_features='sqrt', random_state=669991378),\n DecisionTreeClassifier(max_features='sqrt', random_state=429389014),\n DecisionTreeClassifier(max_features='sqrt', random_state=249467210),\n DecisionTreeClassifier(max_features='sqrt', random_state=1972458954),\n DecisionTreeClassifier(max_features='sqrt', random_state=1572714583),\n DecisionTreeClassifier(max_features='sqrt', random_state=1433267572),\n DecisionTreeClassifier(max_features='sqrt', random_state=434285667),\n DecisionTreeClassifier(max_features='sqrt', random_state=613608295),\n DecisionTreeClassifier(max_features='sqrt', random_state=893664919),\n DecisionTreeClassifier(max_features='sqrt', random_state=648061058),\n DecisionTreeClassifier(max_features='sqrt', random_state=88409749),\n DecisionTreeClassifier(max_features='sqrt', random_state=242285876),\n DecisionTreeClassifier(max_features='sqrt', random_state=2018247425),\n DecisionTreeClassifier(max_features='sqrt', random_state=953477463),\n DecisionTreeClassifier(max_features='sqrt', random_state=1427830251),\n DecisionTreeClassifier(max_features='sqrt', random_state=1883569565),\n DecisionTreeClassifier(max_features='sqrt', random_state=911989541),\n DecisionTreeClassifier(max_features='sqrt', random_state=3344769),\n DecisionTreeClassifier(max_features='sqrt', random_state=780932287),\n DecisionTreeClassifier(max_features='sqrt', random_state=2114032571),\n DecisionTreeClassifier(max_features='sqrt', random_state=787716372),\n DecisionTreeClassifier(max_features='sqrt', random_state=504579232),\n DecisionTreeClassifier(max_features='sqrt', random_state=1306710475),\n DecisionTreeClassifier(max_features='sqrt', random_state=479546681),\n DecisionTreeClassifier(max_features='sqrt', random_state=106328085),\n DecisionTreeClassifier(max_features='sqrt', random_state=30349564),\n DecisionTreeClassifier(max_features='sqrt', random_state=1855189739),\n DecisionTreeClassifier(max_features='sqrt', random_state=99052376),\n DecisionTreeClassifier(max_features='sqrt', random_state=1250819632),\n DecisionTreeClassifier(max_features='sqrt', random_state=106406362),\n DecisionTreeClassifier(max_features='sqrt', random_state=480404538),\n DecisionTreeClassifier(max_features='sqrt', random_state=1717389822),\n DecisionTreeClassifier(max_features='sqrt', random_state=599121577),\n DecisionTreeClassifier(max_features='sqrt', random_state=200427519),\n DecisionTreeClassifier(max_features='sqrt', random_state=1254751707),\n DecisionTreeClassifier(max_features='sqrt', random_state=2034764475),\n DecisionTreeClassifier(max_features='sqrt', random_state=1573512143),\n DecisionTreeClassifier(max_features='sqrt', random_state=999745294),\n DecisionTreeClassifier(max_features='sqrt', random_state=1958805693),\n DecisionTreeClassifier(max_features='sqrt', random_state=389151677),\n DecisionTreeClassifier(max_features='sqrt', random_state=1224821422),\n DecisionTreeClassifier(max_features='sqrt', random_state=508464061),\n DecisionTreeClassifier(max_features='sqrt', random_state=857592370),\n DecisionTreeClassifier(max_features='sqrt', random_state=1642661739),\n DecisionTreeClassifier(max_features='sqrt', random_state=61136438),\n DecisionTreeClassifier(max_features='sqrt', random_state=2075460851),\n DecisionTreeClassifier(max_features='sqrt', random_state=396917567),\n DecisionTreeClassifier(max_features='sqrt', random_state=2004731384),\n DecisionTreeClassifier(max_features='sqrt', random_state=199502978),\n DecisionTreeClassifier(max_features='sqrt', random_state=1545932260),\n DecisionTreeClassifier(max_features='sqrt', random_state=461901618),\n DecisionTreeClassifier(max_features='sqrt', random_state=774414982),\n DecisionTreeClassifier(max_features='sqrt', random_state=732395540),\n DecisionTreeClassifier(max_features='sqrt', random_state=1934879560),\n DecisionTreeClassifier(max_features='sqrt', random_state=279394470),\n DecisionTreeClassifier(max_features='sqrt', random_state=56972561),\n DecisionTreeClassifier(max_features='sqrt', random_state=1927948675),\n DecisionTreeClassifier(max_features='sqrt', random_state=1899242072),\n DecisionTreeClassifier(max_features='sqrt', random_state=1999874363),\n DecisionTreeClassifier(max_features='sqrt', random_state=271820813),\n DecisionTreeClassifier(max_features='sqrt', random_state=1324556529),\n DecisionTreeClassifier(max_features='sqrt', random_state=1655351289),\n DecisionTreeClassifier(max_features='sqrt', random_state=1308306184),\n DecisionTreeClassifier(max_features='sqrt', random_state=68574553),\n DecisionTreeClassifier(max_features='sqrt', random_state=419498548),\n DecisionTreeClassifier(max_features='sqrt', random_state=991681409),\n DecisionTreeClassifier(max_features='sqrt', random_state=791274835),\n DecisionTreeClassifier(max_features='sqrt', random_state=1035196507),\n DecisionTreeClassifier(max_features='sqrt', random_state=1890440558),\n DecisionTreeClassifier(max_features='sqrt', random_state=787110843),\n DecisionTreeClassifier(max_features='sqrt', random_state=524150214),\n DecisionTreeClassifier(max_features='sqrt', random_state=472432043),\n DecisionTreeClassifier(max_features='sqrt', random_state=2126768636),\n DecisionTreeClassifier(max_features='sqrt', random_state=1431061255),\n DecisionTreeClassifier(max_features='sqrt', random_state=147697582),\n DecisionTreeClassifier(max_features='sqrt', random_state=744595490),\n DecisionTreeClassifier(max_features='sqrt', random_state=1758017741),\n DecisionTreeClassifier(max_features='sqrt', random_state=1679592528),\n DecisionTreeClassifier(max_features='sqrt', random_state=1111451555),\n DecisionTreeClassifier(max_features='sqrt', random_state=782698033),\n DecisionTreeClassifier(max_features='sqrt', random_state=698027879),\n DecisionTreeClassifier(max_features='sqrt', random_state=1096768899),\n DecisionTreeClassifier(max_features='sqrt', random_state=1338788865),\n DecisionTreeClassifier(max_features='sqrt', random_state=1826030589),\n DecisionTreeClassifier(max_features='sqrt', random_state=86191493),\n DecisionTreeClassifier(max_features='sqrt', random_state=893102645),\n DecisionTreeClassifier(max_features='sqrt', random_state=200619113),\n DecisionTreeClassifier(max_features='sqrt', random_state=290770691),\n DecisionTreeClassifier(max_features='sqrt', random_state=793943861),\n DecisionTreeClassifier(max_features='sqrt', random_state=134489564)]"
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rd_clf.estimators_"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "0.9995261407341941"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rd_clf.oob_score_"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:    0.9s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:    4.5s finished\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[200], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mrd_clf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdecision_path\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_dev\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Library/Mobile Documents/com~apple~CloudDocs/Documents/2/Diss/lexnorm/.venv/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:311\u001B[0m, in \u001B[0;36mBaseForest.decision_path\u001B[0;34m(self, X)\u001B[0m\n\u001B[1;32m    308\u001B[0m n_nodes\u001B[38;5;241m.\u001B[39mextend([i\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m1\u001B[39m] \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m indicators])\n\u001B[1;32m    309\u001B[0m n_nodes_ptr \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39marray(n_nodes)\u001B[38;5;241m.\u001B[39mcumsum()\n\u001B[0;32m--> 311\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43msparse_hstack\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindicators\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mtocsr(), n_nodes_ptr\n",
      "File \u001B[0;32m~/Library/Mobile Documents/com~apple~CloudDocs/Documents/2/Diss/lexnorm/.venv/lib/python3.10/site-packages/scipy/sparse/_construct.py:535\u001B[0m, in \u001B[0;36mhstack\u001B[0;34m(blocks, format, dtype)\u001B[0m\n\u001B[1;32m    505\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mhstack\u001B[39m(blocks, \u001B[38;5;28mformat\u001B[39m\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, dtype\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[1;32m    506\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    507\u001B[0m \u001B[38;5;124;03m    Stack sparse matrices horizontally (column wise)\u001B[39;00m\n\u001B[1;32m    508\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    533\u001B[0m \n\u001B[1;32m    534\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 535\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mbmat\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\u001B[43mblocks\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mformat\u001B[39;49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mformat\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdtype\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Library/Mobile Documents/com~apple~CloudDocs/Documents/2/Diss/lexnorm/.venv/lib/python3.10/site-packages/scipy/sparse/_construct.py:627\u001B[0m, in \u001B[0;36mbmat\u001B[0;34m(blocks, format, dtype)\u001B[0m\n\u001B[1;32m    623\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\u001B[38;5;28mformat\u001B[39m \u001B[38;5;129;01min\u001B[39;00m (\u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcsr\u001B[39m\u001B[38;5;124m'\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mall\u001B[39m(\u001B[38;5;28misinstance\u001B[39m(b, csr_matrix)\n\u001B[1;32m    624\u001B[0m                                     \u001B[38;5;28;01mfor\u001B[39;00m b \u001B[38;5;129;01min\u001B[39;00m blocks\u001B[38;5;241m.\u001B[39mflat)):\n\u001B[1;32m    625\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m N \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[1;32m    626\u001B[0m         \u001B[38;5;66;03m# stack along columns (axis 1):\u001B[39;00m\n\u001B[0;32m--> 627\u001B[0m         blocks \u001B[38;5;241m=\u001B[39m [[_stack_along_minor_axis(blocks[b, :], \u001B[38;5;241m1\u001B[39m)]\n\u001B[1;32m    628\u001B[0m                   \u001B[38;5;28;01mfor\u001B[39;00m b \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(M)]   \u001B[38;5;66;03m# must have shape: (M, 1)\u001B[39;00m\n\u001B[1;32m    629\u001B[0m         blocks \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39masarray(blocks, dtype\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mobject\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m    631\u001B[0m     \u001B[38;5;66;03m# stack along rows (axis 0):\u001B[39;00m\n",
      "File \u001B[0;32m~/Library/Mobile Documents/com~apple~CloudDocs/Documents/2/Diss/lexnorm/.venv/lib/python3.10/site-packages/scipy/sparse/_construct.py:627\u001B[0m, in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m    623\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\u001B[38;5;28mformat\u001B[39m \u001B[38;5;129;01min\u001B[39;00m (\u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcsr\u001B[39m\u001B[38;5;124m'\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mall\u001B[39m(\u001B[38;5;28misinstance\u001B[39m(b, csr_matrix)\n\u001B[1;32m    624\u001B[0m                                     \u001B[38;5;28;01mfor\u001B[39;00m b \u001B[38;5;129;01min\u001B[39;00m blocks\u001B[38;5;241m.\u001B[39mflat)):\n\u001B[1;32m    625\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m N \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[1;32m    626\u001B[0m         \u001B[38;5;66;03m# stack along columns (axis 1):\u001B[39;00m\n\u001B[0;32m--> 627\u001B[0m         blocks \u001B[38;5;241m=\u001B[39m [[\u001B[43m_stack_along_minor_axis\u001B[49m\u001B[43m(\u001B[49m\u001B[43mblocks\u001B[49m\u001B[43m[\u001B[49m\u001B[43mb\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m:\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m]\n\u001B[1;32m    628\u001B[0m                   \u001B[38;5;28;01mfor\u001B[39;00m b \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(M)]   \u001B[38;5;66;03m# must have shape: (M, 1)\u001B[39;00m\n\u001B[1;32m    629\u001B[0m         blocks \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39masarray(blocks, dtype\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mobject\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m    631\u001B[0m     \u001B[38;5;66;03m# stack along rows (axis 0):\u001B[39;00m\n",
      "File \u001B[0;32m~/Library/Mobile Documents/com~apple~CloudDocs/Documents/2/Diss/lexnorm/.venv/lib/python3.10/site-packages/scipy/sparse/_construct.py:484\u001B[0m, in \u001B[0;36m_stack_along_minor_axis\u001B[0;34m(blocks, axis)\u001B[0m\n\u001B[1;32m    482\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m data_cat\u001B[38;5;241m.\u001B[39msize \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m    483\u001B[0m     indptr_cat \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mconcatenate(indptr_list)\u001B[38;5;241m.\u001B[39mastype(idx_dtype)\n\u001B[0;32m--> 484\u001B[0m     indices_cat \u001B[38;5;241m=\u001B[39m (\u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconcatenate\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\u001B[43mb\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mindices\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mb\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mblocks\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    485\u001B[0m                    \u001B[38;5;241m.\u001B[39mastype(idx_dtype))\n\u001B[1;32m    486\u001B[0m     indptr \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mempty(constant_dim \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m, dtype\u001B[38;5;241m=\u001B[39midx_dtype)\n\u001B[1;32m    487\u001B[0m     indices \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mempty_like(indices_cat)\n",
      "File \u001B[0;32m<__array_function__ internals>:200\u001B[0m, in \u001B[0;36mconcatenate\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "rd_clf.decision_path(X_dev)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "outputs": [
    {
     "data": {
      "text/plain": "array(['cosine_to_orig', 'embeddings_rank', 'from_clipping',\n       'from_original_token', 'from_split', 'norms_seen',\n       'spellcheck_rank', 'in_lexicon', 'length', 'same_order',\n       'orig_norms_seen', 'orig_in_lexicon', 'orig_same_order',\n       'orig_length'], dtype=object)"
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rd_clf.feature_names_in_"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "array([2.52070851e-01, 4.31976125e-03, 7.92054467e-03, 1.64534043e-01,\n       1.61258200e-04, 4.01924066e-01, 3.03852361e-03, 7.35963146e-03,\n       6.02726879e-02, 9.02983575e-03, 3.24685801e-02, 7.44901879e-03,\n       0.00000000e+00, 4.94511981e-02])"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rd_clf.feature_importances_"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.8s finished\n"
     ]
    }
   ],
   "source": [
    "preds = rd_clf.predict_proba(X_dev)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "['brother',\n 'get',\n 'out',\n 'you',\n 'feelings',\n 'lol',\n 'manan',\n 'dund',\n 'paradox',\n 'ter',\n 'insulin',\n 'oroid',\n 'minii',\n 'aav',\n 'xezee',\n 'neg',\n 'cagt',\n 'couscous',\n 'baisan',\n 'photo',\n 'by',\n 'why',\n 'dese',\n 'niggers',\n 'think',\n 'dey',\n 'doing',\n 'summn',\n \"it's\",\n 'about',\n 'more',\n 'than',\n 'number',\n 'brother',\n 'and',\n \"i'm\",\n 'not',\n 'talking',\n 'about',\n 'statistics',\n \"i'm\",\n 'talking',\n 'about',\n 'skill',\n 'level',\n 'yes',\n 'omg',\n 'i',\n \"don't\",\n 'want',\n 'him',\n 'feeling',\n 'unappreciated',\n 'or',\n 'stuff',\n 'like',\n 'that',\n 'lmao',\n 'the',\n 'whole',\n 'time',\n 'actually',\n 'we',\n 'need',\n 'to',\n 'start',\n 'our',\n 'own',\n 'team',\n \"y'all\",\n 'were',\n 'wilding',\n 'last',\n 'nigjt',\n 'yoh',\n 'niya',\n 'ja',\n 'saying',\n 'kau',\n 'aku',\n 'with',\n 'couple',\n 'is',\n 'actually',\n 'sweet',\n 'k',\n 'michelle',\n 'met',\n 'you',\n 'match',\n 'that',\n 'fonk',\n 'rana',\n 'samaha',\n 'ignore',\n 'all',\n 'the',\n 'criticism',\n 'a',\n 'spotlight',\n 'can',\n 'never',\n 'see',\n 'the',\n 'shadows',\n 'anyways',\n \"you're\",\n 'flawless',\n 'boko',\n 'haram',\n 'estimate',\n 'warns',\n 'nigerian',\n 'leaders',\n 'over',\n 'politicization',\n 'of',\n 'chibok',\n 'abduction',\n 'and',\n 'in',\n 'that',\n 'moment',\n 'karma',\n 'realised',\n 'she',\n 'was',\n 'hella',\n 'gay',\n 'for',\n 'her',\n 'bestie',\n 'not',\n 'you',\n 'because',\n 'you',\n 'chillen',\n 'with',\n 'me',\n 'kardashia',\n 'kardashian',\n 'died',\n 'because',\n 'her',\n 'ass',\n 'was',\n 'too',\n 'big',\n 'hahah',\n 'niggra',\n 'summation',\n 'classified',\n 'directory',\n 'high',\n 'grille',\n 'for',\n 'the',\n 'online',\n 'game',\n 'atd',\n 'lexx',\n 'little',\n 'cute',\n 'ass',\n 'woke',\n 'up',\n 'mad',\n 'as',\n 'hell',\n 'lol',\n 'green',\n 'day',\n 'paramore',\n 'yellowcard',\n 'fall',\n 'out',\n 'boy',\n 'is',\n 'that',\n 'minho',\n 'beside',\n 'taemin',\n 'omo',\n 'looks',\n 'like',\n 'him',\n '140529',\n 'smtown',\n 'artists',\n 'with',\n 'exo',\n 'photoset',\n 'flintandpyrite',\n 'ume',\n 'by',\n 'andrea',\n 'rangel',\n 'knits',\n 'for',\n 'spring',\n 'because',\n \"that's\",\n 'going to',\n 'happen',\n 'lol',\n 'need',\n 'to',\n 'revisit',\n 'their',\n 'thameslink',\n 'bid',\n 'the',\n 'moorgate',\n 'route',\n 'closed',\n '4',\n 'years',\n 'ago',\n 'icons',\n 'the',\n 'demi',\n 'sem',\n 'psd',\n 'no',\n 'the',\n 'x',\n 'factoring',\n 'x8',\n 'you',\n \"don't\",\n \"i'm\",\n 'the',\n 'king',\n 'of',\n 'sweg',\n 'haha',\n \"that's\",\n 'funny',\n 'but',\n 'not',\n 'every',\n 'girl',\n 'is',\n 'darlin',\n 'imran',\n 'khan',\n 'saying',\n 'really',\n 'true',\n 'about',\n 'pmln',\n 'and',\n \"you'll\",\n 'always',\n 'be',\n 'my',\n '5',\n 'heros',\n 'who',\n 'can',\n 'always',\n 'make',\n 'me',\n 'smyle',\n 'even',\n 'when',\n 'i',\n 'm',\n 'in',\n 'the',\n 'sadest',\n 'mood',\n 'ever',\n 'xxc',\n 'lol',\n 'what',\n 'he',\n 'tripping',\n 'on',\n 'xabi',\n 'alonso',\n 'just',\n 'achieved',\n 'stratospheric',\n 'heights',\n 'of',\n 'coolness',\n 'lmaoo',\n 'my',\n 'fault',\n 'you',\n \"i'm going to\",\n 'let',\n 'you',\n 'get',\n 'your',\n 'shine',\n 'james',\n 'and',\n 'i',\n 'say',\n 'hi',\n 'with',\n 'a',\n 'dog',\n 'containerize',\n 'omg',\n 'hawt',\n 'like',\n 'omg',\n 'can',\n 'my',\n 'husband',\n 'please',\n 'look',\n 'like',\n 'you',\n 'or',\n 'can',\n 'i',\n 'have',\n 'your',\n 'babies',\n 'i',\n 'miss',\n 'going',\n 'to',\n 'mansion',\n 'elan',\n 'ery',\n 'keen',\n \"i'm\",\n 'going to',\n 'start',\n 'back',\n 'going',\n 'doe',\n 'facts',\n 'bro',\n 'this',\n 'babyface',\n \"ain't\",\n 'helping',\n 'nothing',\n 'lmao',\n 'can',\n 'i',\n 'have',\n 'him',\n 'yet',\n 'orrr',\n 'noo',\n 'ughh',\n '140524',\n 'hyoyeon',\n 'yuri',\n '2nd',\n 'day',\n 'kobe',\n 'by',\n 'rebecca',\n 'crowdfunding',\n 'roundup',\n 'a',\n 'late',\n 'spring',\n 'flowering',\n 'of',\n 'projects',\n 'every',\n 'week',\n 'tuaw',\n 'provides',\n 'reviewers',\n 'with',\n 'an',\n 'update',\n 'o',\n 'mayweather',\n 'needs',\n 'to',\n 'fight',\n 'paquiao',\n 'that',\n 'would',\n 'be',\n 'ammmazing',\n \"i'm\",\n 'actully',\n 'excited',\n 'to',\n 'see',\n 'what',\n 'it',\n 'will',\n 'look',\n 'like',\n 'so',\n 'its',\n 'happening',\n 'when',\n 'the',\n 'lawn',\n 'area',\n 'get',\n 'done',\n 'lol',\n 'who',\n 'about to',\n 'come',\n 'way',\n 'out',\n 'there',\n 'come',\n 'get',\n 'me',\n 'bring',\n 'me',\n 'back',\n \"we'll\",\n 'come',\n 'lol',\n 'i',\n 'forgot',\n 'you',\n 'moved',\n 'canal',\n 'then',\n 'lol',\n 'apple',\n 'forgets',\n 'to',\n 'renew',\n 'ssl',\n 'certificate',\n 'breaking',\n 'os',\n 'x',\n 'software',\n 'update',\n 'i know',\n 'id',\n 'rather',\n 'someone',\n 'unfollow',\n 'me',\n 'than',\n 'mute',\n 'me',\n 'like',\n 'what',\n 'oh',\n 'my',\n 'poor',\n 'zac',\n 'take',\n 'good',\n 'care',\n 'of',\n 'yourself',\n 'kimberley',\n 'walsh',\n 'confirms',\n 'cheryl',\n \"cole's\",\n \"britain's\",\n 'got',\n 'talent',\n 'appearance',\n 'contact music',\n 'comkimberley',\n 'walsh',\n 'confir',\n 'exo',\n 'is',\n 'practising',\n 'for',\n 'concert',\n 'lol',\n 'you',\n 'still',\n 'in',\n 'school',\n 'shut up',\n 'he',\n 'wants',\n 'to',\n 'get',\n 'bigger',\n 'than',\n 'parth',\n 'lmfao',\n 'chanyeol',\n 'is',\n 'so',\n 'caring',\n 'omg',\n 'be',\n 'mine',\n 'please',\n 'nothing',\n 'just',\n 'chilling',\n 'imy',\n 'you',\n \"don't\",\n 'fuck',\n 'with',\n 'a',\n 'thug',\n 'no more',\n 'lol',\n 'udah',\n 'lama',\n 'nda',\n 'how',\n 'about',\n 'it',\n 'how',\n 'about',\n 'that',\n 'lol',\n 'i know',\n 'i',\n 'just',\n 'took',\n 'them',\n 'real',\n 'quick',\n 'i',\n \"wasn't\",\n 'thinking',\n 'the',\n 'order',\n '1',\n 'daily',\n 'follower',\n '0',\n 'unfollowers',\n 'just unfollow',\n \"doesn't\",\n 'miss',\n 'a',\n 'trick',\n \"i'm\",\n 'going to',\n 'do',\n 'it',\n 'after',\n 'graduation',\n 'lol',\n 'second',\n 'time',\n 'this',\n 'shit',\n 'crashed',\n 'on',\n 'datpiff',\n 'so',\n 'we',\n 'on',\n 'soundcloud',\n 'with',\n 'it',\n 'genji',\n 'sports',\n 'pop',\n 'up',\n 'family',\n 'beach',\n 'tent',\n 'and',\n 'beach',\n 'sun shelter',\n 'genji',\n 'sports',\n 'pop',\n 'up',\n 'fami',\n 'titan',\n 'baseball',\n 'beats',\n 'brv',\n 'to',\n 'advance',\n 'to',\n 'county',\n 'championship',\n 'tri',\n 'tomorrow',\n '4',\n 'pm',\n 'jump seat',\n 'radio',\n '018',\n 'combat',\n 'ready',\n 'firefighting',\n 'with',\n 'tony',\n 'kelleher',\n 'repost',\n 'i',\n 'added',\n 'a',\n 'video',\n 'to',\n 'a',\n 'playlist',\n 'plg',\n 'seductive',\n 'retribution',\n 'ep',\n '3',\n \"let's\",\n 'tango',\n 'wildfires',\n 'turkey',\n 'feather',\n 'fletching',\n 'for',\n 'archery',\n 'arrows',\n 'diy',\n 'pheasant',\n 'feathers',\n 'for',\n 'arro',\n 'where',\n 'the',\n 'mj',\n 'though',\n 'lol',\n '492',\n 'luke',\n 'hemmings',\n 'from',\n '5sos',\n 'i',\n 'love',\n 'you',\n 'so',\n 'much',\n 'please',\n 'follow',\n 'me',\n 'you',\n 'are',\n 'everything',\n 'to',\n 'me',\n 'please',\n 'dude',\n 'you',\n 'never',\n 'gave',\n 'me',\n 'the',\n 'kik',\n 'or',\n 'snapchat',\n \"you'll\",\n 'sea',\n 'lmfao',\n 'oookay',\n 'bean',\n 'eating',\n 'headass',\n 'morning',\n 'exercise',\n 'paakyat',\n 'ng',\n 'thunderbird',\n 'i',\n 'love',\n 'you',\n 'phil',\n 'of',\n 'the',\n 'future',\n 'i',\n 'love',\n 'everything',\n 'about',\n 'you',\n 'nao',\n 'superei',\n 'ainda',\n 'this',\n 'is',\n 'for',\n 'niall',\n 'e',\n 'vcs',\n 'wara',\n 'man',\n 'talaga',\n 'forever',\n 'lanyard',\n 'ano',\n 'una',\n 'mo',\n 'imortal',\n 'ka',\n 'hahahahahahhahaha',\n 'hot',\n 'music',\n 'mariah',\n 'carey',\n 'meteorite',\n 'q',\n 'meteorite',\n 'version',\n 'mariah',\n 'carey',\n 'has',\n 'ohh',\n 'i',\n 'feel',\n 'you',\n 'haha',\n 'so',\n 'is',\n 'writing',\n 'a',\n 'book',\n 'about',\n 'jay',\n 'z',\n 'and',\n 'beyonce',\n 'telling',\n 'someone',\n \"else's\",\n 'secrets',\n 'for',\n 'money',\n 'shows',\n 'how',\n 'low',\n 'some',\n 'people',\n 'will',\n 'stoop',\n 'fast',\n 'forward',\n 'to',\n 'june',\n '9',\n 'please',\n 'para',\n 'makapag',\n 'ipon',\n 'na',\n 'hahahahahah',\n 'lol',\n 'thank',\n 'you',\n 'liam',\n 'for',\n 'asking',\n 'louis',\n 'what',\n 'flavour',\n 'harry',\n 'is',\n \"i'm\",\n 'so',\n 'jskcosnxoancoamxis',\n 'ahhhhh',\n 'salt',\n 'and',\n 'vinegar',\n \"don't\",\n 'take',\n 'it',\n 'personal',\n 'kid',\n 'caroline',\n 'wozniacki',\n 'breaks',\n 'silence',\n 'on',\n 'rory',\n 'mcilroy',\n 'breakup',\n '140525',\n 'boakwon',\n 'instagram',\n 'update',\n 'with',\n 'exo',\n 'and',\n 'suju',\n 'siwon',\n 'donghae',\n 'more',\n 'and',\n 'forehead',\n 'ft',\n 'hyuk',\n 'thanks',\n 'for',\n 'that',\n 'spiderman',\n 'carnage',\n 'mask',\n '300rb',\n 'lom',\n 'ama',\n 'ongkir',\n 'ready',\n 'stock',\n 'running',\n 'with',\n 'who',\n 'do',\n 'you',\n 'think',\n 'is',\n 'faster',\n 'nicki',\n 'minaj',\n 'slick',\n 'went',\n 'from',\n 'brownskin',\n 'to',\n 'lightskin',\n 'over',\n 'the',\n 'years',\n 'if',\n 'you',\n \"don't\",\n 'fuck',\n 'with',\n 'dunny',\n 'gage',\n 'then',\n 'fuck',\n 'ya',\n 'life',\n 'last',\n 'day',\n 'of',\n 'school',\n 'gonna',\n 'have',\n 'people',\n 'like',\n 'the',\n 'bad',\n 'pastor',\n 'impostors',\n 'so',\n 'loves',\n 'a',\n 'lottle',\n 'to',\n 'jesus',\n 'the',\n 'people',\n 'can',\n 'intercession',\n 'for',\n 'he',\n 'for',\n 'the',\n 'soul',\n 'peole',\n 'wilshire',\n 'itq',\n 'hehe',\n 'stay',\n 'strong',\n '1',\n 'k',\n 'swim',\n 'sauna',\n 'post',\n 'work',\n 'now',\n 'beer',\n 'work',\n 'tomorrow',\n 'terserah',\n 'suho',\n 'gave',\n 'a',\n 'teddy',\n 'bear',\n 'and',\n 'took',\n 'a',\n 'selca',\n 'with',\n 'a',\n 'fan',\n 'tazz',\n 'needa',\n 'quit',\n 'it',\n 'with',\n 'that',\n 'last',\n 'quote',\n 'lol',\n 'she',\n \"don't\",\n 'live',\n 'that',\n 'life',\n 'why',\n 'the',\n 'fuck',\n 'did',\n 'i',\n 'think',\n 'i',\n 'was',\n 'scheduled',\n 'today',\n 'am',\n 'i',\n 'on',\n 'crack',\n 'lmao',\n 'what',\n 'my',\n 'top',\n '3',\n 'artists',\n 'girls',\n 'aloud',\n '15',\n 'christina',\n 'aguilera',\n '13',\n 'kylie',\n 'minogue',\n '12',\n 'how',\n 'he',\n 'top',\n '5',\n 'scorer',\n 'and',\n 'the',\n 'offense',\n \"ain't\",\n 'around',\n 'atleast',\n 'him',\n 'lol',\n 'that',\n \"don't\",\n 'make',\n 'sense',\n 'that',\n 'mean',\n 'he',\n 'jackin',\n 'then',\n 'lol',\n 'kompany',\n 'going',\n 'up',\n 'to',\n 'yaya',\n 'toure',\n 'after',\n 'he',\n 'found',\n 'out',\n 'he',\n 'wanted',\n 'to',\n 'leave',\n 'first',\n 'class',\n 'thing',\n 'see',\n 'wizkid',\n 'and',\n 'banky',\n 'w',\n 'having',\n 'fun',\n 'on',\n 'via',\n 'the',\n 'old',\n 'bargin',\n 'giant',\n 'would',\n 'make',\n 'a',\n 'good',\n 'entertainment',\n ...]"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev[\"preds\"] = preds[:, 1]\n",
    "predictions = dev.sort_values(\"preds\", ascending=False).drop_duplicates([\"process\", \"tweet\", \"tok\"])\n",
    "pred_tokens = predictions.sort_values([\"process\", \"tweet\", \"tok\"]).index.tolist()\n",
    "pred_tokens"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "from lexnorm.generate_extract.filtering import is_eligible\n",
    "from lexnorm.data.normEval import loadNormData, evaluate\n",
    "raw, norm = loadNormData(os.path.join(DATA_PATH, \"raw/dev.norm\"))\n",
    "# pred_tokens_iter = iter(pred_tokens)\n",
    "#\n",
    "# pred_tweets = []\n",
    "#\n",
    "# for tweet in raw:\n",
    "#     pred_tweet = []\n",
    "#     for tok in tweet:\n",
    "#         if is_eligible(tok):\n",
    "#             pred_tweet.append(next(pred_tokens_iter))\n",
    "#         else:\n",
    "#             pred_tweet.append(tok)\n",
    "#     pred_tweets.append(pred_tweet)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline acc.(LAI): 93.10\n",
      "Accuracy:           96.28\n",
      "ERR:                46.13\n"
     ]
    },
    {
     "data": {
      "text/plain": "(0.9309630275929763, 0.9628094666812084, 0.4612954186413895)"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# better!!\n",
    "evaluate(raw, norm, pred_tweets)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[1.  , 0.  ],\n       [1.  , 0.  ],\n       [1.  , 0.  ],\n       ...,\n       [0.98, 0.02],\n       [1.  , 0.  ],\n       [1.  , 0.  ]])"
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from lexnorm.definitions import DATA_PATH\n",
    "\n",
    "train = pd.read_csv(os.path.join(DATA_PATH, \"hpc/train_processed_annotated.txt\"), index_col=0)\n",
    "dev = pd.read_csv(os.path.join(DATA_PATH, \"hpc/dev_processed.txt\"), index_col=0)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'function' object has no attribute 'loc'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[29], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mtrain\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mloc\u001B[49m[(train[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mprocess\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m63\u001B[39m) \u001B[38;5;241m&\u001B[39m (train[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtweet\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m) \u001B[38;5;241m&\u001B[39m (train[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtok\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m2\u001B[39m)]\n",
      "\u001B[0;31mAttributeError\u001B[0m: 'function' object has no attribute 'loc'"
     ]
    }
   ],
   "source": [
    "train.loc[(train['process'] == 63) & (train['tweet'] == 0) & (train['tok'] == 2)]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "from lexnorm.data.word2vec import get_vectors\n",
    "\n",
    "vectors = get_vectors(os.path.join(DATA_PATH, \"raw/train.norm\"))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/elijoe/Library/Mobile Documents/com~apple~CloudDocs/Documents/2/Diss/lexnorm/.venv/lib/python3.10/site-packages/gensim/models/keyedvectors.py:849: RuntimeWarning: invalid value encountered in divide\n",
      "  dists = dot(self.vectors[clip_start:clip_end], mean) / self.norms[clip_start:clip_end]\n"
     ]
    },
    {
     "data": {
      "text/plain": "     cosine_to_orig  embeddings_rank\nyea        0.912848                1\nyes        0.821176                2\noh         0.817392                3\nnah        0.798594                4\nuh         0.792015                5",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>cosine_to_orig</th>\n      <th>embeddings_rank</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>yea</th>\n      <td>0.912848</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>yes</th>\n      <td>0.821176</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>oh</th>\n      <td>0.817392</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>nah</th>\n      <td>0.798594</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>uh</th>\n      <td>0.792015</td>\n      <td>5</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from lexnorm.generate_extract.modules import word_embeddings\n",
    "\n",
    "word_embeddings(\"yeah\", vectors)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "data": {
      "text/plain": "process  tweet  tok\n0        0      1       16\n                2       34\n                3      291\n                4      113\n                5       11\n                      ... \n58       9      10      49\n                11      55\n                12      38\n                13       6\n                14       1\nLength: 6876, dtype: int64"
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from lexnorm.models.random_forest import train\n",
    "\n",
    "dev.groupby([\"process\", \"tweet\", \"tok\"]).size()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6876\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "\n",
    "for tweet in raw:\n",
    "    for tok in tweet:\n",
    "        if is_eligible(tok):\n",
    "            count += 1\n",
    "\n",
    "print(count)\n",
    "# as expected - candidates generated for every eligible raw token"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lexnorm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[62], line 6\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mlexnorm\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmodels\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m random_forest\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mimportlib\u001B[39;00m\n\u001B[0;32m----> 6\u001B[0m importlib\u001B[38;5;241m.\u001B[39mreload(\u001B[43mlexnorm\u001B[49m\u001B[38;5;241m.\u001B[39mmodels\u001B[38;5;241m.\u001B[39mnormalise\u001B[38;5;241m.\u001B[39mprep_test)\n\u001B[1;32m      7\u001B[0m clf \u001B[38;5;241m=\u001B[39m joblib\u001B[38;5;241m.\u001B[39mload(os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(DATA_PATH, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m../models/rf.joblib\u001B[39m\u001B[38;5;124m\"\u001B[39m))\n\u001B[1;32m      8\u001B[0m dev_X \u001B[38;5;241m=\u001B[39m prep_test(os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(DATA_PATH, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhpc/dev_processed.txt\u001B[39m\u001B[38;5;124m\"\u001B[39m))\n",
      "\u001B[0;31mNameError\u001B[0m: name 'lexnorm' is not defined"
     ]
    }
   ],
   "source": [
    "from lexnorm.models.normalise import prep_test, normalise\n",
    "import joblib\n",
    "\n",
    "from lexnorm.models import random_forest\n",
    "clf = joblib.load(os.path.join(DATA_PATH, \"../models/rf.joblib\"))\n",
    "dev_X = prep_test(os.path.join(DATA_PATH, \"hpc/dev_processed.txt\"))\n",
    "dev = pd.read_csv(os.path.join(DATA_PATH, \"hpc/dev_processed.txt\"), index_col=0)\n",
    "preds = random_forest.predict(clf, dev_X, dev)\n",
    "predictions = normalise(raw, preds)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yo your you\n",
      "xaragdax xaragdax paradox\n",
      "uuliin uuliin insulin\n",
      "zogsoj zogsoj couscous\n",
      "dese these dese\n",
      "dey they dey\n",
      "summn something summn\n",
      "nigjt night nigjt\n",
      "shettima shettima estimate\n",
      "realised realized realised\n",
      "hella hell of a lot of hella\n",
      "bestie best friend bestie\n",
      "chillen chilling chillen\n",
      "niggra nigger niggra\n",
      "yellowcard yellow card yellowcard\n",
      "da da the\n",
      "factor factor factoring\n",
      "darlin darling darlin\n",
      "smyle smile smyle\n",
      "sadest saddest sadest\n",
      "tenerezza tenerezza containerize\n",
      "wkeend weekend keen\n",
      "bro brother bro\n",
      "babyface baby face babyface\n",
      "orrr or orrr\n",
      "noo no noo\n",
      "readers readers reviewers\n",
      "ammmazing amazing ammmazing\n",
      "actully actually actully\n",
      "id i'd id\n",
      "ohhh ohhh oh\n",
      "contactmusic contactmusic contact music\n",
      "datpiff the piff datpiff\n",
      "soundcloud sound cloud soundcloud\n",
      "sunshelter sunshelter sun shelter\n",
      "jumpseat jumpseat jump seat\n",
      "sensual sensual seductive\n",
      "toribash toribash retribution\n",
      "diy do it yourself diy\n",
      "oookay okay oookay\n",
      "headass head ass headass\n",
      "niyata niyata lanyard\n",
      "flavour flavor flavour\n",
      "n n and\n",
      "brownskin brown skin brownskin\n",
      "lightskin light skin lightskin\n",
      "ya your ya\n",
      "lottle lot lottle\n",
      "intercesion intercesion intercession\n",
      "peole people peole\n",
      "atleast at least atleast\n",
      "jackin jacking jackin\n",
      "centre center centre\n",
      "akukamba akukamba akimbo\n",
      "rt rt retweet\n",
      "vag vagina vag\n",
      "thang thing thang\n",
      "tho though tho\n",
      "carcillo carcillo gazillion\n",
      "yovani yovani ovarian\n",
      "nows now is nows\n",
      "pdx portland pdx\n",
      "glisan glisan glissandi\n",
      "whatdoiwear what do i wear whatdoiwear\n",
      "brutha brother brutha\n",
      "asst assistant asst\n",
      "yah you yah\n",
      "n n and\n",
      "fav favorite fav\n",
      "falaknuma falaknuma flakiness\n",
      "whisky whiskey whisky\n",
      "nid need nid\n",
      "2 to 2\n",
      "meetin meeting meetin\n",
      "tho though tho\n",
      "openheart openheart open heart\n",
      "shizz shit shizz\n",
      "ill i'll ill\n",
      "shoutout shout out shoutout\n",
      "d d the\n",
      "bre breath bre\n",
      "happylgovo happy happylgovo\n",
      "gal guy gal\n",
      "nd and nd\n",
      "liek like liek\n",
      "nd and nd\n",
      "doesng doesn't doesng\n",
      "lang long lang\n",
      "yhu you yhu\n",
      "nuh know nuh\n",
      "ntn nothing ntn\n",
      "wearr wear wearr\n",
      "reherseal rehearsal reherseal\n",
      "thotler thotler throttler\n",
      "mnl my new love mnl\n",
      "alrdy already alrdy\n",
      "bongkaran bongkaran background\n",
      "ya you ya\n",
      "poooollll pool poooollll\n",
      "sence sense sence\n",
      "humour humor humour\n",
      "ish ish shit\n",
      "tho though tho\n",
      "tho though tho\n",
      "havr have havr\n",
      "dogg dog dogg\n",
      "dogg dog dogg\n",
      "goddamit god damn it goddamit\n",
      "jornet jornet cornet\n",
      "zegama zegama amalgam\n",
      "b be b\n",
      "w with w\n",
      "throught through throught\n",
      "throught through throught\n",
      "singapore singapore singletree\n",
      "viru virus viru\n",
      "theirselves themselves theirselves\n",
      "golelhum golelhum glenohumeral\n",
      "farrukh farrukh farrago\n",
      "getcha get you getcha\n",
      "openfollow open follow openfollow\n",
      "sayeng sayeng sang\n",
      "vuldemurt vuldemurt voltmeter\n",
      "ur you're your\n",
      "wezird wezird weir\n",
      "ca'nt can't ca'nt\n",
      "whr where whr\n",
      "peewee peewee pee wee\n",
      "voy boy voy\n",
      "photobomb photo bomb photobomb\n",
      "photobomb photo bomb photobomb\n",
      "shoutout shout out shoutout\n",
      "skepta sunglasses skepta\n",
      "youtbe youtube youtbe\n",
      "d d the\n",
      "filmz film filmz\n",
      "bro brother bro\n",
      "adultwork adult work adultwork\n",
      "devs developer devs\n",
      "pies pies pie's\n",
      "ym my ym\n",
      "gdo god gdo\n",
      "keepin keeping keepin\n",
      "witchu with you witchu\n",
      "witchu with you witchu\n",
      "prez president prez\n",
      "quil quill quil\n",
      "nem they nem\n",
      "yo your you\n",
      "fkn fucking fkn\n",
      "tickt ticket tickt\n",
      "swsh swsh swoosh\n",
      "tix tickets ticket\n",
      "ya you ya\n",
      "sim seems sim\n",
      "upto up to upto\n",
      "nizamabad nizamabad consumable\n",
      "ya you ya\n",
      "clareta clareta clarity\n",
      "gospe gospel gospe\n",
      "switc switch switc\n",
      "alot a lot alot\n",
      "fav favorite fav\n",
      "ill i'll ill\n",
      "ill i'll ill\n",
      "rememberin remembering rememberin\n",
      "prayin praying prayin\n",
      "em em them\n",
      "dm dm direct message\n",
      "mamodou mamodou marmot\n",
      "1000follow 1000follow follow\n",
      "satnite saturday night satnite\n",
      "ya you ya\n",
      "esp especially esp\n",
      "hazard hazard hazard's\n",
      "asap asap as soon as possible\n",
      "diaporama diaporama pyramidal\n",
      "clonedvd clonedvd cloned\n",
      "haboyo haboyo harbor\n",
      "kaygeo kaygeo kayo\n",
      "hw homework hw\n",
      "nuffin nothing nuffin\n",
      "diss this diss\n",
      "hapi happy hapi\n",
      "2 to 2\n",
      "nd and nd\n",
      "nd and nd\n",
      "nd and nd\n",
      "2 to 2\n",
      "tho though tho\n",
      "legt left legt\n",
      "doesnot doesn't doesnot\n",
      "inspite in spite inspite\n",
      "r r are\n",
      "452740 452740 4th\n",
      "hapi happy hapi\n",
      "weeknd weeknd weekend\n",
      "hbu how about you hbu\n",
      "yessss yes yessss\n",
      "fu fu fuck\n",
      "fu fu fuck\n",
      "d d the\n",
      "ofcourse of course ofcourse\n",
      "vas was vas\n",
      "happenin happening happenin\n",
      "los los las\n",
      "boliches boliches abolishes\n",
      "closeeee close closeeee\n",
      "turnup turn up turnup\n",
      "redsox red sox redsox\n",
      "lifeee life lifeee\n",
      "loveholic loveholic alcoholic\n",
      "harrystyles harry styles harrystyles\n",
      "feela feels feela\n",
      "runnn run runnn\n",
      "townvilla townvilla town villa\n",
      "moodbreaker mood breaker moodbreaker\n",
      "moodbooster mood booster moodbooster\n",
      "marketwatch market watch marketwatch\n",
      "marketwatch market watch marketwatch\n",
      "fanmeet fan meet fanmeet\n",
      "ur you're your\n",
      "fav favorite fav\n",
      "sup what's up sup\n",
      "fatass fat ass fatass\n",
      "wassup what's up wassup\n",
      "meth methamphetamine meth\n",
      "meth methamphetamine meth\n",
      "d d the\n",
      "cocky cocky conceited\n",
      "c see c\n",
      "c see c\n",
      "4 for 4\n",
      "za that za\n",
      "wateva whatever wateva\n",
      "cums comes cums\n",
      "dnt doesn't don't\n",
      "los los las\n",
      "angeles angeles francisco\n",
      "thou though thou\n",
      "andd and andd\n",
      "ur you're your\n",
      "no not no\n",
      "polls polls elections\n",
      "singles singles dingles\n",
      "da da the\n",
      "mammaye mammaye mayoress\n",
      "nw nw now\n",
      "waitin waiting waitin\n",
      "its it's its\n",
      "onew one onew\n",
      "angeres angeres anger\n",
      "crippers crippers crippler\n",
      "sweatin sweating sweatin\n",
      "otha other otha\n",
      "yuh you yuh\n",
      "yuh you yuh\n",
      "judgement judgment judgement\n",
      "fav favorite fav\n",
      "meth methamphetamine meth\n",
      "cupcake cupcake shortcake\n",
      "orenjy orenjy orange\n",
      "mountsby mountsby mounts by\n",
      "incase in case incase\n",
      "shat shit shat\n",
      "d d the\n",
      "yea you yeah\n",
      "camikum camikum cadmium\n",
      "openfollow open follow openfollow\n",
      "thankyou thank you thankyou\n",
      "anyones anyone anyones\n",
      "tech tech techs\n",
      "sawtell sawtell satellite\n",
      "slp sleep slp\n",
      "famly family famly\n",
      "cupcake cupcake shortcake\n",
      "swearin swearing swearin\n",
      "o on o\n",
      "4 for 4\n",
      "dia their dia\n",
      "gd gd good\n",
      "creekview creek view creekview\n",
      "bc bc because\n",
      "yo your you\n",
      "fuccin fucking fuccin\n",
      "cuh see you cuh\n",
      "3volution 3volution convolution\n",
      "hovsepian hovsepian overspent\n",
      "tonyselznick tonyselznick tony selznick\n",
      "trynna trying to trynna\n",
      "c see c\n",
      "hw homework hw\n",
      "w with w\n",
      "wayeee wayeee freeway\n",
      "pepl people pepl\n",
      "heurelho heurelho heraldic\n",
      "ish ish shit\n",
      "nuff enough nuff\n",
      "teamchill teamchill team chill\n",
      "eatin eating eatin\n",
      "pre preorder pre\n",
      "order  order\n",
      "asheville asheville archival\n",
      "asheville asheville archival\n",
      "donut doughnut donut\n",
      "definitley definitely definitley\n",
      "pics pics pictures\n",
      "yeh yeah yeh\n",
      "tix tickets ticket\n",
      "miles miles kilometers\n",
      "da that the\n",
      "kishwar kishwar coachwork\n",
      "bedi's bedi's bedding's\n",
      "mazkirah mazkirah muskrat\n",
      "rip rip rest in peace\n",
      "v very v\n",
      "l'wren l'wren wren\n",
      "waitin waiting waitin\n",
      "b be b\n",
      "w with w\n",
      "af as fuck af\n",
      "czech czech ukrainian\n",
      "cannnot can't cannnot\n",
      "mybe maybe mybe\n",
      "ull you will ull\n",
      "fqauwh fqauwh foxhole\n",
      "icant i can't icant\n",
      "harrystyles harry styles harrystyles\n",
      "mah my mah\n",
      "brotha brother brotha\n",
      "dottie's dottie's dot tie's\n",
      "v very v\n",
      "b be b\n",
      "rangera rangera anger\n",
      "wada water wada\n",
      "suckin sucking suckin\n",
      "chu you chu\n",
      "loctite loctite lactate\n",
      "epoxy epoxy thermoplastic\n",
      "loctite loctite lactate\n",
      "Baseline acc.(LAI): 93.10\n",
      "Accuracy:           96.29\n",
      "ERR:                46.29\n"
     ]
    },
    {
     "data": {
      "text/plain": "(0.9309630275929763, 0.9629185298287709, 0.4628751974723546)"
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(raw, norm, predictions, verbose=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "114.09050270864114\n",
      "113.20335644201992\n"
     ]
    }
   ],
   "source": [
    "dev = pd.read_csv(os.path.join(DATA_PATH, \"hpc/train_processed_annotated.txt\"), index_col=0)\n",
    "print(dev.groupby([\"process\", \"tweet\", \"tok\"]).size().mean())\n",
    "dev = pd.read_csv(os.path.join(DATA_PATH, \"hpc/train_annotated.txt\"), index_col=0)\n",
    "print(dev.groupby([\"process\", \"tweet\", \"tok\"]).size().mean())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "outputs": [
    {
     "data": {
      "text/plain": "        cosine_to_orig  embeddings_rank  from_clipping  from_original_token  \\\nnah           0.798594              3.0            NaN                  NaN   \noh            0.817392              2.0            NaN                  NaN   \nuh            0.792014              4.0            NaN                  NaN   \nye ah              NaN              NaN            NaN                  NaN   \nye-ah              NaN              NaN            NaN                  NaN   \nyea           0.912848              0.0            NaN                  NaN   \nyea h              NaN              NaN            NaN                  NaN   \nyea-h              NaN              NaN            NaN                  NaN   \nyeah          1.000000              NaN            1.0                  1.0   \nyeah's        0.587786              NaN            1.0                  NaN   \nyeahs         0.659100              NaN            1.0                  NaN   \nyear          0.396437              NaN            NaN                  NaN   \nyeas          0.574666              NaN            NaN                  NaN   \nyeoman        0.221668              NaN            NaN                  NaN   \nyes           0.821176              1.0            NaN                  NaN   \nyeti          0.378206              NaN            NaN                  NaN   \n\n        from_split  norms_seen  spellcheck_rank  in_lexicon  length  \\\nnah            NaN         NaN              NaN         NaN       3   \noh             NaN         NaN              NaN         1.0       2   \nuh             NaN         NaN              NaN         1.0       2   \nye ah          1.0         NaN              4.0         NaN       5   \nye-ah          NaN         NaN              5.0         NaN       5   \nyea            NaN         NaN              0.0         1.0       3   \nyea h          NaN         NaN              6.0         NaN       5   \nyea-h          NaN         NaN              7.0         NaN       5   \nyeah           NaN        18.0              NaN         1.0       4   \nyeah's         NaN         NaN              NaN         1.0       6   \nyeahs          NaN         NaN              1.0         1.0       5   \nyear           NaN         NaN              3.0         1.0       4   \nyeas           NaN         NaN              2.0         1.0       4   \nyeoman         NaN         NaN              9.0         1.0       6   \nyes            NaN         NaN              NaN         1.0       3   \nyeti           NaN         NaN              8.0         1.0       4   \n\n        same_order  orig_norms_seen  orig_in_lexicon  orig_same_order  \\\nnah            NaN             18.0              1.0              1.0   \noh             NaN             18.0              1.0              1.0   \nuh             NaN             18.0              1.0              1.0   \nye ah          1.0             18.0              1.0              1.0   \nye-ah          1.0             18.0              1.0              1.0   \nyea            NaN             18.0              1.0              1.0   \nyea h          1.0             18.0              1.0              1.0   \nyea-h          1.0             18.0              1.0              1.0   \nyeah           1.0             18.0              1.0              1.0   \nyeah's         1.0             18.0              1.0              1.0   \nyeahs          1.0             18.0              1.0              1.0   \nyear           NaN             18.0              1.0              1.0   \nyeas           NaN             18.0              1.0              1.0   \nyeoman         NaN             18.0              1.0              1.0   \nyes            NaN             18.0              1.0              1.0   \nyeti           NaN             18.0              1.0              1.0   \n\n        orig_length  correct  process  tweet  tok  gold  \nnah             4.0      NaN       63      0    2  yeah  \noh              4.0      NaN       63      0    2  yeah  \nuh              4.0      NaN       63      0    2  yeah  \nye ah           4.0      NaN       63      0    2  yeah  \nye-ah           4.0      NaN       63      0    2  yeah  \nyea             4.0      NaN       63      0    2  yeah  \nyea h           4.0      NaN       63      0    2  yeah  \nyea-h           4.0      NaN       63      0    2  yeah  \nyeah            4.0      1.0       63      0    2  yeah  \nyeah's          4.0      NaN       63      0    2  yeah  \nyeahs           4.0      NaN       63      0    2  yeah  \nyear            4.0      NaN       63      0    2  yeah  \nyeas            4.0      NaN       63      0    2  yeah  \nyeoman          4.0      NaN       63      0    2  yeah  \nyes             4.0      NaN       63      0    2  yeah  \nyeti            4.0      NaN       63      0    2  yeah  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>cosine_to_orig</th>\n      <th>embeddings_rank</th>\n      <th>from_clipping</th>\n      <th>from_original_token</th>\n      <th>from_split</th>\n      <th>norms_seen</th>\n      <th>spellcheck_rank</th>\n      <th>in_lexicon</th>\n      <th>length</th>\n      <th>same_order</th>\n      <th>orig_norms_seen</th>\n      <th>orig_in_lexicon</th>\n      <th>orig_same_order</th>\n      <th>orig_length</th>\n      <th>correct</th>\n      <th>process</th>\n      <th>tweet</th>\n      <th>tok</th>\n      <th>gold</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>nah</th>\n      <td>0.798594</td>\n      <td>3.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>3</td>\n      <td>NaN</td>\n      <td>18.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>4.0</td>\n      <td>NaN</td>\n      <td>63</td>\n      <td>0</td>\n      <td>2</td>\n      <td>yeah</td>\n    </tr>\n    <tr>\n      <th>oh</th>\n      <td>0.817392</td>\n      <td>2.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1.0</td>\n      <td>2</td>\n      <td>NaN</td>\n      <td>18.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>4.0</td>\n      <td>NaN</td>\n      <td>63</td>\n      <td>0</td>\n      <td>2</td>\n      <td>yeah</td>\n    </tr>\n    <tr>\n      <th>uh</th>\n      <td>0.792014</td>\n      <td>4.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1.0</td>\n      <td>2</td>\n      <td>NaN</td>\n      <td>18.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>4.0</td>\n      <td>NaN</td>\n      <td>63</td>\n      <td>0</td>\n      <td>2</td>\n      <td>yeah</td>\n    </tr>\n    <tr>\n      <th>ye ah</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1.0</td>\n      <td>NaN</td>\n      <td>4.0</td>\n      <td>NaN</td>\n      <td>5</td>\n      <td>1.0</td>\n      <td>18.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>4.0</td>\n      <td>NaN</td>\n      <td>63</td>\n      <td>0</td>\n      <td>2</td>\n      <td>yeah</td>\n    </tr>\n    <tr>\n      <th>ye-ah</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>5.0</td>\n      <td>NaN</td>\n      <td>5</td>\n      <td>1.0</td>\n      <td>18.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>4.0</td>\n      <td>NaN</td>\n      <td>63</td>\n      <td>0</td>\n      <td>2</td>\n      <td>yeah</td>\n    </tr>\n    <tr>\n      <th>yea</th>\n      <td>0.912848</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>3</td>\n      <td>NaN</td>\n      <td>18.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>4.0</td>\n      <td>NaN</td>\n      <td>63</td>\n      <td>0</td>\n      <td>2</td>\n      <td>yeah</td>\n    </tr>\n    <tr>\n      <th>yea h</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>6.0</td>\n      <td>NaN</td>\n      <td>5</td>\n      <td>1.0</td>\n      <td>18.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>4.0</td>\n      <td>NaN</td>\n      <td>63</td>\n      <td>0</td>\n      <td>2</td>\n      <td>yeah</td>\n    </tr>\n    <tr>\n      <th>yea-h</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>7.0</td>\n      <td>NaN</td>\n      <td>5</td>\n      <td>1.0</td>\n      <td>18.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>4.0</td>\n      <td>NaN</td>\n      <td>63</td>\n      <td>0</td>\n      <td>2</td>\n      <td>yeah</td>\n    </tr>\n    <tr>\n      <th>yeah</th>\n      <td>1.000000</td>\n      <td>NaN</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>NaN</td>\n      <td>18.0</td>\n      <td>NaN</td>\n      <td>1.0</td>\n      <td>4</td>\n      <td>1.0</td>\n      <td>18.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>4.0</td>\n      <td>1.0</td>\n      <td>63</td>\n      <td>0</td>\n      <td>2</td>\n      <td>yeah</td>\n    </tr>\n    <tr>\n      <th>yeah's</th>\n      <td>0.587786</td>\n      <td>NaN</td>\n      <td>1.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1.0</td>\n      <td>6</td>\n      <td>1.0</td>\n      <td>18.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>4.0</td>\n      <td>NaN</td>\n      <td>63</td>\n      <td>0</td>\n      <td>2</td>\n      <td>yeah</td>\n    </tr>\n    <tr>\n      <th>yeahs</th>\n      <td>0.659100</td>\n      <td>NaN</td>\n      <td>1.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>5</td>\n      <td>1.0</td>\n      <td>18.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>4.0</td>\n      <td>NaN</td>\n      <td>63</td>\n      <td>0</td>\n      <td>2</td>\n      <td>yeah</td>\n    </tr>\n    <tr>\n      <th>year</th>\n      <td>0.396437</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>3.0</td>\n      <td>1.0</td>\n      <td>4</td>\n      <td>NaN</td>\n      <td>18.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>4.0</td>\n      <td>NaN</td>\n      <td>63</td>\n      <td>0</td>\n      <td>2</td>\n      <td>yeah</td>\n    </tr>\n    <tr>\n      <th>yeas</th>\n      <td>0.574666</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>4</td>\n      <td>NaN</td>\n      <td>18.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>4.0</td>\n      <td>NaN</td>\n      <td>63</td>\n      <td>0</td>\n      <td>2</td>\n      <td>yeah</td>\n    </tr>\n    <tr>\n      <th>yeoman</th>\n      <td>0.221668</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>9.0</td>\n      <td>1.0</td>\n      <td>6</td>\n      <td>NaN</td>\n      <td>18.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>4.0</td>\n      <td>NaN</td>\n      <td>63</td>\n      <td>0</td>\n      <td>2</td>\n      <td>yeah</td>\n    </tr>\n    <tr>\n      <th>yes</th>\n      <td>0.821176</td>\n      <td>1.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1.0</td>\n      <td>3</td>\n      <td>NaN</td>\n      <td>18.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>4.0</td>\n      <td>NaN</td>\n      <td>63</td>\n      <td>0</td>\n      <td>2</td>\n      <td>yeah</td>\n    </tr>\n    <tr>\n      <th>yeti</th>\n      <td>0.378206</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>8.0</td>\n      <td>1.0</td>\n      <td>4</td>\n      <td>NaN</td>\n      <td>18.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>4.0</td>\n      <td>NaN</td>\n      <td>63</td>\n      <td>0</td>\n      <td>2</td>\n      <td>yeah</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev = pd.read_csv(os.path.join(DATA_PATH, \"hpc/train_annotated.txt\"), index_col=0)\n",
    "dev.loc[(dev['process'] == 63) & (dev['tweet'] == 0) & (dev['tok'] == 2)]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "outputs": [
    {
     "data": {
      "text/plain": "        cosine_to_orig  embeddings_rank  from_clipping  from_original_token  \\\nleah          0.384448              NaN            NaN                  NaN   \nnah           0.798594              5.0            NaN                  NaN   \noh            0.817392              4.0            NaN                  NaN   \nuh            0.792014              6.0            NaN                  NaN   \nye ah              NaN              NaN            NaN                  NaN   \nye-ah              NaN              NaN            NaN                  NaN   \nyea           0.912848              1.0            NaN                  NaN   \nyea h              NaN              NaN            NaN                  NaN   \nyea-h              NaN              NaN            NaN                  NaN   \nyeah          1.000000              2.0            NaN                  1.0   \nyeah's        0.587786              NaN            1.0                  NaN   \nyeahs         0.659100              NaN            1.0                  NaN   \nyear          0.396437              NaN            NaN                  NaN   \nyeas          0.574666              NaN            NaN                  NaN   \nyeoman        0.221668              NaN            NaN                  NaN   \nyes           0.821176              3.0            NaN                  NaN   \nyeti          0.378206              NaN            NaN                  NaN   \n\n        from_split  norms_seen  spellcheck_rank  in_lexicon  length  \\\nleah           NaN         NaN              5.0         1.0       4   \nnah            NaN         NaN              NaN         NaN       3   \noh             NaN         NaN              NaN         1.0       2   \nuh             NaN         NaN              NaN         1.0       2   \nye ah          1.0         NaN              6.0         NaN       5   \nye-ah          NaN         NaN              7.0         NaN       5   \nyea            NaN         NaN              1.0         1.0       3   \nyea h          NaN         NaN              8.0         NaN       5   \nyea-h          NaN         NaN              9.0         NaN       5   \nyeah           NaN        18.0              NaN         1.0       4   \nyeah's         NaN         NaN              NaN         1.0       6   \nyeahs          NaN         NaN              2.0         1.0       5   \nyear           NaN         NaN              4.0         1.0       4   \nyeas           NaN         NaN              3.0         1.0       4   \nyeoman         NaN         NaN             11.0         1.0       6   \nyes            NaN         NaN              NaN         1.0       3   \nyeti           NaN         NaN             10.0         1.0       4   \n\n        same_order  orig_norms_seen  orig_in_lexicon  orig_same_order  \\\nleah           NaN             18.0              1.0              1.0   \nnah            NaN             18.0              1.0              1.0   \noh             NaN             18.0              1.0              1.0   \nuh             NaN             18.0              1.0              1.0   \nye ah          1.0             18.0              1.0              1.0   \nye-ah          1.0             18.0              1.0              1.0   \nyea            NaN             18.0              1.0              1.0   \nyea h          1.0             18.0              1.0              1.0   \nyea-h          1.0             18.0              1.0              1.0   \nyeah           1.0             18.0              1.0              1.0   \nyeah's         1.0             18.0              1.0              1.0   \nyeahs          1.0             18.0              1.0              1.0   \nyear           NaN             18.0              1.0              1.0   \nyeas           NaN             18.0              1.0              1.0   \nyeoman         NaN             18.0              1.0              1.0   \nyes            NaN             18.0              1.0              1.0   \nyeti           NaN             18.0              1.0              1.0   \n\n        orig_length   raw  gold  correct  process  tweet  tok  \nleah            4.0  yeah  yeah      NaN       63      0    2  \nnah             4.0  yeah  yeah      NaN       63      0    2  \noh              4.0  yeah  yeah      NaN       63      0    2  \nuh              4.0  yeah  yeah      NaN       63      0    2  \nye ah           4.0  yeah  yeah      NaN       63      0    2  \nye-ah           4.0  yeah  yeah      NaN       63      0    2  \nyea             4.0  yeah  yeah      NaN       63      0    2  \nyea h           4.0  yeah  yeah      NaN       63      0    2  \nyea-h           4.0  yeah  yeah      NaN       63      0    2  \nyeah            4.0  yeah  yeah      1.0       63      0    2  \nyeah's          4.0  yeah  yeah      NaN       63      0    2  \nyeahs           4.0  yeah  yeah      NaN       63      0    2  \nyear            4.0  yeah  yeah      NaN       63      0    2  \nyeas            4.0  yeah  yeah      NaN       63      0    2  \nyeoman          4.0  yeah  yeah      NaN       63      0    2  \nyes             4.0  yeah  yeah      NaN       63      0    2  \nyeti            4.0  yeah  yeah      NaN       63      0    2  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>cosine_to_orig</th>\n      <th>embeddings_rank</th>\n      <th>from_clipping</th>\n      <th>from_original_token</th>\n      <th>from_split</th>\n      <th>norms_seen</th>\n      <th>spellcheck_rank</th>\n      <th>in_lexicon</th>\n      <th>length</th>\n      <th>same_order</th>\n      <th>orig_norms_seen</th>\n      <th>orig_in_lexicon</th>\n      <th>orig_same_order</th>\n      <th>orig_length</th>\n      <th>raw</th>\n      <th>gold</th>\n      <th>correct</th>\n      <th>process</th>\n      <th>tweet</th>\n      <th>tok</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>leah</th>\n      <td>0.384448</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>5.0</td>\n      <td>1.0</td>\n      <td>4</td>\n      <td>NaN</td>\n      <td>18.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>4.0</td>\n      <td>yeah</td>\n      <td>yeah</td>\n      <td>NaN</td>\n      <td>63</td>\n      <td>0</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>nah</th>\n      <td>0.798594</td>\n      <td>5.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>3</td>\n      <td>NaN</td>\n      <td>18.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>4.0</td>\n      <td>yeah</td>\n      <td>yeah</td>\n      <td>NaN</td>\n      <td>63</td>\n      <td>0</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>oh</th>\n      <td>0.817392</td>\n      <td>4.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1.0</td>\n      <td>2</td>\n      <td>NaN</td>\n      <td>18.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>4.0</td>\n      <td>yeah</td>\n      <td>yeah</td>\n      <td>NaN</td>\n      <td>63</td>\n      <td>0</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>uh</th>\n      <td>0.792014</td>\n      <td>6.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1.0</td>\n      <td>2</td>\n      <td>NaN</td>\n      <td>18.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>4.0</td>\n      <td>yeah</td>\n      <td>yeah</td>\n      <td>NaN</td>\n      <td>63</td>\n      <td>0</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>ye ah</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1.0</td>\n      <td>NaN</td>\n      <td>6.0</td>\n      <td>NaN</td>\n      <td>5</td>\n      <td>1.0</td>\n      <td>18.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>4.0</td>\n      <td>yeah</td>\n      <td>yeah</td>\n      <td>NaN</td>\n      <td>63</td>\n      <td>0</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>ye-ah</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>7.0</td>\n      <td>NaN</td>\n      <td>5</td>\n      <td>1.0</td>\n      <td>18.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>4.0</td>\n      <td>yeah</td>\n      <td>yeah</td>\n      <td>NaN</td>\n      <td>63</td>\n      <td>0</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>yea</th>\n      <td>0.912848</td>\n      <td>1.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>3</td>\n      <td>NaN</td>\n      <td>18.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>4.0</td>\n      <td>yeah</td>\n      <td>yeah</td>\n      <td>NaN</td>\n      <td>63</td>\n      <td>0</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>yea h</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>8.0</td>\n      <td>NaN</td>\n      <td>5</td>\n      <td>1.0</td>\n      <td>18.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>4.0</td>\n      <td>yeah</td>\n      <td>yeah</td>\n      <td>NaN</td>\n      <td>63</td>\n      <td>0</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>yea-h</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>9.0</td>\n      <td>NaN</td>\n      <td>5</td>\n      <td>1.0</td>\n      <td>18.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>4.0</td>\n      <td>yeah</td>\n      <td>yeah</td>\n      <td>NaN</td>\n      <td>63</td>\n      <td>0</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>yeah</th>\n      <td>1.000000</td>\n      <td>2.0</td>\n      <td>NaN</td>\n      <td>1.0</td>\n      <td>NaN</td>\n      <td>18.0</td>\n      <td>NaN</td>\n      <td>1.0</td>\n      <td>4</td>\n      <td>1.0</td>\n      <td>18.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>4.0</td>\n      <td>yeah</td>\n      <td>yeah</td>\n      <td>1.0</td>\n      <td>63</td>\n      <td>0</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>yeah's</th>\n      <td>0.587786</td>\n      <td>NaN</td>\n      <td>1.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1.0</td>\n      <td>6</td>\n      <td>1.0</td>\n      <td>18.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>4.0</td>\n      <td>yeah</td>\n      <td>yeah</td>\n      <td>NaN</td>\n      <td>63</td>\n      <td>0</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>yeahs</th>\n      <td>0.659100</td>\n      <td>NaN</td>\n      <td>1.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>5</td>\n      <td>1.0</td>\n      <td>18.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>4.0</td>\n      <td>yeah</td>\n      <td>yeah</td>\n      <td>NaN</td>\n      <td>63</td>\n      <td>0</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>year</th>\n      <td>0.396437</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>4.0</td>\n      <td>1.0</td>\n      <td>4</td>\n      <td>NaN</td>\n      <td>18.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>4.0</td>\n      <td>yeah</td>\n      <td>yeah</td>\n      <td>NaN</td>\n      <td>63</td>\n      <td>0</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>yeas</th>\n      <td>0.574666</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>3.0</td>\n      <td>1.0</td>\n      <td>4</td>\n      <td>NaN</td>\n      <td>18.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>4.0</td>\n      <td>yeah</td>\n      <td>yeah</td>\n      <td>NaN</td>\n      <td>63</td>\n      <td>0</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>yeoman</th>\n      <td>0.221668</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>11.0</td>\n      <td>1.0</td>\n      <td>6</td>\n      <td>NaN</td>\n      <td>18.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>4.0</td>\n      <td>yeah</td>\n      <td>yeah</td>\n      <td>NaN</td>\n      <td>63</td>\n      <td>0</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>yes</th>\n      <td>0.821176</td>\n      <td>3.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1.0</td>\n      <td>3</td>\n      <td>NaN</td>\n      <td>18.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>4.0</td>\n      <td>yeah</td>\n      <td>yeah</td>\n      <td>NaN</td>\n      <td>63</td>\n      <td>0</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>yeti</th>\n      <td>0.378206</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>10.0</td>\n      <td>1.0</td>\n      <td>4</td>\n      <td>NaN</td>\n      <td>18.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>4.0</td>\n      <td>yeah</td>\n      <td>yeah</td>\n      <td>NaN</td>\n      <td>63</td>\n      <td>0</td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev = pd.read_csv(os.path.join(DATA_PATH, \"hpc/train_processed_annotated.txt\"), index_col=0)\n",
    "dev.loc[(dev['process'] == 63) & (dev['tweet'] == 0) & (dev['tok'] == 2)]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/elijoe/Library/Mobile Documents/com~apple~CloudDocs/Documents/2/Diss/lexnorm/.venv/lib/python3.10/site-packages/sklearn/base.py:420: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'test'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[77], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mclf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpredict_log_proba\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtest\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Library/Mobile Documents/com~apple~CloudDocs/Documents/2/Diss/lexnorm/.venv/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:907\u001B[0m, in \u001B[0;36mForestClassifier.predict_log_proba\u001B[0;34m(self, X)\u001B[0m\n\u001B[1;32m    886\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mpredict_log_proba\u001B[39m(\u001B[38;5;28mself\u001B[39m, X):\n\u001B[1;32m    887\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    888\u001B[0m \u001B[38;5;124;03m    Predict class log-probabilities for X.\u001B[39;00m\n\u001B[1;32m    889\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    905\u001B[0m \u001B[38;5;124;03m        classes corresponds to that in the attribute :term:`classes_`.\u001B[39;00m\n\u001B[1;32m    906\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 907\u001B[0m     proba \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpredict_proba\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    909\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_outputs_ \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[1;32m    910\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m np\u001B[38;5;241m.\u001B[39mlog(proba)\n",
      "File \u001B[0;32m~/Library/Mobile Documents/com~apple~CloudDocs/Documents/2/Diss/lexnorm/.venv/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:862\u001B[0m, in \u001B[0;36mForestClassifier.predict_proba\u001B[0;34m(self, X)\u001B[0m\n\u001B[1;32m    860\u001B[0m check_is_fitted(\u001B[38;5;28mself\u001B[39m)\n\u001B[1;32m    861\u001B[0m \u001B[38;5;66;03m# Check data\u001B[39;00m\n\u001B[0;32m--> 862\u001B[0m X \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_validate_X_predict\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    864\u001B[0m \u001B[38;5;66;03m# Assign chunk of trees to jobs\u001B[39;00m\n\u001B[1;32m    865\u001B[0m n_jobs, _, _ \u001B[38;5;241m=\u001B[39m _partition_estimators(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_estimators, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_jobs)\n",
      "File \u001B[0;32m~/Library/Mobile Documents/com~apple~CloudDocs/Documents/2/Diss/lexnorm/.venv/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:602\u001B[0m, in \u001B[0;36mBaseForest._validate_X_predict\u001B[0;34m(self, X)\u001B[0m\n\u001B[1;32m    599\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    600\u001B[0m \u001B[38;5;124;03mValidate X whenever one tries to predict, apply, predict_proba.\"\"\"\u001B[39;00m\n\u001B[1;32m    601\u001B[0m check_is_fitted(\u001B[38;5;28mself\u001B[39m)\n\u001B[0;32m--> 602\u001B[0m X \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_validate_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mDTYPE\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maccept_sparse\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcsr\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreset\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[1;32m    603\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m issparse(X) \u001B[38;5;129;01mand\u001B[39;00m (X\u001B[38;5;241m.\u001B[39mindices\u001B[38;5;241m.\u001B[39mdtype \u001B[38;5;241m!=\u001B[39m np\u001B[38;5;241m.\u001B[39mintc \u001B[38;5;129;01mor\u001B[39;00m X\u001B[38;5;241m.\u001B[39mindptr\u001B[38;5;241m.\u001B[39mdtype \u001B[38;5;241m!=\u001B[39m np\u001B[38;5;241m.\u001B[39mintc):\n\u001B[1;32m    604\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNo support for np.int64 index based sparse matrices\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/Library/Mobile Documents/com~apple~CloudDocs/Documents/2/Diss/lexnorm/.venv/lib/python3.10/site-packages/sklearn/base.py:546\u001B[0m, in \u001B[0;36mBaseEstimator._validate_data\u001B[0;34m(self, X, y, reset, validate_separately, **check_params)\u001B[0m\n\u001B[1;32m    544\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mValidation should be done on X, y or both.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    545\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m no_val_X \u001B[38;5;129;01mand\u001B[39;00m no_val_y:\n\u001B[0;32m--> 546\u001B[0m     X \u001B[38;5;241m=\u001B[39m \u001B[43mcheck_array\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minput_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mX\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mcheck_params\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    547\u001B[0m     out \u001B[38;5;241m=\u001B[39m X\n\u001B[1;32m    548\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m no_val_X \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m no_val_y:\n",
      "File \u001B[0;32m~/Library/Mobile Documents/com~apple~CloudDocs/Documents/2/Diss/lexnorm/.venv/lib/python3.10/site-packages/sklearn/utils/validation.py:879\u001B[0m, in \u001B[0;36mcheck_array\u001B[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001B[0m\n\u001B[1;32m    877\u001B[0m         array \u001B[38;5;241m=\u001B[39m xp\u001B[38;5;241m.\u001B[39mastype(array, dtype, copy\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[1;32m    878\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 879\u001B[0m         array \u001B[38;5;241m=\u001B[39m \u001B[43m_asarray_with_order\u001B[49m\u001B[43m(\u001B[49m\u001B[43marray\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43morder\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43morder\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdtype\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mxp\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mxp\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    880\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m ComplexWarning \u001B[38;5;28;01mas\u001B[39;00m complex_warning:\n\u001B[1;32m    881\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    882\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mComplex data not supported\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(array)\n\u001B[1;32m    883\u001B[0m     ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mcomplex_warning\u001B[39;00m\n",
      "File \u001B[0;32m~/Library/Mobile Documents/com~apple~CloudDocs/Documents/2/Diss/lexnorm/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:185\u001B[0m, in \u001B[0;36m_asarray_with_order\u001B[0;34m(array, dtype, order, copy, xp)\u001B[0m\n\u001B[1;32m    182\u001B[0m     xp, _ \u001B[38;5;241m=\u001B[39m get_namespace(array)\n\u001B[1;32m    183\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m xp\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m \u001B[38;5;129;01min\u001B[39;00m {\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnumpy\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnumpy.array_api\u001B[39m\u001B[38;5;124m\"\u001B[39m}:\n\u001B[1;32m    184\u001B[0m     \u001B[38;5;66;03m# Use NumPy API to support order\u001B[39;00m\n\u001B[0;32m--> 185\u001B[0m     array \u001B[38;5;241m=\u001B[39m \u001B[43mnumpy\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43masarray\u001B[49m\u001B[43m(\u001B[49m\u001B[43marray\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43morder\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43morder\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdtype\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    186\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m xp\u001B[38;5;241m.\u001B[39masarray(array, copy\u001B[38;5;241m=\u001B[39mcopy)\n\u001B[1;32m    187\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "\u001B[0;31mValueError\u001B[0m: could not convert string to float: 'test'"
     ]
    }
   ],
   "source": [
    "clf.predict_log_proba(\"test\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/elijoe/Library/Mobile Documents/com~apple~CloudDocs/Documents/2/Diss/lexnorm/.venv/lib/python3.10/site-packages/gensim/models/keyedvectors.py:849: RuntimeWarning: invalid value encountered in divide\n",
      "  dists = dot(self.vectors[clip_start:clip_end], mean) / self.norms[clip_start:clip_end]\n"
     ]
    },
    {
     "data": {
      "text/plain": "     cosine_to_orig  embeddings_rank\nThe        0.757326                1",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>cosine_to_orig</th>\n      <th>embeddings_rank</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>The</th>\n      <td>0.757326</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "from lexnorm.generate_extract import modules\n",
    "from lexnorm.generate_extract import candidate_generation\n",
    "importlib.reload(modules)\n",
    "\n",
    "modules.word_embeddings(\"the\", vectors)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
