{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from lexnorm.definitions import DATA_PATH\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "train = pd.read_csv(os.path.join(DATA_PATH, \"hpc/train_annotated.txt\"), index_col=0)\n",
    "dev = pd.read_csv(os.path.join(DATA_PATH, \"hpc/dev_annotated_with_train_info.txt\"), index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "outputs": [],
   "source": [
    "train.fillna(0, inplace=True)\n",
    "dev.fillna(0, inplace=True)\n",
    "X_train = train.drop([\"correct\", \"raw_tok_index\", \"gold\"], axis=1)\n",
    "X_dev = dev.drop([\"correct\", \"process\", \"tweet\", \"tok\", \"gold\"], axis=1)\n",
    "y_train = train[\"correct\"]\n",
    "y_dev = dev[\"correct\"]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "feature_vals = X_train.values\n",
    "scaler = preprocessing.StandardScaler().fit(feature_vals)\n",
    "feature_vals = scaler.transform(feature_vals)\n",
    "X_train[:] = feature_vals\n",
    "\n",
    "feature_vals = X_dev.values\n",
    "scaler = preprocessing.StandardScaler().fit(feature_vals)\n",
    "feature_vals = scaler.transform(feature_vals)\n",
    "X_dev[:] = feature_vals"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear]....................................................................................................\n",
      "optimization finished, #iter = 1000\n",
      "\n",
      "WARNING: reaching max number of iterations\n",
      "Using -s 2 may be faster (also see FAQ)\n",
      "\n",
      "Objective value = -77126.360842\n",
      "nSV = 259470\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/elijoe/Library/Mobile Documents/com~apple~CloudDocs/Documents/2/Diss/lexnorm/.venv/lib/python3.10/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "svm = LinearSVC(class_weight=\"balanced\", verbose=1).fit(X_train, y_train.values.ravel())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[ 0.17172243, -0.13962874, -0.20333317,  0.12403685,  0.00285971,\n         1.8674295 , -0.0935477 ,  0.16077809, -0.05016316,  0.14621954,\n        -0.64900722, -0.14582517,  0.        ,  0.08600042]])"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from joblib import dump, load\n",
    "dump(svm, os.path.join(DATA_PATH, \"../models/svm.joblib\"))\n",
    "test = load(os.path.join(DATA_PATH, \"../models/svm.joblib\"))\n",
    "test.coef_"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "outputs": [],
   "source": [
    "preds = svm.decision_function(X_dev)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "outputs": [
    {
     "data": {
      "text/plain": "0.9983700596728575"
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm.score(X_dev, y_dev)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "outputs": [
    {
     "data": {
      "text/plain": "          cosine_to_orig  embeddings_rank  from_clipping  from_original_token  \\\nnah             2.656120         3.877197      -2.593884            -0.094405   \noh              2.745444         2.535195      -2.593884            -0.094405   \nuh              2.624856         5.219199      -2.593884            -0.094405   \nye ah          -1.138580        -0.148809      -2.593884            -0.094405   \nye-ah          -1.138580        -0.148809      -2.593884            -0.094405   \n...                  ...              ...            ...                  ...   \nupward          0.569510        -0.148809       0.385522            -0.094405   \nupwardly        0.702616        -0.148809       0.385522            -0.094405   \nupwards         0.833669        -0.148809       0.385522            -0.094405   \nus              1.122110        -0.148809      -2.593884            -0.094405   \nyup            -0.101099        -0.148809      -2.593884            -0.094405   \n\n          from_split  norms_seen  spellcheck_rank  in_lexicon    length  \\\nnah        -0.046030   -0.048241        -0.268871   -4.348835 -1.763720   \noh         -0.046030   -0.048241        -0.268871    0.229947 -2.095137   \nuh         -0.046030   -0.048241        -0.268871    0.229947 -2.095137   \nye ah      21.724887   -0.048241         1.360023   -4.348835 -1.100884   \nye-ah      -0.046030   -0.048241         1.767247   -4.348835 -1.100884   \n...              ...         ...              ...         ...       ...   \nupward     -0.046030   -0.048241        -0.268871    0.229947 -0.769467   \nupwardly   -0.046030   -0.048241        -0.268871    0.229947 -0.106631   \nupwards    -0.046030   -0.048241        -0.268871    0.229947 -0.438049   \nus         -0.046030   -0.048241         2.988917    0.229947 -2.095137   \nyup        -0.046030   -0.048241         2.581694    0.229947 -1.763720   \n\n          same_order  orig_norms_seen  orig_in_lexicon  orig_same_order  \\\nnah        -3.155060        -1.057043         0.345895              0.0   \noh         -3.155060        -1.057043         0.345895              0.0   \nuh         -3.155060        -1.057043         0.345895              0.0   \nye ah       0.316951        -1.057043         0.345895              0.0   \nye-ah       0.316951        -1.057043         0.345895              0.0   \n...              ...              ...              ...              ...   \nupward      0.316951        -0.688526         0.345895              0.0   \nupwardly    0.316951        -0.688526         0.345895              0.0   \nupwards     0.316951        -0.688526         0.345895              0.0   \nus         -3.155060        -0.688526         0.345895              0.0   \nyup         0.316951        -0.688526         0.345895              0.0   \n\n          orig_length  \nnah          1.361154  \noh           1.361154  \nuh           1.361154  \nye ah        1.361154  \nye-ah        1.361154  \n...               ...  \nupward      -0.462068  \nupwardly    -0.462068  \nupwards     -0.462068  \nus          -0.462068  \nyup         -0.462068  \n\n[2988229 rows x 14 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>cosine_to_orig</th>\n      <th>embeddings_rank</th>\n      <th>from_clipping</th>\n      <th>from_original_token</th>\n      <th>from_split</th>\n      <th>norms_seen</th>\n      <th>spellcheck_rank</th>\n      <th>in_lexicon</th>\n      <th>length</th>\n      <th>same_order</th>\n      <th>orig_norms_seen</th>\n      <th>orig_in_lexicon</th>\n      <th>orig_same_order</th>\n      <th>orig_length</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>nah</th>\n      <td>2.656120</td>\n      <td>3.877197</td>\n      <td>-2.593884</td>\n      <td>-0.094405</td>\n      <td>-0.046030</td>\n      <td>-0.048241</td>\n      <td>-0.268871</td>\n      <td>-4.348835</td>\n      <td>-1.763720</td>\n      <td>-3.155060</td>\n      <td>-1.057043</td>\n      <td>0.345895</td>\n      <td>0.0</td>\n      <td>1.361154</td>\n    </tr>\n    <tr>\n      <th>oh</th>\n      <td>2.745444</td>\n      <td>2.535195</td>\n      <td>-2.593884</td>\n      <td>-0.094405</td>\n      <td>-0.046030</td>\n      <td>-0.048241</td>\n      <td>-0.268871</td>\n      <td>0.229947</td>\n      <td>-2.095137</td>\n      <td>-3.155060</td>\n      <td>-1.057043</td>\n      <td>0.345895</td>\n      <td>0.0</td>\n      <td>1.361154</td>\n    </tr>\n    <tr>\n      <th>uh</th>\n      <td>2.624856</td>\n      <td>5.219199</td>\n      <td>-2.593884</td>\n      <td>-0.094405</td>\n      <td>-0.046030</td>\n      <td>-0.048241</td>\n      <td>-0.268871</td>\n      <td>0.229947</td>\n      <td>-2.095137</td>\n      <td>-3.155060</td>\n      <td>-1.057043</td>\n      <td>0.345895</td>\n      <td>0.0</td>\n      <td>1.361154</td>\n    </tr>\n    <tr>\n      <th>ye ah</th>\n      <td>-1.138580</td>\n      <td>-0.148809</td>\n      <td>-2.593884</td>\n      <td>-0.094405</td>\n      <td>21.724887</td>\n      <td>-0.048241</td>\n      <td>1.360023</td>\n      <td>-4.348835</td>\n      <td>-1.100884</td>\n      <td>0.316951</td>\n      <td>-1.057043</td>\n      <td>0.345895</td>\n      <td>0.0</td>\n      <td>1.361154</td>\n    </tr>\n    <tr>\n      <th>ye-ah</th>\n      <td>-1.138580</td>\n      <td>-0.148809</td>\n      <td>-2.593884</td>\n      <td>-0.094405</td>\n      <td>-0.046030</td>\n      <td>-0.048241</td>\n      <td>1.767247</td>\n      <td>-4.348835</td>\n      <td>-1.100884</td>\n      <td>0.316951</td>\n      <td>-1.057043</td>\n      <td>0.345895</td>\n      <td>0.0</td>\n      <td>1.361154</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>upward</th>\n      <td>0.569510</td>\n      <td>-0.148809</td>\n      <td>0.385522</td>\n      <td>-0.094405</td>\n      <td>-0.046030</td>\n      <td>-0.048241</td>\n      <td>-0.268871</td>\n      <td>0.229947</td>\n      <td>-0.769467</td>\n      <td>0.316951</td>\n      <td>-0.688526</td>\n      <td>0.345895</td>\n      <td>0.0</td>\n      <td>-0.462068</td>\n    </tr>\n    <tr>\n      <th>upwardly</th>\n      <td>0.702616</td>\n      <td>-0.148809</td>\n      <td>0.385522</td>\n      <td>-0.094405</td>\n      <td>-0.046030</td>\n      <td>-0.048241</td>\n      <td>-0.268871</td>\n      <td>0.229947</td>\n      <td>-0.106631</td>\n      <td>0.316951</td>\n      <td>-0.688526</td>\n      <td>0.345895</td>\n      <td>0.0</td>\n      <td>-0.462068</td>\n    </tr>\n    <tr>\n      <th>upwards</th>\n      <td>0.833669</td>\n      <td>-0.148809</td>\n      <td>0.385522</td>\n      <td>-0.094405</td>\n      <td>-0.046030</td>\n      <td>-0.048241</td>\n      <td>-0.268871</td>\n      <td>0.229947</td>\n      <td>-0.438049</td>\n      <td>0.316951</td>\n      <td>-0.688526</td>\n      <td>0.345895</td>\n      <td>0.0</td>\n      <td>-0.462068</td>\n    </tr>\n    <tr>\n      <th>us</th>\n      <td>1.122110</td>\n      <td>-0.148809</td>\n      <td>-2.593884</td>\n      <td>-0.094405</td>\n      <td>-0.046030</td>\n      <td>-0.048241</td>\n      <td>2.988917</td>\n      <td>0.229947</td>\n      <td>-2.095137</td>\n      <td>-3.155060</td>\n      <td>-0.688526</td>\n      <td>0.345895</td>\n      <td>0.0</td>\n      <td>-0.462068</td>\n    </tr>\n    <tr>\n      <th>yup</th>\n      <td>-0.101099</td>\n      <td>-0.148809</td>\n      <td>-2.593884</td>\n      <td>-0.094405</td>\n      <td>-0.046030</td>\n      <td>-0.048241</td>\n      <td>2.581694</td>\n      <td>0.229947</td>\n      <td>-1.763720</td>\n      <td>0.316951</td>\n      <td>-0.688526</td>\n      <td>0.345895</td>\n      <td>0.0</td>\n      <td>-0.462068</td>\n    </tr>\n  </tbody>\n</table>\n<p>2988229 rows × 14 columns</p>\n</div>"
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "outputs": [
    {
     "data": {
      "text/plain": "      cosine_to_orig  embeddings_rank  from_clipping  from_original_token  \\\nan          0.377180              0.0            0.0                  0.0   \nca          0.255824              0.0            0.0                  0.0   \ncab         0.261057              0.0            0.0                  0.0   \ncam         0.299136              0.0            0.0                  0.0   \ncan         1.000000              0.0            1.0                  1.0   \n...              ...              ...            ...                  ...   \nlulu        0.401415              0.0            0.0                  0.0   \npol         0.230619              0.0            0.0                  0.0   \nsmh         0.896277              1.0            0.0                  0.0   \nsol         0.285978              0.0            0.0                  0.0   \nvol         0.235724              0.0            0.0                  0.0   \n\n      from_split  norms_seen  spellcheck_rank  in_lexicon  length  same_order  \\\nan           0.0         0.0              3.0         1.0       2         0.0   \nca           0.0         0.0              4.0         1.0       2         0.0   \ncab          0.0         0.0              1.0         1.0       3         0.0   \ncam          0.0         0.0              2.0         1.0       3         0.0   \ncan          0.0        72.0              0.0         1.0       3         1.0   \n...          ...         ...              ...         ...     ...         ...   \nlulu         0.0         0.0             14.0         1.0       4         0.0   \npol          0.0         0.0              9.0         1.0       3         0.0   \nsmh          0.0         0.0              0.0         0.0       3         0.0   \nsol          0.0         0.0              3.0         1.0       3         0.0   \nvol          0.0         0.0             12.0         0.0       3         0.0   \n\n      orig_norms_seen  orig_in_lexicon  orig_same_order  orig_length  correct  \\\nan               72.0              1.0              1.0          3.0      0.0   \nca               72.0              1.0              1.0          3.0      0.0   \ncab              72.0              1.0              1.0          3.0      0.0   \ncam              72.0              1.0              1.0          3.0      0.0   \ncan              72.0              1.0              1.0          3.0      1.0   \n...               ...              ...              ...          ...      ...   \nlulu            205.0              1.0              1.0          3.0      0.0   \npol             205.0              1.0              1.0          3.0      0.0   \nsmh             205.0              1.0              1.0          3.0      0.0   \nsol             205.0              1.0              1.0          3.0      0.0   \nvol             205.0              1.0              1.0          3.0      0.0   \n\n      process  tweet  tok gold     preds  \nan         35      0    2  can -1.434081  \nca         35      0    2  can -1.568795  \ncab        35      0    2  can -1.469996  \ncam        35      0    2  can -1.476411  \ncan        35      0    2  can  9.580333  \n...       ...    ...  ...  ...       ...  \nlulu       32      9  195  lol -2.435480  \npol        32      9  195  lol -2.371043  \nsmh        32      9  195  lol -2.399906  \nsol        32      9  195  lol -2.104146  \nvol        32      9  195  lol -3.194602  \n\n[727634 rows x 20 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>cosine_to_orig</th>\n      <th>embeddings_rank</th>\n      <th>from_clipping</th>\n      <th>from_original_token</th>\n      <th>from_split</th>\n      <th>norms_seen</th>\n      <th>spellcheck_rank</th>\n      <th>in_lexicon</th>\n      <th>length</th>\n      <th>same_order</th>\n      <th>orig_norms_seen</th>\n      <th>orig_in_lexicon</th>\n      <th>orig_same_order</th>\n      <th>orig_length</th>\n      <th>correct</th>\n      <th>process</th>\n      <th>tweet</th>\n      <th>tok</th>\n      <th>gold</th>\n      <th>preds</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>an</th>\n      <td>0.377180</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>3.0</td>\n      <td>1.0</td>\n      <td>2</td>\n      <td>0.0</td>\n      <td>72.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>3.0</td>\n      <td>0.0</td>\n      <td>35</td>\n      <td>0</td>\n      <td>2</td>\n      <td>can</td>\n      <td>-1.434081</td>\n    </tr>\n    <tr>\n      <th>ca</th>\n      <td>0.255824</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>4.0</td>\n      <td>1.0</td>\n      <td>2</td>\n      <td>0.0</td>\n      <td>72.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>3.0</td>\n      <td>0.0</td>\n      <td>35</td>\n      <td>0</td>\n      <td>2</td>\n      <td>can</td>\n      <td>-1.568795</td>\n    </tr>\n    <tr>\n      <th>cab</th>\n      <td>0.261057</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>3</td>\n      <td>0.0</td>\n      <td>72.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>3.0</td>\n      <td>0.0</td>\n      <td>35</td>\n      <td>0</td>\n      <td>2</td>\n      <td>can</td>\n      <td>-1.469996</td>\n    </tr>\n    <tr>\n      <th>cam</th>\n      <td>0.299136</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>3</td>\n      <td>0.0</td>\n      <td>72.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>3.0</td>\n      <td>0.0</td>\n      <td>35</td>\n      <td>0</td>\n      <td>2</td>\n      <td>can</td>\n      <td>-1.476411</td>\n    </tr>\n    <tr>\n      <th>can</th>\n      <td>1.000000</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>72.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>3</td>\n      <td>1.0</td>\n      <td>72.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>3.0</td>\n      <td>1.0</td>\n      <td>35</td>\n      <td>0</td>\n      <td>2</td>\n      <td>can</td>\n      <td>9.580333</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>lulu</th>\n      <td>0.401415</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>14.0</td>\n      <td>1.0</td>\n      <td>4</td>\n      <td>0.0</td>\n      <td>205.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>3.0</td>\n      <td>0.0</td>\n      <td>32</td>\n      <td>9</td>\n      <td>195</td>\n      <td>lol</td>\n      <td>-2.435480</td>\n    </tr>\n    <tr>\n      <th>pol</th>\n      <td>0.230619</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>9.0</td>\n      <td>1.0</td>\n      <td>3</td>\n      <td>0.0</td>\n      <td>205.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>3.0</td>\n      <td>0.0</td>\n      <td>32</td>\n      <td>9</td>\n      <td>195</td>\n      <td>lol</td>\n      <td>-2.371043</td>\n    </tr>\n    <tr>\n      <th>smh</th>\n      <td>0.896277</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>3</td>\n      <td>0.0</td>\n      <td>205.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>3.0</td>\n      <td>0.0</td>\n      <td>32</td>\n      <td>9</td>\n      <td>195</td>\n      <td>lol</td>\n      <td>-2.399906</td>\n    </tr>\n    <tr>\n      <th>sol</th>\n      <td>0.285978</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>3.0</td>\n      <td>1.0</td>\n      <td>3</td>\n      <td>0.0</td>\n      <td>205.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>3.0</td>\n      <td>0.0</td>\n      <td>32</td>\n      <td>9</td>\n      <td>195</td>\n      <td>lol</td>\n      <td>-2.104146</td>\n    </tr>\n    <tr>\n      <th>vol</th>\n      <td>0.235724</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>12.0</td>\n      <td>0.0</td>\n      <td>3</td>\n      <td>0.0</td>\n      <td>205.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>3.0</td>\n      <td>0.0</td>\n      <td>32</td>\n      <td>9</td>\n      <td>195</td>\n      <td>lol</td>\n      <td>-3.194602</td>\n    </tr>\n  </tbody>\n</table>\n<p>727634 rows × 20 columns</p>\n</div>"
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev[\"preds\"] = preds\n",
    "dev"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "outputs": [
    {
     "data": {
      "text/plain": "['brother',\n 'get',\n 'out',\n 'your',\n 'feelings',\n 'lol',\n 'manan',\n 'dund',\n 'xaragdax',\n 'ter',\n 'uuliin',\n 'oroid',\n 'minii',\n 'aav',\n 'xezee',\n 'neg',\n 'cagt',\n 'zogsoj',\n 'baisan',\n 'photo',\n 'by',\n 'why',\n 'these',\n 'niggers',\n 'think',\n 'they',\n 'doing',\n 'something',\n \"it's\",\n 'about',\n 'more',\n 'than',\n 'number',\n 'brother',\n 'and',\n \"i'm\",\n 'not',\n 'talking',\n 'about',\n 'statistics',\n \"i'm\",\n 'talking',\n 'about',\n 'skill',\n 'level',\n 'yes',\n 'omg',\n 'i',\n \"don't\",\n 'want',\n 'him',\n 'feeling',\n 'unappreciated',\n 'or',\n 'stuff',\n 'like',\n 'that',\n 'lmao',\n 'the',\n 'whole',\n 'time',\n 'actually',\n 'we',\n 'need',\n 'to',\n 'start',\n 'our',\n 'own',\n 'team',\n \"y'all\",\n 'were',\n 'wilding',\n 'last',\n 'night',\n 'yoh',\n 'niya',\n 'ja',\n 'saying',\n 'kau',\n 'aku',\n 'with',\n 'couple',\n 'is',\n 'actually',\n 'sweet',\n 'k',\n 'michelle',\n 'met',\n 'you',\n 'match',\n 'that',\n 'fonk',\n 'rana',\n 'samaha',\n 'ignore',\n 'all',\n 'the',\n 'criticism',\n 'a',\n 'spotlight',\n 'can',\n 'never',\n 'see',\n 'the',\n 'shadows',\n 'anyways',\n \"you're\",\n 'flawless',\n 'boko',\n 'haram',\n 'shettima',\n 'warns',\n 'nigerian',\n 'leaders',\n 'over',\n 'politicization',\n 'of',\n 'chibok',\n 'abduction',\n 'and',\n 'in',\n 'that',\n 'moment',\n 'karma',\n 'realized',\n 'she',\n 'was',\n 'hell of a lot of',\n 'gay',\n 'for',\n 'her',\n 'best friend',\n 'not',\n 'you',\n 'because',\n 'you',\n 'chilling',\n 'with',\n 'me',\n 'kardashia',\n 'kardashian',\n 'died',\n 'because',\n 'her',\n 'ass',\n 'was',\n 'too',\n 'big',\n 'hahah',\n 'nigger',\n 'summation',\n 'classified',\n 'directory',\n 'high',\n 'grille',\n 'for',\n 'the',\n 'online',\n 'game',\n 'atd',\n 'lexx',\n 'little',\n 'cute',\n 'ass',\n 'woke',\n 'up',\n 'mad',\n 'as',\n 'hell',\n 'lol',\n 'green',\n 'day',\n 'paramore',\n 'yellow card',\n 'fall',\n 'out',\n 'boy',\n 'is',\n 'that',\n 'minho',\n 'beside',\n 'taemin',\n 'omo',\n 'looks',\n 'like',\n 'him',\n '140529',\n 'smtown',\n 'artists',\n 'with',\n 'exo',\n 'photoset',\n 'flintandpyrite',\n 'ume',\n 'by',\n 'andrea',\n 'rangel',\n 'knits',\n 'for',\n 'spring',\n 'because',\n \"that's\",\n 'going to',\n 'happen',\n 'lol',\n 'need',\n 'to',\n 'revisit',\n 'their',\n 'thameslink',\n 'bid',\n 'the',\n 'moorgate',\n 'route',\n 'closed',\n '4',\n 'years',\n 'ago',\n 'icons',\n 'da',\n 'demi',\n 'sem',\n 'psd',\n 'no',\n 'the',\n 'x',\n 'factor',\n 'x8',\n 'you',\n \"don't\",\n \"i'm\",\n 'the',\n 'king',\n 'of',\n 'sweg',\n 'haha',\n \"that's\",\n 'funny',\n 'but',\n 'not',\n 'every',\n 'girl',\n 'is',\n 'darling',\n 'imran',\n 'khan',\n 'saying',\n 'really',\n 'true',\n 'about',\n 'pmln',\n 'and',\n \"you'll\",\n 'always',\n 'be',\n 'my',\n '5',\n 'heros',\n 'who',\n 'can',\n 'always',\n 'make',\n 'me',\n 'smile',\n 'even',\n 'when',\n 'i',\n 'm',\n 'in',\n 'the',\n 'saddest',\n 'mood',\n 'ever',\n 'xxc',\n 'lol',\n 'what',\n 'he',\n 'tripping',\n 'on',\n 'xabi',\n 'alonso',\n 'just',\n 'achieved',\n 'stratospheric',\n 'heights',\n 'of',\n 'coolness',\n 'lmaoo',\n 'my',\n 'fault',\n 'you',\n \"i'm going to\",\n 'let',\n 'you',\n 'get',\n 'your',\n 'shine',\n 'james',\n 'and',\n 'i',\n 'say',\n 'hi',\n 'with',\n 'a',\n 'dog',\n 'tenerezza',\n 'omg',\n 'hawt',\n 'like',\n 'omg',\n 'can',\n 'my',\n 'husband',\n 'please',\n 'look',\n 'like',\n 'you',\n 'or',\n 'can',\n 'i',\n 'have',\n 'your',\n 'babies',\n 'i',\n 'miss',\n 'going',\n 'to',\n 'mansion',\n 'elan',\n 'ery',\n 'weekend',\n \"i'm\",\n 'going to',\n 'start',\n 'back',\n 'going',\n 'doe',\n 'facts',\n 'brother',\n 'this',\n 'baby face',\n \"ain't\",\n 'helping',\n 'nothing',\n 'lmao',\n 'can',\n 'i',\n 'have',\n 'him',\n 'yet',\n 'or',\n 'no',\n 'ughh',\n '140524',\n 'hyoyeon',\n 'yuri',\n '2nd',\n 'day',\n 'kobe',\n 'by',\n 'rebecca',\n 'crowdfunding',\n 'roundup',\n 'a',\n 'late',\n 'spring',\n 'flowering',\n 'of',\n 'projects',\n 'every',\n 'week',\n 'tuaw',\n 'provides',\n 'readers',\n 'with',\n 'an',\n 'update',\n 'o',\n 'mayweather',\n 'needs',\n 'to',\n 'fight',\n 'paquiao',\n 'that',\n 'would',\n 'be',\n 'amazing',\n \"i'm\",\n 'actually',\n 'excited',\n 'to',\n 'see',\n 'what',\n 'it',\n 'will',\n 'look',\n 'like',\n 'so',\n 'its',\n 'happening',\n 'when',\n 'the',\n 'lawn',\n 'area',\n 'get',\n 'done',\n 'lol',\n 'who',\n 'about to',\n 'come',\n 'way',\n 'out',\n 'there',\n 'come',\n 'get',\n 'me',\n 'bring',\n 'me',\n 'back',\n \"we'll\",\n 'come',\n 'lol',\n 'i',\n 'forgot',\n 'you',\n 'moved',\n 'canal',\n 'then',\n 'lol',\n 'apple',\n 'forgets',\n 'to',\n 'renew',\n 'ssl',\n 'certificate',\n 'breaking',\n 'os',\n 'x',\n 'software',\n 'update',\n 'i know',\n \"i'd\",\n 'rather',\n 'someone',\n 'unfollow',\n 'me',\n 'than',\n 'mute',\n 'me',\n 'like',\n 'what',\n 'ohhh',\n 'my',\n 'poor',\n 'zac',\n 'take',\n 'good',\n 'care',\n 'of',\n 'yourself',\n 'kimberley',\n 'walsh',\n 'confirms',\n 'cheryl',\n \"cole's\",\n \"britain's\",\n 'got',\n 'talent',\n 'appearance',\n 'contactmusic',\n 'comkimberley',\n 'walsh',\n 'confir',\n 'exo',\n 'is',\n 'practising',\n 'for',\n 'concert',\n 'lol',\n 'you',\n 'still',\n 'in',\n 'school',\n 'shut up',\n 'he',\n 'wants',\n 'to',\n 'get',\n 'bigger',\n 'than',\n 'parth',\n 'lmfao',\n 'chanyeol',\n 'is',\n 'so',\n 'caring',\n 'omg',\n 'be',\n 'mine',\n 'please',\n 'nothing',\n 'just',\n 'chilling',\n 'imy',\n 'you',\n \"don't\",\n 'fuck',\n 'with',\n 'a',\n 'thug',\n 'no more',\n 'lol',\n 'udah',\n 'lama',\n 'nda',\n 'how',\n 'about',\n 'it',\n 'how',\n 'about',\n 'that',\n 'lol',\n 'i know',\n 'i',\n 'just',\n 'took',\n 'them',\n 'real',\n 'quick',\n 'i',\n \"wasn't\",\n 'thinking',\n 'the',\n 'order',\n '1',\n 'daily',\n 'follower',\n '0',\n 'unfollowers',\n 'just unfollow',\n \"doesn't\",\n 'miss',\n 'a',\n 'trick',\n \"i'm\",\n 'going to',\n 'do',\n 'it',\n 'after',\n 'graduation',\n 'lol',\n 'second',\n 'time',\n 'this',\n 'shit',\n 'crashed',\n 'on',\n 'the piff',\n 'so',\n 'we',\n 'on',\n 'sound cloud',\n 'with',\n 'it',\n 'genji',\n 'sports',\n 'pop',\n 'up',\n 'family',\n 'beach',\n 'tent',\n 'and',\n 'beach',\n 'sunshelter',\n 'genji',\n 'sports',\n 'pop',\n 'up',\n 'fami',\n 'titan',\n 'baseball',\n 'beats',\n 'brv',\n 'to',\n 'advance',\n 'to',\n 'county',\n 'championship',\n 'tri',\n 'tomorrow',\n '4',\n 'pm',\n 'jumpseat',\n 'radio',\n '018',\n 'combat',\n 'ready',\n 'firefighting',\n 'with',\n 'tony',\n 'kelleher',\n 'repost',\n 'i',\n 'added',\n 'a',\n 'video',\n 'to',\n 'a',\n 'playlist',\n 'plg',\n 'sensual',\n 'toribash',\n 'ep',\n '3',\n \"let's\",\n 'tango',\n 'wildfires',\n 'turkey',\n 'feather',\n 'fletching',\n 'for',\n 'archery',\n 'arrows',\n 'do it yourself',\n 'pheasant',\n 'feathers',\n 'for',\n 'arro',\n 'where',\n 'the',\n 'mj',\n 'though',\n 'lol',\n '492',\n 'luke',\n 'hemmings',\n 'from',\n '5sos',\n 'i',\n 'love',\n 'you',\n 'so',\n 'much',\n 'please',\n 'follow',\n 'me',\n 'you',\n 'are',\n 'everything',\n 'to',\n 'me',\n 'please',\n 'dude',\n 'you',\n 'never',\n 'gave',\n 'me',\n 'the',\n 'kik',\n 'or',\n 'snapchat',\n \"you'll\",\n 'sea',\n 'lmfao',\n 'okay',\n 'bean',\n 'eating',\n 'head ass',\n 'morning',\n 'exercise',\n 'paakyat',\n 'ng',\n 'thunderbird',\n 'i',\n 'love',\n 'you',\n 'phil',\n 'of',\n 'the',\n 'future',\n 'i',\n 'love',\n 'everything',\n 'about',\n 'you',\n 'nao',\n 'superei',\n 'ainda',\n 'this',\n 'is',\n 'for',\n 'niall',\n 'e',\n 'vcs',\n 'wara',\n 'man',\n 'talaga',\n 'forever',\n 'niyata',\n 'ano',\n 'una',\n 'mo',\n 'imortal',\n 'ka',\n 'hahahahahahhahaha',\n 'hot',\n 'music',\n 'mariah',\n 'carey',\n 'meteorite',\n 'q',\n 'meteorite',\n 'version',\n 'mariah',\n 'carey',\n 'has',\n 'ohh',\n 'i',\n 'feel',\n 'you',\n 'haha',\n 'so',\n 'is',\n 'writing',\n 'a',\n 'book',\n 'about',\n 'jay',\n 'z',\n 'and',\n 'beyonce',\n 'telling',\n 'someone',\n \"else's\",\n 'secrets',\n 'for',\n 'money',\n 'shows',\n 'how',\n 'low',\n 'some',\n 'people',\n 'will',\n 'stoop',\n 'fast',\n 'forward',\n 'to',\n 'june',\n '9',\n 'please',\n 'para',\n 'makapag',\n 'ipon',\n 'na',\n 'hahahahahah',\n 'lol',\n 'thank',\n 'you',\n 'liam',\n 'for',\n 'asking',\n 'louis',\n 'what',\n 'flavor',\n 'harry',\n 'is',\n \"i'm\",\n 'so',\n 'jskcosnxoancoamxis',\n 'ahhhhh',\n 'salt',\n 'and',\n 'vinegar',\n \"don't\",\n 'take',\n 'it',\n 'personal',\n 'kid',\n 'caroline',\n 'wozniacki',\n 'breaks',\n 'silence',\n 'on',\n 'rory',\n 'mcilroy',\n 'breakup',\n '140525',\n 'boakwon',\n 'instagram',\n 'update',\n 'with',\n 'exo',\n 'and',\n 'suju',\n 'siwon',\n 'donghae',\n 'more',\n 'n',\n 'forehead',\n 'ft',\n 'hyuk',\n 'thanks',\n 'for',\n 'that',\n 'spiderman',\n 'carnage',\n 'mask',\n '300rb',\n 'lom',\n 'ama',\n 'ongkir',\n 'ready',\n 'stock',\n 'running',\n 'with',\n 'who',\n 'do',\n 'you',\n 'think',\n 'is',\n 'faster',\n 'nicki',\n 'minaj',\n 'slick',\n 'went',\n 'from',\n 'brown skin',\n 'to',\n 'light skin',\n 'over',\n 'the',\n 'years',\n 'if',\n 'you',\n \"don't\",\n 'fuck',\n 'with',\n 'dunny',\n 'gage',\n 'then',\n 'fuck',\n 'your',\n 'life',\n 'last',\n 'day',\n 'of',\n 'school',\n 'gonna',\n 'have',\n 'people',\n 'like',\n 'the',\n 'bad',\n 'pastor',\n 'impostors',\n 'so',\n 'loves',\n 'a',\n 'lot',\n 'to',\n 'jesus',\n 'the',\n 'people',\n 'can',\n 'intercesion',\n 'for',\n 'he',\n 'for',\n 'the',\n 'soul',\n 'people',\n 'wilshire',\n 'itq',\n 'hehe',\n 'stay',\n 'strong',\n '1',\n 'k',\n 'swim',\n 'sauna',\n 'post',\n 'work',\n 'now',\n 'beer',\n 'work',\n 'tomorrow',\n 'terserah',\n 'suho',\n 'gave',\n 'a',\n 'teddy',\n 'bear',\n 'and',\n 'took',\n 'a',\n 'selca',\n 'with',\n 'a',\n 'fan',\n 'tazz',\n 'needa',\n 'quit',\n 'it',\n 'with',\n 'that',\n 'last',\n 'quote',\n 'lol',\n 'she',\n \"don't\",\n 'live',\n 'that',\n 'life',\n 'why',\n 'the',\n 'fuck',\n 'did',\n 'i',\n 'think',\n 'i',\n 'was',\n 'scheduled',\n 'today',\n 'am',\n 'i',\n 'on',\n 'crack',\n 'lmao',\n 'what',\n 'my',\n 'top',\n '3',\n 'artists',\n 'girls',\n 'aloud',\n '15',\n 'christina',\n 'aguilera',\n '13',\n 'kylie',\n 'minogue',\n '12',\n 'how',\n 'he',\n 'top',\n '5',\n 'scorer',\n 'and',\n 'the',\n 'offense',\n \"ain't\",\n 'around',\n 'at least',\n 'him',\n 'lol',\n 'that',\n \"don't\",\n 'make',\n 'sense',\n 'that',\n 'mean',\n 'he',\n 'jacking',\n 'then',\n 'lol',\n 'kompany',\n 'going',\n 'up',\n 'to',\n 'yaya',\n 'toure',\n 'after',\n 'he',\n 'found',\n 'out',\n 'he',\n 'wanted',\n 'to',\n 'leave',\n 'first',\n 'class',\n 'thing',\n 'see',\n 'wizkid',\n 'and',\n 'banky',\n 'w',\n 'having',\n 'fun',\n 'on',\n 'via',\n 'the',\n 'old',\n 'bargin',\n 'giant',\n 'would',\n 'make',\n 'a',\n 'good',\n 'entertainment',\n ...]"
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get top prediction for each raw token\n",
    "predictions = dev.sort_values(\"preds\", ascending=False).drop_duplicates([\"process\", \"tweet\", \"tok\"])\n",
    "pred_tokens = predictions.sort_values([\"process\", \"tweet\", \"tok\"])[\"gold\"].tolist()\n",
    "pred_tokens"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "outputs": [
    {
     "data": {
      "text/plain": "correct\n0.0     407\n1.0    6469\ndtype: int64"
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pretty good job!\n",
    "predictions.groupby(\"correct\").size()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "outputs": [],
   "source": [
    "from lexnorm.data.normEval import loadNormData\n",
    "\n",
    "raw, norm = loadNormData(os.path.join(DATA_PATH, \"raw/dev.norm\"))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "outputs": [],
   "source": [
    "from lexnorm.models.filtering import is_eligible\n",
    "\n",
    "pred_tokens_iter = iter(pred_tokens)\n",
    "\n",
    "pred_tweets = []\n",
    "\n",
    "for tweet in raw:\n",
    "    pred_tweet = []\n",
    "    for tok in tweet:\n",
    "        if is_eligible(tok):\n",
    "            pred_tweet.append(next(pred_tokens_iter))\n",
    "        else:\n",
    "            pred_tweet.append(tok)\n",
    "    pred_tweets.append(pred_tweet)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline acc.(LAI): 93.10\n",
      "Accuracy:           99.97\n",
      "ERR:                99.53\n"
     ]
    },
    {
     "data": {
      "text/plain": "(0.9309630275929763, 0.9996728105573127, 0.9952606635071095)"
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from lexnorm.data.normEval import evaluate\n",
    "\n",
    "evaluate(raw, norm, pred_tweets)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
