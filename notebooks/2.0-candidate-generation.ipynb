{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from lexnorm.data import lexicon\n",
    "from lexnorm.data import norm_dict\n",
    "from lexnorm.data.normEval import loadNormData\n",
    "from lexnorm.definitions import DATA_PATH\n",
    "from lexnorm.models.normalise import load_candidates\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "raw, norm = loadNormData(os.path.join(DATA_PATH, \"processed/combined.txt\"))\n",
    "norm_dictionary = norm_dict.construct(raw, norm)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "data": {
      "text/plain": "              cosine_to_orig  frac_norms_seen  from_clipping  from_embeddings  \\\nthessaloníki             NaN              NaN            1.0              NaN   \nthessaloníki             NaN              NaN            1.0              NaN   \nthessaloníki             NaN              NaN            1.0              NaN   \nthessaloníki             NaN              NaN            1.0              NaN   \nthessaloníki             NaN              NaN            1.0              NaN   \n...                      ...              ...            ...              ...   \nthessaloníki             NaN              NaN            1.0              NaN   \nthessaloníki             NaN              NaN            1.0              NaN   \nthessaloníki             NaN              NaN            1.0              NaN   \nthessaloníki             NaN              NaN            1.0              NaN   \nthessaloníki             NaN              NaN            1.0              NaN   \n\n              from_original_token  from_spellcheck  from_split  norms_seen  \\\nthessaloníki                  NaN              NaN         NaN         NaN   \nthessaloníki                  NaN              NaN         NaN         NaN   \nthessaloníki                  NaN              NaN         NaN         NaN   \nthessaloníki                  NaN              NaN         NaN         NaN   \nthessaloníki                  NaN              NaN         NaN         NaN   \n...                           ...              ...         ...         ...   \nthessaloníki                  NaN              NaN         NaN         NaN   \nthessaloníki                  NaN              NaN         NaN         NaN   \nthessaloníki                  NaN              NaN         NaN         NaN   \nthessaloníki                  NaN              NaN         NaN         NaN   \nthessaloníki                  NaN              NaN         NaN         NaN   \n\n              spellcheck_score  length  ...  twitter_bi_next      wiki_uni  \\\nthessaloníki               -24      12  ...              0.0  5.767776e-09   \nthessaloníki               -24      12  ...              0.0  5.767776e-09   \nthessaloníki               -24      12  ...              0.0  5.767776e-09   \nthessaloníki               -24      12  ...              0.0  5.767776e-09   \nthessaloníki               -24      12  ...              0.0  5.767776e-09   \n...                        ...     ...  ...              ...           ...   \nthessaloníki               -24      12  ...              0.0  5.767776e-09   \nthessaloníki               -24      12  ...              0.0  5.767776e-09   \nthessaloníki               -24      12  ...              0.0  5.767776e-09   \nthessaloníki               -24      12  ...              0.0  5.767776e-09   \nthessaloníki               -24      12  ...              0.0  5.767776e-09   \n\n              wiki_bi_prev  wiki_bi_next  orig_twitter_uni  \\\nthessaloníki           0.0           0.0          0.018497   \nthessaloníki           0.0           0.0          0.018497   \nthessaloníki           0.0           0.0          0.018497   \nthessaloníki           0.0           0.0          0.018497   \nthessaloníki           0.0           0.0          0.018497   \n...                    ...           ...               ...   \nthessaloníki           0.0           0.0          0.018497   \nthessaloníki           0.0           0.0          0.018497   \nthessaloníki           0.0           0.0          0.018497   \nthessaloníki           0.0           0.0          0.018497   \nthessaloníki           0.0           0.0          0.018497   \n\n              orig_twitter_bi_prev orig_twitter_bi_next orig_wiki_uni  \\\nthessaloníki              0.045914             0.256350      0.070067   \nthessaloníki              0.026903             0.031517      0.070067   \nthessaloníki              0.019594             0.029688      0.070067   \nthessaloníki              0.212503             0.064734      0.070067   \nthessaloníki              0.065076             0.600666      0.070067   \n...                            ...                  ...           ...   \nthessaloníki              0.012056             0.108169      0.070067   \nthessaloníki              0.151509             0.093742      0.070067   \nthessaloníki              0.008760             0.072405      0.070067   \nthessaloníki              0.043378             0.072405      0.070067   \nthessaloníki              0.054050             0.075253      0.070067   \n\n             orig_wiki_bi_prev  orig_wiki_bi_next  \nthessaloníki          0.037588           0.102629  \nthessaloníki          0.031906           0.060369  \nthessaloníki          0.000000           0.185942  \nthessaloníki          0.227539           0.092017  \nthessaloníki          0.082351           0.442959  \n...                        ...                ...  \nthessaloníki          0.029170           0.000000  \nthessaloníki          0.114203           0.067757  \nthessaloníki          0.161784           0.000000  \nthessaloníki          0.145342           0.000000  \nthessaloníki          0.017765           0.000000  \n\n[116 rows x 34 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>cosine_to_orig</th>\n      <th>frac_norms_seen</th>\n      <th>from_clipping</th>\n      <th>from_embeddings</th>\n      <th>from_original_token</th>\n      <th>from_spellcheck</th>\n      <th>from_split</th>\n      <th>norms_seen</th>\n      <th>spellcheck_score</th>\n      <th>length</th>\n      <th>...</th>\n      <th>twitter_bi_next</th>\n      <th>wiki_uni</th>\n      <th>wiki_bi_prev</th>\n      <th>wiki_bi_next</th>\n      <th>orig_twitter_uni</th>\n      <th>orig_twitter_bi_prev</th>\n      <th>orig_twitter_bi_next</th>\n      <th>orig_wiki_uni</th>\n      <th>orig_wiki_bi_prev</th>\n      <th>orig_wiki_bi_next</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>thessaloníki</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>-24</td>\n      <td>12</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>5.767776e-09</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.018497</td>\n      <td>0.045914</td>\n      <td>0.256350</td>\n      <td>0.070067</td>\n      <td>0.037588</td>\n      <td>0.102629</td>\n    </tr>\n    <tr>\n      <th>thessaloníki</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>-24</td>\n      <td>12</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>5.767776e-09</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.018497</td>\n      <td>0.026903</td>\n      <td>0.031517</td>\n      <td>0.070067</td>\n      <td>0.031906</td>\n      <td>0.060369</td>\n    </tr>\n    <tr>\n      <th>thessaloníki</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>-24</td>\n      <td>12</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>5.767776e-09</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.018497</td>\n      <td>0.019594</td>\n      <td>0.029688</td>\n      <td>0.070067</td>\n      <td>0.000000</td>\n      <td>0.185942</td>\n    </tr>\n    <tr>\n      <th>thessaloníki</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>-24</td>\n      <td>12</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>5.767776e-09</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.018497</td>\n      <td>0.212503</td>\n      <td>0.064734</td>\n      <td>0.070067</td>\n      <td>0.227539</td>\n      <td>0.092017</td>\n    </tr>\n    <tr>\n      <th>thessaloníki</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>-24</td>\n      <td>12</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>5.767776e-09</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.018497</td>\n      <td>0.065076</td>\n      <td>0.600666</td>\n      <td>0.070067</td>\n      <td>0.082351</td>\n      <td>0.442959</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>thessaloníki</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>-24</td>\n      <td>12</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>5.767776e-09</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.018497</td>\n      <td>0.012056</td>\n      <td>0.108169</td>\n      <td>0.070067</td>\n      <td>0.029170</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>thessaloníki</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>-24</td>\n      <td>12</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>5.767776e-09</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.018497</td>\n      <td>0.151509</td>\n      <td>0.093742</td>\n      <td>0.070067</td>\n      <td>0.114203</td>\n      <td>0.067757</td>\n    </tr>\n    <tr>\n      <th>thessaloníki</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>-24</td>\n      <td>12</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>5.767776e-09</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.018497</td>\n      <td>0.008760</td>\n      <td>0.072405</td>\n      <td>0.070067</td>\n      <td>0.161784</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>thessaloníki</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>-24</td>\n      <td>12</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>5.767776e-09</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.018497</td>\n      <td>0.043378</td>\n      <td>0.072405</td>\n      <td>0.070067</td>\n      <td>0.145342</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>thessaloníki</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>-24</td>\n      <td>12</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>5.767776e-09</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.018497</td>\n      <td>0.054050</td>\n      <td>0.075253</td>\n      <td>0.070067</td>\n      <td>0.017765</td>\n      <td>0.000000</td>\n    </tr>\n  </tbody>\n</table>\n<p>116 rows × 34 columns</p>\n</div>"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cands = load_candidates(os.path.join(DATA_PATH, \"hpc/fixed_dev_ngrams.norm\"))\n",
    "cands.loc[\"thessaloníki\"]\n",
    "\n",
    "# with open(os.path.join(DATA_PATH, \"processed/feature_lexicon.txt\"), \"rb\") as f:\n",
    "#     lex = pickle.load(f)\n",
    "# \"lol\" in lex"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def original_token(tok):\n",
    "    # MONOISE\n",
    "    # needed if detect step is skipped, as all tokens will be replaced by one from the list of candidates\n",
    "    return {tok}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "from lexnorm.data import word2vec\n",
    "vectors = word2vec.get_vectors(raw)\n",
    "# load embeddings from van der goot. Used params -size 100 -window 5 -cbow 0 -binary 1 -threads 45\n",
    "# there are 400 length embeddings which supposedly give slight performance improvement, but quite slow\n",
    "# unicode incompatibilities present so must ignore when loading"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/elijoe/Library/Mobile Documents/com~apple~CloudDocs/Documents/2/Diss/lexnorm/.venv/lib/python3.10/site-packages/gensim/models/keyedvectors.py:849: RuntimeWarning: invalid value encountered in divide\n",
      "  dists = dot(self.vectors[clip_start:clip_end], mean) / self.norms[clip_start:clip_end]\n"
     ]
    },
    {
     "data": {
      "text/plain": "[('ya', 0.8311921954154968),\n ('yo’', 0.8208641409873962),\n ('nigga’s', 0.7973958849906921),\n ('citch', 0.7568634748458862),\n ('shordy', 0.7551469206809998),\n ('😂😂🚮', 0.7489941716194153),\n ('😭like', 0.7452959418296814),\n ('nigga', 0.7426594495773315),\n ('hoe', 0.741933286190033),\n ('bitch🗣', 0.7406516671180725)]"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from lexnorm.generate_extract.filtering import is_eligible\n",
    "\n",
    "def word_embeddings(tok, vectors, threshold=0):\n",
    "    # TODO uni, bigram freqs?\n",
    "    # TODO implement word2vec with keras. Use newer embeddings. Experiment with different no. of candidates generated. Could even create twitter embeddings myself? Could clean up as VDG did before creating train embeddings. Cosine similarity threshold?\n",
    "    # MONOISE\n",
    "    # can use twitter embeddings from van der Goot - based on distributional hypothesis to find tokens with similar semantics\n",
    "    # could use cosine similarity as a feature for selection? Using here to get most similar candidates.\n",
    "    # ISSUE: antonyms also often present in same contexts.\n",
    "    candidates = set()\n",
    "    if tok in vectors:\n",
    "        candidates = set(vectors.similar_by_vector(tok))\n",
    "    return {c[0].lower() for c in candidates if is_eligible(c[0]) and c[1] >= threshold}\n",
    "\n",
    "# pretty much no tokens in train set not in twitter embeddings from vdg but can't assume this is the case\n",
    "# possible that tok will not be in vectors\n",
    "vectors.similar_by_vector(\"yo\")\n",
    "# pretty terrible performance for train_vectors - to be expected as such a low amount of data could literally just use external set as contains almost every word anyway and will be much more accurate"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "def lookup(tok, dictionary):\n",
    "    # TODO: external norm dicts?\n",
    "    # MONOISE\n",
    "    # lookup in list of all replacement pairs found in the training data (and external sources?)\n",
    "    # all norm tokens with raw token tok are included as candidates\n",
    "    return [(k, v) for k, v in dictionary.get(tok, {}).items()]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "def clipping(tok, lex):\n",
    "    # MONOISE\n",
    "    # all words in lexicon that have tok as a prefix (capturing abbreviation). May only consider for tok length above 2?\n",
    "    candidates = set()\n",
    "    if len(tok) < 2:\n",
    "        return set()\n",
    "    # TODO: length threshold? prune generated (only some degree of clipping allowed w.r.t. edit distance)?\n",
    "    return [t for t in lex if t.startswith(tok)]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "# TODO: number of candidates on average generated by each module and with all modules\n",
    "# TODO: which modules contribute the most / most unique correct candidates\n",
    "# TODO: for modules and whole, percentage of correct vs incorrect candidates"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "def split(tok, lex):\n",
    "    # MONOISE\n",
    "    # hypothesis splits on (every/some) position and check if both words are in lexicon. May only consider of tok length above 3?\n",
    "    candidates = set()\n",
    "    if len(tok) < 3:\n",
    "        return set()\n",
    "    for pos in range(1, len(tok)):\n",
    "        left = tok[:pos]\n",
    "        right = tok[pos:]\n",
    "        if left in lex and right in lex:\n",
    "            candidates.add(\" \".join([left, right]))\n",
    "    # TODO: recursive candidate generation on each left and right? Probably not... More than one split? Probably not either...\n",
    "    # TODO: length threshold?\n",
    "    return candidates"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "from spylls.hunspell import Dictionary\n",
    "\n",
    "def spellcheck(tok):\n",
    "    # TODO: no control over this - can I change in source code? Try and load in custom lexicon.\n",
    "    dictionary = Dictionary.from_files('en_US')\n",
    "    return {c.lower() for c in dictionary.suggest(tok)}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "# TODO: contextual features?\n",
    "# def generate_candidates(tweet):\n",
    "#     for token in tweet:\n",
    "#         ..."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "def generate_candidates(tok):\n",
    "    candidates = set()\n",
    "    candidates = candidates.union(original_token(tok))\n",
    "    candidates = candidates.union(word_embeddings(tok, w2v_vectors))\n",
    "    candidates = candidates.union(spellcheck(tok))\n",
    "    # obviously lookup on the train set will always give the correct answer!\n",
    "    # candidates = candidates.union(lookup(tok, norm_dictionary))\n",
    "    candidates = candidates.union(clipping(tok, lex))\n",
    "    candidates = candidates.union(split(tok, lex))\n",
    "    return candidates"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "from lexnorm.evaluation import condition_normalisation\n",
    "from lexnorm.data import normEval\n",
    "import os\n",
    "from lexnorm.definitions import DATA_PATH\n",
    "from lexnorm.data import norm_dict\n",
    "from lexnorm.models.filtering import is_eligible\n",
    "from random import sample\n",
    "\n",
    "norm_dictionary = norm_dict.construct(os.path.join(DATA_PATH, \"interim/train.txt\"))\n",
    "# keys = sample(list(norm_dictionary.keys()), 100)\n",
    "# assert condition_normalisation.contingency(\n",
    "#     raw, norm, lambda x: x[0] == \"a\", True\n",
    "# ) == condition_normalisation.contingency_from_dict(\n",
    "#     norm_dictionary, lambda x: x[0][0] == \"a\"\n",
    "# )\n",
    "a, b, c, d = condition_normalisation.contingency_from_dict(\n",
    "    norm_dictionary, lambda x: not x[1] in generate_candidates(x[0])\n",
    ")\n",
    "\n",
    "# print(sum(a.values()) + sum(b.values()) + sum(c.values()) + sum(d.values()))\n",
    "\n",
    "# a2, b2, c2, d2 = condition_normalisation.contingency(raw, norm, lambda x: x[0] == \"a\", True)\n",
    "\n",
    "# print(sum(a.values()) + sum(b.values()) + sum(c.values()) + sum(d.values()))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "# TODO merge module that checks some tokens ahead of current token (perhaps only one)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [],
   "source": [
    "import lexnorm.models.candidate_generation as candidate_generation\n",
    "import importlib\n",
    "import numpy as np\n",
    "from spylls.hunspell import Dictionary\n",
    "importlib.reload(candidate_generation)\n",
    "dictionary = Dictionary.from_files(\"en_US\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "train_data = pd.DataFrame()\n",
    "candidates = candidate_generation.candidates_from_token(\"bruh\", w2v_vectors, norm_dictionary, lex, dictionary)\n",
    "# candidates.cosine_to_orig = candidates.index.map(lambda x: w2v_vectors.similarity(x, \"lol\") if x in w2v_vectors else 0)\n",
    "# w2v_vectors.similarity(candidates.index, \"lol\") if indexes in w2v_vectors else 0\n",
    "# candidates.index.to_series()\n",
    "candidates"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "d1 = pd.DataFrame(columns=[\"feature\"])\n",
    "d2 = pd.DataFrame(columns=[\"feature\"])\n",
    "d1.loc[\"key\"] = {\"feature\": np.nan}\n",
    "d2.loc[\"key\"] = {\"feature\": 1}\n",
    "d2.loc[\"key2\"] = {\"feature\": 3}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "d1.combine_first(d2)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(os.path.join(DATA_PATH, \"hpc/dev_pipeline.txt\"), index_col=0)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join(DATA_PATH, \"../hpc/candidates.txt\"), index_col=0)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "outputs": [],
   "source": [
    "# get all tokens where correct normalisation not produced by candidate generation\n",
    "filtered = df.groupby(\"raw_tok_index\").filter(lambda x: x.sum()[\"correct\"] == 0)\n",
    "ungenerated = filtered.loc[filtered[\"from_original_token\"] == 1.0][\"gold\"]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "outputs": [],
   "source": [
    "raw, _ = loadNormData(os.path.join(DATA_PATH, \"raw/dev.norm\"))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "outputs": [
    {
     "data": {
      "text/plain": "6876"
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count = 0\n",
    "for raw_tweet, norm_tweet in zip(raw, norm):\n",
    "    for raw_tok, norm_tok in zip(raw_tweet, norm_tweet):\n",
    "        if is_eligible(raw_tok):\n",
    "            count += 1\n",
    "count\n",
    "# as expected - perhaps can be used to test candidate_generation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "outputs": [
    {
     "data": {
      "text/plain": "[(('v', 'very'), 2),\n (('hapi', 'happy'), 2),\n (('witchu', 'with you'), 2),\n (('yessss', 'yes'), 1),\n (('bestie', 'best friend'), 1),\n (('niggra', 'nigger'), 1),\n (('wada', 'water'), 1),\n (('chu', 'you'), 1),\n (('nows', 'now is'), 1),\n (('nuh', 'know'), 1),\n (('ntn', 'nothing'), 1),\n (('mnl', 'my new love'), 1),\n (('za', 'that'), 1),\n (('skepta', 'sunglasses'), 1),\n (('whatdoiwear', 'what do i wear'), 1),\n (('brutha', 'brother'), 1),\n (('yah', 'you'), 1),\n (('nuff', 'enough'), 1),\n (('shizz', 'shit'), 1),\n (('nuffin', 'nothing'), 1),\n (('diss', 'this'), 1),\n (('vas', 'was'), 1),\n (('redsox', 'red sox'), 1),\n (('nem', 'they'), 1),\n (('fkn', 'fucking'), 1),\n (('sim', 'seems'), 1),\n (('hbu', 'how about you'), 1),\n (('goddamit', 'god damn it'), 1),\n (('satnite', 'saturday night'), 1),\n (('dese', 'these'), 1),\n (('summn', 'something'), 1),\n (('cums', 'comes'), 1),\n (('dnt', \"doesn't\"), 1),\n (('getcha', 'get you'), 1),\n (('trynna', 'trying to'), 1),\n (('ya', 'your'), 1),\n (('lottle', 'lot'), 1),\n (('happylgovo', 'happy'), 1),\n (('gal', 'guy'), 1),\n (('fuccin', 'fucking'), 1),\n (('cuh', 'see you'), 1),\n (('datpiff', 'the piff'), 1),\n (('diy', 'do it yourself'), 1),\n (('da', 'that'), 1),\n (('shat', 'shit'), 1),\n (('slp', 'sleep'), 1),\n (('dia', 'their'), 1),\n (('devs', 'developer'), 1),\n (('yea', 'you'), 1),\n (('cannnot', \"can't\"), 1),\n (('ull', 'you will'), 1),\n (('icant', \"i can't\"), 1),\n (('brotha', 'brother'), 1),\n (('poooollll', 'pool'), 1),\n (('sup', \"what's up\"), 1),\n (('wassup', \"what's up\"), 1),\n (('pre', 'preorder'), 1),\n (('order', nan), 1)]"
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "Counter(zip(ungenerated.index, ungenerated)).most_common()\n",
    "# very few ungenerated correct candidates! So candidate generation module perhaps alright."
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
