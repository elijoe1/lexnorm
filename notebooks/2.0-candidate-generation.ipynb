{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from lexnorm.data import lexicon\n",
    "from lexnorm.data import norm_dict\n",
    "from lexnorm.data.normEval import loadNormData\n",
    "from lexnorm.definitions import DATA_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "raw, norm = loadNormData(os.path.join(DATA_PATH, \"interim/train.txt\"))\n",
    "norm_dictionary = norm_dict.from_train(os.path.join(DATA_PATH, \"interim/train.txt\"))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "lex = lexicon.build(\n",
    "        {\"english\", \"american\"},\n",
    "        {\"contractions\", \"proper-names\", \"upper\", \"words\"},\n",
    "        50,\n",
    "        1,\n",
    "    )\n",
    "\n",
    "lex = lexicon.refine(lex.union(lexicon.build_abbreviations()))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation of lexicon with normalisation: 0.43\n",
      "Most common un-normalised raw alphanumeric tokens in lexicon: [(('i', 'i'), 648), (('the', 'the'), 631), (('to', 'to'), 534), (('a', 'a'), 479), (('and', 'and'), 411), (('you', 'you'), 340), (('in', 'in'), 326), (('for', 'for'), 320), (('is', 'is'), 318), (('me', 'me'), 281), (('my', 'my'), 278), (('lol', 'lol'), 272), (('on', 'on'), 271), (('of', 'of'), 249), (('it', 'it'), 203), (('with', 'with'), 186), (('that', 'that'), 185), (('this', 'this'), 180), (('so', 'so'), 180), (('be', 'be'), 159)]\n",
      "Most common normalised raw alphanumeric tokens in lexicon: [(('nigga', 'nigger'), 57), (('niggas', 'niggers'), 52), (('ur', 'your'), 33), (('gonna', 'going to'), 29), (('bout', 'about'), 21), (('wit', 'with'), 17), (('tho', 'though'), 17), (('cause', 'because'), 16), (('wanna', 'want to'), 12), (('cant', \"can't\"), 12), (('ur', \"you're\"), 12), (('yo', 'you'), 11), (('nd', 'and'), 11), (('ill', \"i'll\"), 9), (('dis', 'this'), 9), (('yo', 'your'), 8), (('em', 'them'), 8), (('yea', 'yeah'), 7), (('its', \"it's\"), 7), (('rn', 'right now'), 6)]\n",
      "Most common un-normalised raw alphanumeric tokens not in lexicon: [(('haha', 'haha'), 81), (('bae', 'bae'), 45), (('x', 'x'), 44), (('niall', 'niall'), 40), (('2', '2'), 40), (('hahaha', 'hahaha'), 36), (('idk', 'idk'), 36), (('1', '1'), 34), (('rp', 'rp'), 33), (('exo', 'exo'), 31), (('wtf', 'wtf'), 26), (('t', 't'), 25), (('5', '5'), 25), (('zayn', 'zayn'), 23), (('2014', '2014'), 23), (('smh', 'smh'), 21), (('c', 'c'), 21), (('b', 'b'), 21), (('3', '3'), 21), (('4', '4'), 20)]\n",
      "Most common normalised raw alphanumeric tokens not in lexicon: [(('u', 'you'), 328), (('im', \"i'm\"), 181), (('dont', \"don't\"), 92), (('n', 'and'), 47), (('pls', 'please'), 43), (('lil', 'little'), 35), (('thats', \"that's\"), 33), (('bruh', 'brother'), 31), (('aint', \"ain't\"), 30), (('rt', 'retweet'), 26), (('ima', \"i'm going to\"), 25), (('ppl', 'people'), 25), (('yall', \"y'all\"), 23), (('cuz', 'because'), 22), (('r', 'are'), 22), (('d', 'the'), 22), (('didnt', \"didn't\"), 19), (('gon', 'gonna'), 18), (('tryna', 'trying to'), 18), (('imma', \"i'm going to\"), 18)]\n"
     ]
    }
   ],
   "source": [
    "lexicon.evaluate(raw, norm, lex)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def original_token(tok):\n",
    "    # MONOISE\n",
    "    # needed if detect step is skipped, as all tokens will be replaced by one from the list of candidates\n",
    "    return {tok}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "import gensim\n",
    "\n",
    "w2v_vectors = gensim.models.KeyedVectors.load_word2vec_format(\n",
    "        os.path.join(DATA_PATH, \"external/monoise_data/w2v.bin\"),\n",
    "        binary=True,\n",
    "        unicode_errors=\"ignore\",\n",
    "    )\n",
    "# load embeddings from van der goot. Used params -size 100 -window 5 -cbow 0 -binary 1 -threads 45\n",
    "# unicode imcompatibilities present so must ignore"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "train_model = model = gensim.models.Word2Vec(sentences=raw, vector_size=100, window=5)\n",
    "# get keyed vectors only as finished training model\n",
    "train_vectors = train_model.wv\n",
    "w2v_vectors.add_vectors(train_vectors.index_to_key, train_vectors.vectors)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/elijoe/Library/Mobile Documents/com~apple~CloudDocs/Documents/2/Diss/lexnorm/.venv/lib/python3.10/site-packages/gensim/models/keyedvectors.py:849: RuntimeWarning: invalid value encountered in divide\n",
      "  dists = dot(self.vectors[clip_start:clip_end], mean) / self.norms[clip_start:clip_end]\n"
     ]
    },
    {
     "data": {
      "text/plain": "{'lmao', 'lmaooo', 'lmfao', 'smh'}"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from lexnorm.models.annotation import list_eligible\n",
    "\n",
    "\n",
    "def word_embeddings(tok, vectors, threshold=0):\n",
    "    # TODO uni, bigram freqs?\n",
    "    # TODO implement word2vec with keras. Use newer embeddings. Experiment with different no. of candidates generated. Could even create twitter embeddings myself? Could clean up as VDG did before creating train embeddings. Cosine similarity threshold?\n",
    "    # MONOISE\n",
    "    # can use twitter embeddings from van der Goot - based on distributional hypothesis to find tokens with similar semantics\n",
    "    # could use cosine similarity as a feature for selection? Using here to get most similar candidates.\n",
    "    # ISSUE: antonyms also often present in same contexts.\n",
    "    candidates = set()\n",
    "    if tok in vectors:\n",
    "        candidates = set(vectors.most_similar([tok]))\n",
    "    return {c[0] for c in candidates if list_eligible([c[0]]) == [1] and c[1] >= threshold}\n",
    "\n",
    "# pretty much no tokens in train set not in twitter embeddings from vdg but can't assume this is the case\n",
    "# possible that tok will not be in vectors\n",
    "word_embeddings(\"lol\", w2v_vectors)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "def lookup(tok, dictionary):\n",
    "    # TODO: external norm dicts?\n",
    "    # MONOISE\n",
    "    # lookup in list of all replacement pairs found in the training data (and external sources?)\n",
    "    # all norm tokens with raw token tok are included as candidates\n",
    "    return {v for v in dictionary.get(tok, {}).keys()}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "def clipping(tok, lex):\n",
    "    # MONOISE\n",
    "    # all words in lexicon that have tok as a prefix (capturing abbreviation). May only consider for tok length above 2?\n",
    "    candidates = set()\n",
    "    if len(tok) < 2:\n",
    "        return set()\n",
    "    # TODO: length threshold? prune generated (only some degree of clipping allowed w.r.t. edit distance)?\n",
    "    return [t for t in lex if t.startswith(tok)]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "# TODO: number of candidates on average generated by each module and with all modules\n",
    "# TODO: which modules contribute the most / most unique correct candidates\n",
    "# TODO: for modules and whole, percentage of correct vs incorrect candidates"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "def split(tok, lex):\n",
    "    # MONOISE\n",
    "    # hypothesis splits on (every/some) position and check if both words are in lexicon. May only consider of tok length above 3?\n",
    "    candidates = set()\n",
    "    if len(tok) < 3:\n",
    "        return set()\n",
    "    for pos in range(1, len(tok)):\n",
    "        left = tok[:pos]\n",
    "        right = tok[pos:]\n",
    "        if left in lex and right in lex:\n",
    "            candidates.add(\" \".join([left, right]))\n",
    "    # TODO: recursive candidate generation on each left and right? Probably not... More than one split? Probably not either...\n",
    "    # TODO: length threshold?\n",
    "    return candidates"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "from spylls.hunspell import Dictionary\n",
    "\n",
    "def spellcheck(tok):\n",
    "    # TODO: no control over this - can I change in source code? Try and load in custom lexicon.\n",
    "    dictionary = Dictionary.from_files('en_US')\n",
    "    return {c for c in dictionary.suggest(tok)}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "# TODO: contextual features?\n",
    "# def generate_candidates(tweet):\n",
    "#     for token in tweet:\n",
    "#         ..."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "def generate_candidates(tok):\n",
    "    candidates = set()\n",
    "    candidates = candidates.union(original_token(tok))\n",
    "    candidates = candidates.union(word_embeddings(tok, w2v_vectors))\n",
    "    candidates = candidates.union(spellcheck(tok))\n",
    "    candidates = candidates.union(lookup(tok, norm_dictionary))\n",
    "    candidates = candidates.union(clipping(tok, lex))\n",
    "    candidates = candidates.union(split(tok, lex))\n",
    "    return candidates"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "{'Deng',\n 'Peking',\n 'blick',\n 'butters',\n 'butterz',\n 'clapt',\n 'minging',\n 'neeky',\n 'pang',\n 'peeing',\n 'peen',\n 'peg',\n 'pen',\n 'pen g',\n 'pen-g',\n 'pend',\n 'peng',\n 'penger',\n 'pengggggg',\n 'penguin',\n \"penguin's\",\n 'penguins',\n 'pens',\n 'pent',\n 'peonage',\n 'pieing',\n 'ping',\n 'pong'}"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_candidates(\"peng\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
