{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from lexnorm.data import lexicon\n",
    "from lexnorm.data import norm_dict\n",
    "from lexnorm.data.normEval import loadNormData\n",
    "from lexnorm.definitions import DATA_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "raw, norm = loadNormData(os.path.join(DATA_PATH, \"processed/combined.txt\"))\n",
    "norm_dictionary = norm_dict.construct(raw, norm)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def original_token(tok):\n",
    "    # MONOISE\n",
    "    # needed if detect step is skipped, as all tokens will be replaced by one from the list of candidates\n",
    "    return {tok}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "from lexnorm.data import word2vec\n",
    "vectors = word2vec.get_vectors(raw)\n",
    "# load embeddings from van der goot. Used params -size 100 -window 5 -cbow 0 -binary 1 -threads 45\n",
    "# there are 400 length embeddings which supposedly give slight performance improvement, but quite slow\n",
    "# unicode incompatibilities present so must ignore when loading"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/elijoe/Library/Mobile Documents/com~apple~CloudDocs/Documents/2/Diss/lexnorm/.venv/lib/python3.10/site-packages/gensim/models/keyedvectors.py:849: RuntimeWarning: invalid value encountered in divide\n",
      "  dists = dot(self.vectors[clip_start:clip_end], mean) / self.norms[clip_start:clip_end]\n"
     ]
    },
    {
     "data": {
      "text/plain": "[('ya', 0.8311921954154968),\n ('yoâ€™', 0.8208641409873962),\n ('niggaâ€™s', 0.7973958849906921),\n ('citch', 0.7568634748458862),\n ('shordy', 0.7551469206809998),\n ('ðŸ˜‚ðŸ˜‚ðŸš®', 0.7489941716194153),\n ('ðŸ˜­like', 0.7452959418296814),\n ('nigga', 0.7426594495773315),\n ('hoe', 0.741933286190033),\n ('bitchðŸ—£', 0.7406516671180725)]"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from lexnorm.generate_extract.filtering import is_eligible\n",
    "\n",
    "def word_embeddings(tok, vectors, threshold=0):\n",
    "    # TODO uni, bigram freqs?\n",
    "    # TODO implement word2vec with keras. Use newer embeddings. Experiment with different no. of candidates generated. Could even create twitter embeddings myself? Could clean up as VDG did before creating train embeddings. Cosine similarity threshold?\n",
    "    # MONOISE\n",
    "    # can use twitter embeddings from van der Goot - based on distributional hypothesis to find tokens with similar semantics\n",
    "    # could use cosine similarity as a feature for selection? Using here to get most similar candidates.\n",
    "    # ISSUE: antonyms also often present in same contexts.\n",
    "    candidates = set()\n",
    "    if tok in vectors:\n",
    "        candidates = set(vectors.similar_by_vector(tok))\n",
    "    return {c[0].lower() for c in candidates if is_eligible(c[0]) and c[1] >= threshold}\n",
    "\n",
    "# pretty much no tokens in train set not in twitter embeddings from vdg but can't assume this is the case\n",
    "# possible that tok will not be in vectors\n",
    "vectors.similar_by_vector(\"yo\")\n",
    "# pretty terrible performance for train_vectors - to be expected as such a low amount of data could literally just use external set as contains almost every word anyway and will be much more accurate"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "[('bitch', 28)]"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def lookup(tok, dictionary):\n",
    "    # TODO: external norm dicts?\n",
    "    # MONOISE\n",
    "    # lookup in list of all replacement pairs found in the training data (and external sources?)\n",
    "    # all norm tokens with raw token tok are included as candidates\n",
    "    return [(k, v) for k, v in dictionary.get(tok, {}).items()]\n",
    "\n",
    "lookup(\"bitch\", norm_dictionary)\n",
    "# norm_dictionary"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "def clipping(tok, lex):\n",
    "    # MONOISE\n",
    "    # all words in lexicon that have tok as a prefix (capturing abbreviation). May only consider for tok length above 2?\n",
    "    candidates = set()\n",
    "    if len(tok) < 2:\n",
    "        return set()\n",
    "    # TODO: length threshold? prune generated (only some degree of clipping allowed w.r.t. edit distance)?\n",
    "    return [t for t in lex if t.startswith(tok)]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "# TODO: number of candidates on average generated by each module and with all modules\n",
    "# TODO: which modules contribute the most / most unique correct candidates\n",
    "# TODO: for modules and whole, percentage of correct vs incorrect candidates"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "def split(tok, lex):\n",
    "    # MONOISE\n",
    "    # hypothesis splits on (every/some) position and check if both words are in lexicon. May only consider of tok length above 3?\n",
    "    candidates = set()\n",
    "    if len(tok) < 3:\n",
    "        return set()\n",
    "    for pos in range(1, len(tok)):\n",
    "        left = tok[:pos]\n",
    "        right = tok[pos:]\n",
    "        if left in lex and right in lex:\n",
    "            candidates.add(\" \".join([left, right]))\n",
    "    # TODO: recursive candidate generation on each left and right? Probably not... More than one split? Probably not either...\n",
    "    # TODO: length threshold?\n",
    "    return candidates"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "from spylls.hunspell import Dictionary\n",
    "\n",
    "def spellcheck(tok):\n",
    "    # TODO: no control over this - can I change in source code? Try and load in custom lexicon.\n",
    "    dictionary = Dictionary.from_files('en_US')\n",
    "    return {c.lower() for c in dictionary.suggest(tok)}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "# TODO: contextual features?\n",
    "# def generate_candidates(tweet):\n",
    "#     for token in tweet:\n",
    "#         ..."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "def generate_candidates(tok):\n",
    "    candidates = set()\n",
    "    candidates = candidates.union(original_token(tok))\n",
    "    candidates = candidates.union(word_embeddings(tok, w2v_vectors))\n",
    "    candidates = candidates.union(spellcheck(tok))\n",
    "    # obviously lookup on the train set will always give the correct answer!\n",
    "    # candidates = candidates.union(lookup(tok, norm_dictionary))\n",
    "    candidates = candidates.union(clipping(tok, lex))\n",
    "    candidates = candidates.union(split(tok, lex))\n",
    "    return candidates"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[21], line 16\u001B[0m\n\u001B[1;32m      9\u001B[0m norm_dictionary \u001B[38;5;241m=\u001B[39m norm_dict\u001B[38;5;241m.\u001B[39mconstruct(os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(DATA_PATH, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124minterim/train.txt\u001B[39m\u001B[38;5;124m\"\u001B[39m))\n\u001B[1;32m     10\u001B[0m \u001B[38;5;66;03m# keys = sample(list(norm_dictionary.keys()), 100)\u001B[39;00m\n\u001B[1;32m     11\u001B[0m \u001B[38;5;66;03m# assert condition_normalisation.contingency(\u001B[39;00m\n\u001B[1;32m     12\u001B[0m \u001B[38;5;66;03m#     raw, norm, lambda x: x[0] == \"a\", True\u001B[39;00m\n\u001B[1;32m     13\u001B[0m \u001B[38;5;66;03m# ) == condition_normalisation.contingency_from_dict(\u001B[39;00m\n\u001B[1;32m     14\u001B[0m \u001B[38;5;66;03m#     norm_dictionary, lambda x: x[0][0] == \"a\"\u001B[39;00m\n\u001B[1;32m     15\u001B[0m \u001B[38;5;66;03m# )\u001B[39;00m\n\u001B[0;32m---> 16\u001B[0m a, b, c, d \u001B[38;5;241m=\u001B[39m \u001B[43mcondition_normalisation\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcontingency_from_dict\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     17\u001B[0m \u001B[43m    \u001B[49m\u001B[43mnorm_dictionary\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mlambda\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mx\u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mnot\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mx\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mgenerate_candidates\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     18\u001B[0m \u001B[43m)\u001B[49m\n\u001B[1;32m     20\u001B[0m \u001B[38;5;66;03m# print(sum(a.values()) + sum(b.values()) + sum(c.values()) + sum(d.values()))\u001B[39;00m\n\u001B[1;32m     21\u001B[0m \n\u001B[1;32m     22\u001B[0m \u001B[38;5;66;03m# a2, b2, c2, d2 = condition_normalisation.contingency(raw, norm, lambda x: x[0] == \"a\", True)\u001B[39;00m\n\u001B[1;32m     23\u001B[0m \n\u001B[1;32m     24\u001B[0m \u001B[38;5;66;03m# print(sum(a.values()) + sum(b.values()) + sum(c.values()) + sum(d.values()))\u001B[39;00m\n",
      "File \u001B[0;32m~/Library/Mobile Documents/com~apple~CloudDocs/Documents/2/Diss/lexnorm/src/lexnorm/evaluation/condition_normalisation.py:75\u001B[0m, in \u001B[0;36mcontingency_from_dict\u001B[0;34m(norm_dict, condition, result)\u001B[0m\n\u001B[1;32m     71\u001B[0m to_update \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m     72\u001B[0m     pair \u001B[38;5;28;01mif\u001B[39;00m result \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpair\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m (pair[\u001B[38;5;241m0\u001B[39m] \u001B[38;5;28;01mif\u001B[39;00m result \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mraw\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m pair[\u001B[38;5;241m1\u001B[39m])\n\u001B[1;32m     73\u001B[0m )\n\u001B[1;32m     74\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m pair[\u001B[38;5;241m0\u001B[39m] \u001B[38;5;241m==\u001B[39m pair[\u001B[38;5;241m1\u001B[39m]:\n\u001B[0;32m---> 75\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[43mcondition\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpair\u001B[49m\u001B[43m)\u001B[49m:\n\u001B[1;32m     76\u001B[0m         p_unnormed\u001B[38;5;241m.\u001B[39mupdate([to_update] \u001B[38;5;241m*\u001B[39m count)\n\u001B[1;32m     77\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "Cell \u001B[0;32mIn[21], line 17\u001B[0m, in \u001B[0;36m<lambda>\u001B[0;34m(x)\u001B[0m\n\u001B[1;32m      9\u001B[0m norm_dictionary \u001B[38;5;241m=\u001B[39m norm_dict\u001B[38;5;241m.\u001B[39mconstruct(os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(DATA_PATH, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124minterim/train.txt\u001B[39m\u001B[38;5;124m\"\u001B[39m))\n\u001B[1;32m     10\u001B[0m \u001B[38;5;66;03m# keys = sample(list(norm_dictionary.keys()), 100)\u001B[39;00m\n\u001B[1;32m     11\u001B[0m \u001B[38;5;66;03m# assert condition_normalisation.contingency(\u001B[39;00m\n\u001B[1;32m     12\u001B[0m \u001B[38;5;66;03m#     raw, norm, lambda x: x[0] == \"a\", True\u001B[39;00m\n\u001B[1;32m     13\u001B[0m \u001B[38;5;66;03m# ) == condition_normalisation.contingency_from_dict(\u001B[39;00m\n\u001B[1;32m     14\u001B[0m \u001B[38;5;66;03m#     norm_dictionary, lambda x: x[0][0] == \"a\"\u001B[39;00m\n\u001B[1;32m     15\u001B[0m \u001B[38;5;66;03m# )\u001B[39;00m\n\u001B[1;32m     16\u001B[0m a, b, c, d \u001B[38;5;241m=\u001B[39m condition_normalisation\u001B[38;5;241m.\u001B[39mcontingency_from_dict(\n\u001B[0;32m---> 17\u001B[0m     norm_dictionary, \u001B[38;5;28;01mlambda\u001B[39;00m x: \u001B[38;5;129;01mnot\u001B[39;00m x[\u001B[38;5;241m1\u001B[39m] \u001B[38;5;129;01min\u001B[39;00m \u001B[43mgenerate_candidates\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     18\u001B[0m )\n\u001B[1;32m     20\u001B[0m \u001B[38;5;66;03m# print(sum(a.values()) + sum(b.values()) + sum(c.values()) + sum(d.values()))\u001B[39;00m\n\u001B[1;32m     21\u001B[0m \n\u001B[1;32m     22\u001B[0m \u001B[38;5;66;03m# a2, b2, c2, d2 = condition_normalisation.contingency(raw, norm, lambda x: x[0] == \"a\", True)\u001B[39;00m\n\u001B[1;32m     23\u001B[0m \n\u001B[1;32m     24\u001B[0m \u001B[38;5;66;03m# print(sum(a.values()) + sum(b.values()) + sum(c.values()) + sum(d.values()))\u001B[39;00m\n",
      "Cell \u001B[0;32mIn[20], line 4\u001B[0m, in \u001B[0;36mgenerate_candidates\u001B[0;34m(tok)\u001B[0m\n\u001B[1;32m      2\u001B[0m candidates \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n\u001B[1;32m      3\u001B[0m candidates \u001B[38;5;241m=\u001B[39m candidates\u001B[38;5;241m.\u001B[39munion(original_token(tok))\n\u001B[0;32m----> 4\u001B[0m candidates \u001B[38;5;241m=\u001B[39m candidates\u001B[38;5;241m.\u001B[39munion(\u001B[43mword_embeddings\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtok\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mw2v_vectors\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[1;32m      5\u001B[0m candidates \u001B[38;5;241m=\u001B[39m candidates\u001B[38;5;241m.\u001B[39munion(spellcheck(tok))\n\u001B[1;32m      6\u001B[0m \u001B[38;5;66;03m# obviously lookup on the train set will always give the correct answer!\u001B[39;00m\n\u001B[1;32m      7\u001B[0m \u001B[38;5;66;03m# candidates = candidates.union(lookup(tok, norm_dictionary))\u001B[39;00m\n",
      "Cell \u001B[0;32mIn[13], line 13\u001B[0m, in \u001B[0;36mword_embeddings\u001B[0;34m(tok, vectors, threshold)\u001B[0m\n\u001B[1;32m     11\u001B[0m candidates \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n\u001B[1;32m     12\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m tok \u001B[38;5;129;01min\u001B[39;00m vectors:\n\u001B[0;32m---> 13\u001B[0m     candidates \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m(\u001B[43mvectors\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msimilar_by_vector\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtok\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[1;32m     14\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m {c[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39mlower() \u001B[38;5;28;01mfor\u001B[39;00m c \u001B[38;5;129;01min\u001B[39;00m candidates \u001B[38;5;28;01mif\u001B[39;00m is_eligible(c[\u001B[38;5;241m0\u001B[39m]) \u001B[38;5;129;01mand\u001B[39;00m c[\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m>\u001B[39m\u001B[38;5;241m=\u001B[39m threshold}\n",
      "File \u001B[0;32m~/Library/Mobile Documents/com~apple~CloudDocs/Documents/2/Diss/lexnorm/.venv/lib/python3.10/site-packages/gensim/models/keyedvectors.py:914\u001B[0m, in \u001B[0;36mKeyedVectors.similar_by_vector\u001B[0;34m(self, vector, topn, restrict_vocab)\u001B[0m\n\u001B[1;32m    890\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21msimilar_by_vector\u001B[39m(\u001B[38;5;28mself\u001B[39m, vector, topn\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m10\u001B[39m, restrict_vocab\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[1;32m    891\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Find the top-N most similar keys by vector.\u001B[39;00m\n\u001B[1;32m    892\u001B[0m \n\u001B[1;32m    893\u001B[0m \u001B[38;5;124;03m    Parameters\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    912\u001B[0m \n\u001B[1;32m    913\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 914\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmost_similar\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpositive\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m[\u001B[49m\u001B[43mvector\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtopn\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtopn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrestrict_vocab\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrestrict_vocab\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Library/Mobile Documents/com~apple~CloudDocs/Documents/2/Diss/lexnorm/.venv/lib/python3.10/site-packages/gensim/models/keyedvectors.py:852\u001B[0m, in \u001B[0;36mKeyedVectors.most_similar\u001B[0;34m(self, positive, negative, topn, clip_start, clip_end, restrict_vocab, indexer)\u001B[0m\n\u001B[1;32m    850\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m topn:\n\u001B[1;32m    851\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m dists\n\u001B[0;32m--> 852\u001B[0m best \u001B[38;5;241m=\u001B[39m \u001B[43mmatutils\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43margsort\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdists\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtopn\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtopn\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;28;43mlen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mall_keys\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreverse\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[1;32m    853\u001B[0m \u001B[38;5;66;03m# ignore (don't return) keys from the input\u001B[39;00m\n\u001B[1;32m    854\u001B[0m result \u001B[38;5;241m=\u001B[39m [\n\u001B[1;32m    855\u001B[0m     (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mindex_to_key[sim \u001B[38;5;241m+\u001B[39m clip_start], \u001B[38;5;28mfloat\u001B[39m(dists[sim]))\n\u001B[1;32m    856\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m sim \u001B[38;5;129;01min\u001B[39;00m best \u001B[38;5;28;01mif\u001B[39;00m (sim \u001B[38;5;241m+\u001B[39m clip_start) \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m all_keys\n\u001B[1;32m    857\u001B[0m ]\n",
      "File \u001B[0;32m~/Library/Mobile Documents/com~apple~CloudDocs/Documents/2/Diss/lexnorm/.venv/lib/python3.10/site-packages/gensim/matutils.py:74\u001B[0m, in \u001B[0;36margsort\u001B[0;34m(x, topn, reverse)\u001B[0m\n\u001B[1;32m     72\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m reverse:\n\u001B[1;32m     73\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m-\u001B[39mx\n\u001B[0;32m---> 74\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m topn \u001B[38;5;241m>\u001B[39m\u001B[38;5;241m=\u001B[39m x\u001B[38;5;241m.\u001B[39msize \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;43mhasattr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mnp\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43margpartition\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m:\n\u001B[1;32m     75\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m np\u001B[38;5;241m.\u001B[39margsort(x)[:topn]\n\u001B[1;32m     76\u001B[0m \u001B[38;5;66;03m# np >= 1.8 has a fast partial argsort, use that!\u001B[39;00m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "from lexnorm.evaluation import condition_normalisation\n",
    "from lexnorm.data import normEval\n",
    "import os\n",
    "from lexnorm.definitions import DATA_PATH\n",
    "from lexnorm.data import norm_dict\n",
    "from lexnorm.models.filtering import is_eligible\n",
    "from random import sample\n",
    "\n",
    "norm_dictionary = norm_dict.construct(os.path.join(DATA_PATH, \"interim/train.txt\"))\n",
    "# keys = sample(list(norm_dictionary.keys()), 100)\n",
    "# assert condition_normalisation.contingency(\n",
    "#     raw, norm, lambda x: x[0] == \"a\", True\n",
    "# ) == condition_normalisation.contingency_from_dict(\n",
    "#     norm_dictionary, lambda x: x[0][0] == \"a\"\n",
    "# )\n",
    "a, b, c, d = condition_normalisation.contingency_from_dict(\n",
    "    norm_dictionary, lambda x: not x[1] in generate_candidates(x[0])\n",
    ")\n",
    "\n",
    "# print(sum(a.values()) + sum(b.values()) + sum(c.values()) + sum(d.values()))\n",
    "\n",
    "# a2, b2, c2, d2 = condition_normalisation.contingency(raw, norm, lambda x: x[0] == \"a\", True)\n",
    "\n",
    "# print(sum(a.values()) + sum(b.values()) + sum(c.values()) + sum(d.values()))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "# TODO merge module that checks some tokens ahead of current token (perhaps only one)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot set a row with mismatched columns",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[23], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mpandas\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mpd\u001B[39;00m\n\u001B[1;32m      2\u001B[0m candidates \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mDataFrame(columns\u001B[38;5;241m=\u001B[39m[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfeature1\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfeature2\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfeature3\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n\u001B[0;32m----> 3\u001B[0m \u001B[43mcandidates\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mloc\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtesting\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m \u001B[38;5;241m=\u001B[39m [\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m3\u001B[39m]\n\u001B[1;32m      4\u001B[0m candidates\n\u001B[1;32m      5\u001B[0m \u001B[38;5;66;03m# new_candidates = pd.DataFrame(columns=)\u001B[39;00m\n",
      "File \u001B[0;32m~/Library/Mobile Documents/com~apple~CloudDocs/Documents/2/Diss/lexnorm/.venv/lib/python3.10/site-packages/pandas/core/indexing.py:818\u001B[0m, in \u001B[0;36m_LocationIndexer.__setitem__\u001B[0;34m(self, key, value)\u001B[0m\n\u001B[1;32m    815\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_has_valid_setitem_indexer(key)\n\u001B[1;32m    817\u001B[0m iloc \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mname \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124miloc\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mobj\u001B[38;5;241m.\u001B[39miloc\n\u001B[0;32m--> 818\u001B[0m \u001B[43miloc\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_setitem_with_indexer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindexer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvalue\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Library/Mobile Documents/com~apple~CloudDocs/Documents/2/Diss/lexnorm/.venv/lib/python3.10/site-packages/pandas/core/indexing.py:1785\u001B[0m, in \u001B[0;36m_iLocIndexer._setitem_with_indexer\u001B[0;34m(self, indexer, value, name)\u001B[0m\n\u001B[1;32m   1782\u001B[0m     indexer, missing \u001B[38;5;241m=\u001B[39m convert_missing_indexer(indexer)\n\u001B[1;32m   1784\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m missing:\n\u001B[0;32m-> 1785\u001B[0m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_setitem_with_indexer_missing\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindexer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvalue\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1786\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m\n\u001B[1;32m   1788\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m name \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mloc\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m   1789\u001B[0m     \u001B[38;5;66;03m# must come after setting of missing\u001B[39;00m\n",
      "File \u001B[0;32m~/Library/Mobile Documents/com~apple~CloudDocs/Documents/2/Diss/lexnorm/.venv/lib/python3.10/site-packages/pandas/core/indexing.py:2160\u001B[0m, in \u001B[0;36m_iLocIndexer._setitem_with_indexer_missing\u001B[0;34m(self, indexer, value)\u001B[0m\n\u001B[1;32m   2157\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m is_list_like_indexer(value):\n\u001B[1;32m   2158\u001B[0m         \u001B[38;5;66;03m# must have conforming columns\u001B[39;00m\n\u001B[1;32m   2159\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(value) \u001B[38;5;241m!=\u001B[39m \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mobj\u001B[38;5;241m.\u001B[39mcolumns):\n\u001B[0;32m-> 2160\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcannot set a row with mismatched columns\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m   2162\u001B[0m     value \u001B[38;5;241m=\u001B[39m Series(value, index\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mobj\u001B[38;5;241m.\u001B[39mcolumns, name\u001B[38;5;241m=\u001B[39mindexer)\n\u001B[1;32m   2164\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mobj):\n\u001B[1;32m   2165\u001B[0m     \u001B[38;5;66;03m# We will ignore the existing dtypes instead of using\u001B[39;00m\n\u001B[1;32m   2166\u001B[0m     \u001B[38;5;66;03m#  internals.concat logic\u001B[39;00m\n",
      "\u001B[0;31mValueError\u001B[0m: cannot set a row with mismatched columns"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "candidates = pd.DataFrame(columns=[\"feature1\", \"feature2\", \"feature3\"])\n",
    "candidates.loc[\"testing\"] = [1, 3]\n",
    "candidates\n",
    "# new_candidates = pd.DataFrame(columns=)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [],
   "source": [
    "import lexnorm.models.candidate_generation as candidate_generation\n",
    "import importlib\n",
    "import numpy as np\n",
    "from spylls.hunspell import Dictionary\n",
    "importlib.reload(candidate_generation)\n",
    "dictionary = Dictionary.from_files(\"en_US\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'candidate_generation' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[42], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m train_data \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mDataFrame()\n\u001B[0;32m----> 2\u001B[0m candidates \u001B[38;5;241m=\u001B[39m \u001B[43mcandidate_generation\u001B[49m\u001B[38;5;241m.\u001B[39mcandidates_from_token(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbruh\u001B[39m\u001B[38;5;124m\"\u001B[39m, w2v_vectors, norm_dictionary, lex, dictionary)\n\u001B[1;32m      3\u001B[0m \u001B[38;5;66;03m# candidates.cosine_to_orig = candidates.index.map(lambda x: w2v_vectors.similarity(x, \"lol\") if x in w2v_vectors else 0)\u001B[39;00m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;66;03m# w2v_vectors.similarity(candidates.index, \"lol\") if indexes in w2v_vectors else 0\u001B[39;00m\n\u001B[1;32m      5\u001B[0m \u001B[38;5;66;03m# candidates.index.to_series()\u001B[39;00m\n\u001B[1;32m      6\u001B[0m candidates\n",
      "\u001B[0;31mNameError\u001B[0m: name 'candidate_generation' is not defined"
     ]
    }
   ],
   "source": [
    "train_data = pd.DataFrame()\n",
    "candidates = candidate_generation.candidates_from_token(\"bruh\", w2v_vectors, norm_dictionary, lex, dictionary)\n",
    "# candidates.cosine_to_orig = candidates.index.map(lambda x: w2v_vectors.similarity(x, \"lol\") if x in w2v_vectors else 0)\n",
    "# w2v_vectors.similarity(candidates.index, \"lol\") if indexes in w2v_vectors else 0\n",
    "# candidates.index.to_series()\n",
    "candidates"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "d1 = pd.DataFrame(columns=[\"feature\"])\n",
    "d2 = pd.DataFrame(columns=[\"feature\"])\n",
    "d1.loc[\"key\"] = {\"feature\": np.nan}\n",
    "d2.loc[\"key\"] = {\"feature\": 1}\n",
    "d2.loc[\"key2\"] = {\"feature\": 3}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'd1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[28], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43md1\u001B[49m\u001B[38;5;241m.\u001B[39mcombine_first(d2)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'd1' is not defined"
     ]
    }
   ],
   "source": [
    "d1.combine_first(d2)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(os.path.join(DATA_PATH, \"hpc/dev_pipeline.txt\"), index_col=0)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "        cosine_to_orig  embeddings_rank  from_clipping  from_original_token  \\\nare           0.074208              NaN            NaN                  NaN   \nbare          0.278520              NaN            NaN                  NaN   \nbe            0.124159              NaN            NaN                  NaN   \nbee           0.465715              NaN            NaN                  NaN   \nbore          0.213253              NaN            NaN                  NaN   \n...                ...              ...            ...                  ...   \nnasia         0.859196             10.0            NaN                  NaN   \nnesha         0.870439              5.0            NaN                  NaN   \nquisha        0.863121              9.0            NaN                  NaN   \nre            0.369029              NaN            NaN                  NaN   \ntre           0.878271              4.0            NaN                  NaN   \n\n        from_split  norms_seen  spellcheck_rank  in_lexicon  length  \\\nare            NaN         NaN             15.0         1.0       3   \nbare           NaN         NaN              6.0         1.0       4   \nbe             NaN         NaN              4.0         1.0       2   \nbee            NaN         NaN              1.0         1.0       3   \nbore           NaN         NaN              8.0         1.0       4   \n...            ...         ...              ...         ...     ...   \nnasia          NaN         NaN              NaN         NaN       5   \nnesha          NaN         NaN              NaN         NaN       5   \nquisha         NaN         NaN              NaN         NaN       6   \nre             NaN         NaN              3.0         1.0       2   \ntre            NaN         NaN              NaN         NaN       3   \n\n        same_order  ...  raw  prev  next  twitter_uni twitter_bi_prev  \\\nare            NaN  ...  bre  cant     a    114620533        0.000001   \nbare           1.0  ...  bre  cant     a       405502        0.003344   \nbe             NaN  ...  bre  cant     a    142558006        0.001847   \nbee            NaN  ...  bre  cant     a       400352        0.000472   \nbore           1.0  ...  bre  cant     a       106947        0.000094   \n...            ...  ...  ...   ...   ...          ...             ...   \nnasia          NaN  ...  bre  cant     a         2420        0.000000   \nnesha          NaN  ...  bre  cant     a         8433        0.000000   \nquisha         NaN  ...  bre  cant     a         1939        0.000000   \nre             NaN  ...  bre  cant     a      1126137        0.000144   \ntre            NaN  ...  bre  cant     a        74291        0.000054   \n\n       twitter_bi_next wiki_uni  wiki_bi_prev  wiki_bi_next  tok_id  \nare           0.023993  6939612  4.323008e-07      0.020870    1445  \nbare          0.002205    14059  0.000000e+00      0.002347    1445  \nbe            0.075743  5834246  8.570088e-07      0.055767    1445  \nbee           0.008350    18210  0.000000e+00      0.001153    1445  \nbore          0.008135    29114  0.000000e+00      0.084289    1445  \n...                ...      ...           ...           ...     ...  \nnasia         0.007438        7  0.000000e+00      0.000000    1445  \nnesha         0.007352      109  0.000000e+00      0.000000    1445  \nquisha        0.005157        3  0.000000e+00      0.000000    1445  \nre            0.007077     8852  0.000000e+00      0.001469    1445  \ntre           0.003782     2494  0.000000e+00      0.000000    1445  \n\n[194 rows x 24 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>cosine_to_orig</th>\n      <th>embeddings_rank</th>\n      <th>from_clipping</th>\n      <th>from_original_token</th>\n      <th>from_split</th>\n      <th>norms_seen</th>\n      <th>spellcheck_rank</th>\n      <th>in_lexicon</th>\n      <th>length</th>\n      <th>same_order</th>\n      <th>...</th>\n      <th>raw</th>\n      <th>prev</th>\n      <th>next</th>\n      <th>twitter_uni</th>\n      <th>twitter_bi_prev</th>\n      <th>twitter_bi_next</th>\n      <th>wiki_uni</th>\n      <th>wiki_bi_prev</th>\n      <th>wiki_bi_next</th>\n      <th>tok_id</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>are</th>\n      <td>0.074208</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>15.0</td>\n      <td>1.0</td>\n      <td>3</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>bre</td>\n      <td>cant</td>\n      <td>a</td>\n      <td>114620533</td>\n      <td>0.000001</td>\n      <td>0.023993</td>\n      <td>6939612</td>\n      <td>4.323008e-07</td>\n      <td>0.020870</td>\n      <td>1445</td>\n    </tr>\n    <tr>\n      <th>bare</th>\n      <td>0.278520</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>6.0</td>\n      <td>1.0</td>\n      <td>4</td>\n      <td>1.0</td>\n      <td>...</td>\n      <td>bre</td>\n      <td>cant</td>\n      <td>a</td>\n      <td>405502</td>\n      <td>0.003344</td>\n      <td>0.002205</td>\n      <td>14059</td>\n      <td>0.000000e+00</td>\n      <td>0.002347</td>\n      <td>1445</td>\n    </tr>\n    <tr>\n      <th>be</th>\n      <td>0.124159</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>4.0</td>\n      <td>1.0</td>\n      <td>2</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>bre</td>\n      <td>cant</td>\n      <td>a</td>\n      <td>142558006</td>\n      <td>0.001847</td>\n      <td>0.075743</td>\n      <td>5834246</td>\n      <td>8.570088e-07</td>\n      <td>0.055767</td>\n      <td>1445</td>\n    </tr>\n    <tr>\n      <th>bee</th>\n      <td>0.465715</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>3</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>bre</td>\n      <td>cant</td>\n      <td>a</td>\n      <td>400352</td>\n      <td>0.000472</td>\n      <td>0.008350</td>\n      <td>18210</td>\n      <td>0.000000e+00</td>\n      <td>0.001153</td>\n      <td>1445</td>\n    </tr>\n    <tr>\n      <th>bore</th>\n      <td>0.213253</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>8.0</td>\n      <td>1.0</td>\n      <td>4</td>\n      <td>1.0</td>\n      <td>...</td>\n      <td>bre</td>\n      <td>cant</td>\n      <td>a</td>\n      <td>106947</td>\n      <td>0.000094</td>\n      <td>0.008135</td>\n      <td>29114</td>\n      <td>0.000000e+00</td>\n      <td>0.084289</td>\n      <td>1445</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>nasia</th>\n      <td>0.859196</td>\n      <td>10.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>5</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>bre</td>\n      <td>cant</td>\n      <td>a</td>\n      <td>2420</td>\n      <td>0.000000</td>\n      <td>0.007438</td>\n      <td>7</td>\n      <td>0.000000e+00</td>\n      <td>0.000000</td>\n      <td>1445</td>\n    </tr>\n    <tr>\n      <th>nesha</th>\n      <td>0.870439</td>\n      <td>5.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>5</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>bre</td>\n      <td>cant</td>\n      <td>a</td>\n      <td>8433</td>\n      <td>0.000000</td>\n      <td>0.007352</td>\n      <td>109</td>\n      <td>0.000000e+00</td>\n      <td>0.000000</td>\n      <td>1445</td>\n    </tr>\n    <tr>\n      <th>quisha</th>\n      <td>0.863121</td>\n      <td>9.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>6</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>bre</td>\n      <td>cant</td>\n      <td>a</td>\n      <td>1939</td>\n      <td>0.000000</td>\n      <td>0.005157</td>\n      <td>3</td>\n      <td>0.000000e+00</td>\n      <td>0.000000</td>\n      <td>1445</td>\n    </tr>\n    <tr>\n      <th>re</th>\n      <td>0.369029</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>3.0</td>\n      <td>1.0</td>\n      <td>2</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>bre</td>\n      <td>cant</td>\n      <td>a</td>\n      <td>1126137</td>\n      <td>0.000144</td>\n      <td>0.007077</td>\n      <td>8852</td>\n      <td>0.000000e+00</td>\n      <td>0.001469</td>\n      <td>1445</td>\n    </tr>\n    <tr>\n      <th>tre</th>\n      <td>0.878271</td>\n      <td>4.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>3</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>bre</td>\n      <td>cant</td>\n      <td>a</td>\n      <td>74291</td>\n      <td>0.000054</td>\n      <td>0.003782</td>\n      <td>2494</td>\n      <td>0.000000e+00</td>\n      <td>0.000000</td>\n      <td>1445</td>\n    </tr>\n  </tbody>\n</table>\n<p>194 rows Ã— 24 columns</p>\n</div>"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df.set_index([\"raw_tok_index\", \"candidate\"], verify_integrity=True)\n",
    "# df.groupby([\"raw_tok_index\"]).sum()\n",
    "# df[np.isnan(df[\"norms_seen\"])].groupby(\"raw_tok_index\").sum()\n",
    "df.loc[df.tok_id == 1445]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/elijoe/Library/Mobile Documents/com~apple~CloudDocs/Documents/2/Diss/lexnorm/.venv/lib/python3.10/site-packages/gensim/models/keyedvectors.py:849: RuntimeWarning: invalid value encountered in divide\n",
      "  dists = dot(self.vectors[clip_start:clip_end], mean) / self.norms[clip_start:clip_end]\n",
      "/Users/elijoe/Library/Mobile Documents/com~apple~CloudDocs/Documents/2/Diss/lexnorm/.venv/lib/python3.10/site-packages/gensim/models/keyedvectors.py:849: RuntimeWarning: invalid value encountered in divide\n",
      "  dists = dot(self.vectors[clip_start:clip_end], mean) / self.norms[clip_start:clip_end]\n",
      "/Users/elijoe/Library/Mobile Documents/com~apple~CloudDocs/Documents/2/Diss/lexnorm/.venv/lib/python3.10/site-packages/gensim/models/keyedvectors.py:849: RuntimeWarning: invalid value encountered in divide\n",
      "  dists = dot(self.vectors[clip_start:clip_end], mean) / self.norms[clip_start:clip_end]\n",
      "/Users/elijoe/Library/Mobile Documents/com~apple~CloudDocs/Documents/2/Diss/lexnorm/.venv/lib/python3.10/site-packages/gensim/models/keyedvectors.py:849: RuntimeWarning: invalid value encountered in divide\n",
      "  dists = dot(self.vectors[clip_start:clip_end], mean) / self.norms[clip_start:clip_end]\n",
      "/Users/elijoe/Library/Mobile Documents/com~apple~CloudDocs/Documents/2/Diss/lexnorm/.venv/lib/python3.10/site-packages/gensim/models/keyedvectors.py:849: RuntimeWarning: invalid value encountered in divide\n",
      "  dists = dot(self.vectors[clip_start:clip_end], mean) / self.norms[clip_start:clip_end]\n",
      "/Users/elijoe/Library/Mobile Documents/com~apple~CloudDocs/Documents/2/Diss/lexnorm/.venv/lib/python3.10/site-packages/gensim/models/keyedvectors.py:849: RuntimeWarning: invalid value encountered in divide\n",
      "  dists = dot(self.vectors[clip_start:clip_end], mean) / self.norms[clip_start:clip_end]\n",
      "/Users/elijoe/Library/Mobile Documents/com~apple~CloudDocs/Documents/2/Diss/lexnorm/.venv/lib/python3.10/site-packages/gensim/models/keyedvectors.py:849: RuntimeWarning: invalid value encountered in divide\n",
      "  dists = dot(self.vectors[clip_start:clip_end], mean) / self.norms[clip_start:clip_end]\n",
      "/Users/elijoe/Library/Mobile Documents/com~apple~CloudDocs/Documents/2/Diss/lexnorm/.venv/lib/python3.10/site-packages/gensim/models/keyedvectors.py:849: RuntimeWarning: invalid value encountered in divide\n",
      "  dists = dot(self.vectors[clip_start:clip_end], mean) / self.norms[clip_start:clip_end]\n",
      "/Users/elijoe/Library/Mobile Documents/com~apple~CloudDocs/Documents/2/Diss/lexnorm/.venv/lib/python3.10/site-packages/gensim/models/keyedvectors.py:849: RuntimeWarning: invalid value encountered in divide\n",
      "  dists = dot(self.vectors[clip_start:clip_end], mean) / self.norms[clip_start:clip_end]\n",
      "/Users/elijoe/Library/Mobile Documents/com~apple~CloudDocs/Documents/2/Diss/lexnorm/.venv/lib/python3.10/site-packages/gensim/models/keyedvectors.py:849: RuntimeWarning: invalid value encountered in divide\n",
      "  dists = dot(self.vectors[clip_start:clip_end], mean) / self.norms[clip_start:clip_end]\n",
      "/Users/elijoe/Library/Mobile Documents/com~apple~CloudDocs/Documents/2/Diss/lexnorm/.venv/lib/python3.10/site-packages/gensim/models/keyedvectors.py:849: RuntimeWarning: invalid value encountered in divide\n",
      "  dists = dot(self.vectors[clip_start:clip_end], mean) / self.norms[clip_start:clip_end]\n",
      "/Users/elijoe/Library/Mobile Documents/com~apple~CloudDocs/Documents/2/Diss/lexnorm/.venv/lib/python3.10/site-packages/gensim/models/keyedvectors.py:849: RuntimeWarning: invalid value encountered in divide\n",
      "  dists = dot(self.vectors[clip_start:clip_end], mean) / self.norms[clip_start:clip_end]\n",
      "/Users/elijoe/Library/Mobile Documents/com~apple~CloudDocs/Documents/2/Diss/lexnorm/.venv/lib/python3.10/site-packages/gensim/models/keyedvectors.py:849: RuntimeWarning: invalid value encountered in divide\n",
      "  dists = dot(self.vectors[clip_start:clip_end], mean) / self.norms[clip_start:clip_end]\n",
      "/Users/elijoe/Library/Mobile Documents/com~apple~CloudDocs/Documents/2/Diss/lexnorm/.venv/lib/python3.10/site-packages/gensim/models/keyedvectors.py:849: RuntimeWarning: invalid value encountered in divide\n",
      "  dists = dot(self.vectors[clip_start:clip_end], mean) / self.norms[clip_start:clip_end]\n",
      "/Users/elijoe/Library/Mobile Documents/com~apple~CloudDocs/Documents/2/Diss/lexnorm/.venv/lib/python3.10/site-packages/gensim/models/keyedvectors.py:849: RuntimeWarning: invalid value encountered in divide\n",
      "  dists = dot(self.vectors[clip_start:clip_end], mean) / self.norms[clip_start:clip_end]\n",
      "/Users/elijoe/Library/Mobile Documents/com~apple~CloudDocs/Documents/2/Diss/lexnorm/.venv/lib/python3.10/site-packages/gensim/models/keyedvectors.py:849: RuntimeWarning: invalid value encountered in divide\n",
      "  dists = dot(self.vectors[clip_start:clip_end], mean) / self.norms[clip_start:clip_end]\n",
      "/Users/elijoe/Library/Mobile Documents/com~apple~CloudDocs/Documents/2/Diss/lexnorm/.venv/lib/python3.10/site-packages/gensim/models/keyedvectors.py:849: RuntimeWarning: invalid value encountered in divide\n",
      "  dists = dot(self.vectors[clip_start:clip_end], mean) / self.norms[clip_start:clip_end]\n",
      "/Users/elijoe/Library/Mobile Documents/com~apple~CloudDocs/Documents/2/Diss/lexnorm/.venv/lib/python3.10/site-packages/gensim/models/keyedvectors.py:849: RuntimeWarning: invalid value encountered in divide\n",
      "  dists = dot(self.vectors[clip_start:clip_end], mean) / self.norms[clip_start:clip_end]\n"
     ]
    },
    {
     "data": {
      "text/plain": "        cosine_to_orig  embeddings_rank  from_clipping  from_original_token  \\\na             0.406318              NaN            NaN                  NaN   \naah           0.734173              NaN            NaN                  NaN   \nagu           0.764286              1.0            NaN                  NaN   \nah            1.000000              NaN            1.0                  1.0   \naha           0.422756              NaN            1.0                  NaN   \n...                ...              ...            ...                  ...   \nrawer         0.301921              NaN            NaN                  NaN   \nrawr          1.000000              NaN            NaN                  1.0   \nrawrr         0.781826              0.0            NaN                  NaN   \nrawrrr        0.772067              1.0            NaN                  NaN   \nwrap          0.140178              NaN            NaN                  NaN   \n\n        from_split  norms_seen  spellcheck_rank  in_lexicon  length  \\\na              NaN         NaN              3.0           1       1   \naah            NaN         NaN              5.0           0       3   \nagu            NaN         NaN              NaN           0       3   \nah             NaN         8.0              NaN           1       2   \naha            NaN         NaN              6.0           1       3   \n...            ...         ...              ...         ...     ...   \nrawer          NaN         NaN              1.0           1       5   \nrawr           NaN         2.0              NaN           0       4   \nrawrr          NaN         NaN              NaN           0       5   \nrawrrr         NaN         NaN              NaN           0       6   \nwrap           NaN         NaN              5.0           1       4   \n\n        same_order  orig_norms_seen  orig_in_lexicon  orig_same_order  \\\na                0              8.0              1.0              1.0   \naah              1              8.0              1.0              1.0   \nagu              0              8.0              1.0              1.0   \nah               1              8.0              1.0              1.0   \naha              1              8.0              1.0              1.0   \n...            ...              ...              ...              ...   \nrawer            1              2.0              0.0              1.0   \nrawr             1              2.0              0.0              1.0   \nrawrr            1              2.0              0.0              1.0   \nrawrrr           1              2.0              0.0              1.0   \nwrap             0              2.0              0.0              1.0   \n\n        orig_length  \na               2.0  \naah             2.0  \nagu             2.0  \nah              2.0  \naha             2.0  \n...             ...  \nrawer           4.0  \nrawr            4.0  \nrawrr           4.0  \nrawrrr          4.0  \nwrap            4.0  \n\n[1286 rows x 14 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>cosine_to_orig</th>\n      <th>embeddings_rank</th>\n      <th>from_clipping</th>\n      <th>from_original_token</th>\n      <th>from_split</th>\n      <th>norms_seen</th>\n      <th>spellcheck_rank</th>\n      <th>in_lexicon</th>\n      <th>length</th>\n      <th>same_order</th>\n      <th>orig_norms_seen</th>\n      <th>orig_in_lexicon</th>\n      <th>orig_same_order</th>\n      <th>orig_length</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>a</th>\n      <td>0.406318</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>3.0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>8.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>2.0</td>\n    </tr>\n    <tr>\n      <th>aah</th>\n      <td>0.734173</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>5.0</td>\n      <td>0</td>\n      <td>3</td>\n      <td>1</td>\n      <td>8.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>2.0</td>\n    </tr>\n    <tr>\n      <th>agu</th>\n      <td>0.764286</td>\n      <td>1.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>3</td>\n      <td>0</td>\n      <td>8.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>2.0</td>\n    </tr>\n    <tr>\n      <th>ah</th>\n      <td>1.000000</td>\n      <td>NaN</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>NaN</td>\n      <td>8.0</td>\n      <td>NaN</td>\n      <td>1</td>\n      <td>2</td>\n      <td>1</td>\n      <td>8.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>2.0</td>\n    </tr>\n    <tr>\n      <th>aha</th>\n      <td>0.422756</td>\n      <td>NaN</td>\n      <td>1.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>6.0</td>\n      <td>1</td>\n      <td>3</td>\n      <td>1</td>\n      <td>8.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>2.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>rawer</th>\n      <td>0.301921</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1.0</td>\n      <td>1</td>\n      <td>5</td>\n      <td>1</td>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>4.0</td>\n    </tr>\n    <tr>\n      <th>rawr</th>\n      <td>1.000000</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1.0</td>\n      <td>NaN</td>\n      <td>2.0</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>4</td>\n      <td>1</td>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>4.0</td>\n    </tr>\n    <tr>\n      <th>rawrr</th>\n      <td>0.781826</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>5</td>\n      <td>1</td>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>4.0</td>\n    </tr>\n    <tr>\n      <th>rawrrr</th>\n      <td>0.772067</td>\n      <td>1.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>6</td>\n      <td>1</td>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>4.0</td>\n    </tr>\n    <tr>\n      <th>wrap</th>\n      <td>0.140178</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>5.0</td>\n      <td>1</td>\n      <td>4</td>\n      <td>0</td>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>4.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>1286 rows Ã— 14 columns</p>\n</div>"
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candidate_generation.candidates_from_tweets(raw[:2], w2v_vectors, norm_dictionary, lex, dictionary)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join(DATA_PATH, \"../hpc/candidates.txt\"), index_col=0)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "outputs": [],
   "source": [
    "# get all tokens where correct normalisation not produced by candidate generation\n",
    "filtered = df.groupby(\"raw_tok_index\").filter(lambda x: x.sum()[\"correct\"] == 0)\n",
    "ungenerated = filtered.loc[filtered[\"from_original_token\"] == 1.0][\"gold\"]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "outputs": [],
   "source": [
    "raw, _ = loadNormData(os.path.join(DATA_PATH, \"raw/dev.norm\"))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "outputs": [
    {
     "data": {
      "text/plain": "6876"
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count = 0\n",
    "for raw_tweet, norm_tweet in zip(raw, norm):\n",
    "    for raw_tok, norm_tok in zip(raw_tweet, norm_tweet):\n",
    "        if is_eligible(raw_tok):\n",
    "            count += 1\n",
    "count\n",
    "# as expected - perhaps can be used to test candidate_generation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "outputs": [
    {
     "data": {
      "text/plain": "[(('v', 'very'), 2),\n (('hapi', 'happy'), 2),\n (('witchu', 'with you'), 2),\n (('yessss', 'yes'), 1),\n (('bestie', 'best friend'), 1),\n (('niggra', 'nigger'), 1),\n (('wada', 'water'), 1),\n (('chu', 'you'), 1),\n (('nows', 'now is'), 1),\n (('nuh', 'know'), 1),\n (('ntn', 'nothing'), 1),\n (('mnl', 'my new love'), 1),\n (('za', 'that'), 1),\n (('skepta', 'sunglasses'), 1),\n (('whatdoiwear', 'what do i wear'), 1),\n (('brutha', 'brother'), 1),\n (('yah', 'you'), 1),\n (('nuff', 'enough'), 1),\n (('shizz', 'shit'), 1),\n (('nuffin', 'nothing'), 1),\n (('diss', 'this'), 1),\n (('vas', 'was'), 1),\n (('redsox', 'red sox'), 1),\n (('nem', 'they'), 1),\n (('fkn', 'fucking'), 1),\n (('sim', 'seems'), 1),\n (('hbu', 'how about you'), 1),\n (('goddamit', 'god damn it'), 1),\n (('satnite', 'saturday night'), 1),\n (('dese', 'these'), 1),\n (('summn', 'something'), 1),\n (('cums', 'comes'), 1),\n (('dnt', \"doesn't\"), 1),\n (('getcha', 'get you'), 1),\n (('trynna', 'trying to'), 1),\n (('ya', 'your'), 1),\n (('lottle', 'lot'), 1),\n (('happylgovo', 'happy'), 1),\n (('gal', 'guy'), 1),\n (('fuccin', 'fucking'), 1),\n (('cuh', 'see you'), 1),\n (('datpiff', 'the piff'), 1),\n (('diy', 'do it yourself'), 1),\n (('da', 'that'), 1),\n (('shat', 'shit'), 1),\n (('slp', 'sleep'), 1),\n (('dia', 'their'), 1),\n (('devs', 'developer'), 1),\n (('yea', 'you'), 1),\n (('cannnot', \"can't\"), 1),\n (('ull', 'you will'), 1),\n (('icant', \"i can't\"), 1),\n (('brotha', 'brother'), 1),\n (('poooollll', 'pool'), 1),\n (('sup', \"what's up\"), 1),\n (('wassup', \"what's up\"), 1),\n (('pre', 'preorder'), 1),\n (('order', nan), 1)]"
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "Counter(zip(ungenerated.index, ungenerated)).most_common()\n",
    "# very few ungenerated correct candidates! So candidate generation module perhaps alright."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "outputs": [
    {
     "data": {
      "text/plain": "['broths',\n 'broth',\n 'broth a',\n 'broth-a',\n 'brouhaha',\n 'Botha',\n 'Hasbro',\n 'troth',\n 'breath',\n 'brethren']"
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x for x in dictionary.suggest(\"brotha\")]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
