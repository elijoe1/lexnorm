{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pd as pd\n",
    "\n",
    "from lexnorm.data import lexicon\n",
    "from lexnorm.data import norm_dict\n",
    "from lexnorm.data.normEval import loadNormData\n",
    "from lexnorm.definitions import DATA_PATH\n",
    "from lexnorm.models.normalise import load_candidates\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "raw, norm = loadNormData(os.path.join(DATA_PATH, \"processed/combined.txt\"))\n",
    "norm_dictionary = norm_dict.construct(raw, norm)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# unigrams = load_candidates(os.path.join(DATA_PATH, \"processed/wiki_unigram_df\"))\n",
    "# cands = load_candidates(os.path.join(DATA_PATH, \"hpc/fixed_dev.norm\"))\n",
    "# cands.join(unigrams)\n",
    "\n",
    "with open(os.path.join(DATA_PATH, \"interim/wiki_ngrams.1\")) as f:\n",
    "    for chunk in pd.read_csv(\n",
    "        f,\n",
    "        sep=\"\\t\",\n",
    "        index_col=0,\n",
    "        header=0,\n",
    "        names=[\"wiki_bigrams\"],\n",
    "        na_values=\"\",\n",
    "        keep_default_na=False,\n",
    "    ):\n",
    "        print(chunk)\n",
    "\n",
    "# with open(os.path.join(DATA_PATH, \"processed/feature_lexicon.txt\"), \"rb\") as f:\n",
    "#     lex = pickle.load(f)\n",
    "# \"lol\" in lex"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "outputs": [],
   "source": [
    "from lexnorm.data import word_ngrams\n",
    "import importlib\n",
    "importlib.reload(word_ngrams)\n",
    "\n",
    "def add_ngrams_chunks(dataframe, ngram_file, col_name, verbose=False):\n",
    "    dataframe[col_name] = np.nan\n",
    "    for chunk in pd.read_csv(\n",
    "        os.path.join(DATA_PATH, f\"processed/{ngram_file}.ngr\"),\n",
    "        index_col=0,\n",
    "        na_values=\"\",\n",
    "        header=0,\n",
    "        names=[col_name],\n",
    "        dtype={col_name: int},\n",
    "        keep_default_na=False,\n",
    "        chunksize=10**6,\n",
    "    ):\n",
    "        dataframe[col_name] = dataframe[col_name].fillna(chunk[col_name])\n",
    "        if verbose:\n",
    "            print(candidates[col_name].isna().sum())\n",
    "    return dataframe"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "outputs": [
    {
     "data": {
      "text/plain": "        cosine_to_orig  frac_norms_seen  from_clipping  from_embeddings  \\\n0             0.377180              NaN            NaN              NaN   \n1             0.255824              NaN            NaN              NaN   \n2             0.261057              NaN            NaN              NaN   \n3             0.245124              NaN            NaN              NaN   \n4             0.299136              NaN            NaN              NaN   \n...                ...              ...            ...              ...   \n310324        0.376790              NaN            NaN              NaN   \n310325        0.296913              NaN            NaN              NaN   \n310326        0.230619              NaN            NaN              NaN   \n310327        0.285978              NaN            NaN              NaN   \n310328        0.235724              NaN            NaN              NaN   \n\n        from_original_token  from_spellcheck  from_split  norms_seen  \\\n0                       NaN              1.0         NaN         NaN   \n1                       NaN              1.0         NaN         NaN   \n2                       NaN              1.0         NaN         NaN   \n3                       NaN              1.0         NaN         NaN   \n4                       NaN              1.0         NaN         NaN   \n...                     ...              ...         ...         ...   \n310324                  NaN              1.0         NaN         NaN   \n310325                  NaN              1.0         NaN         NaN   \n310326                  NaN              1.0         NaN         NaN   \n310327                  NaN              1.0         NaN         NaN   \n310328                  NaN              1.0         NaN         NaN   \n\n        spellcheck_score  length  ...  orig_length  orig_in_feature_lex  raw  \\\n0                      8       2  ...          3.0                    1  can   \n1                     11       2  ...          3.0                    1  can   \n2                      8       3  ...          3.0                    1  can   \n3                     14       4  ...          3.0                    1  can   \n4                      8       3  ...          3.0                    1  can   \n...                  ...     ...  ...          ...                  ...  ...   \n310324                11       3  ...          3.0                    1  lol   \n310325                 9       3  ...          3.0                    1  lol   \n310326                 9       3  ...          3.0                    1  lol   \n310327                 9       3  ...          3.0                    1  lol   \n310328                 9       3  ...          3.0                    1  lol   \n\n              prev  next  process tweet tok   wiki_uni  wiki_uni_orig  \n0       #askcamila   you       35     0   2  9348892.0      1938303.0  \n1       #askcamila   you       35     0   2    13145.0      1938303.0  \n2       #askcamila   you       35     0   2    13063.0      1938303.0  \n3       #askcamila   you       35     0   2     7646.0      1938303.0  \n4       #askcamila   you       35     0   2    10015.0      1938303.0  \n...            ...   ...      ...   ...  ..        ...            ...  \n310324       thots     \"       32     9  25   272700.0          762.0  \n310325       thots     \"       32     9  25     1621.0          762.0  \n310326       thots     \"       32     9  25     3721.0          762.0  \n310327       thots     \"       32     9  25     9104.0          762.0  \n310328       thots     \"       32     9  25     2936.0          762.0  \n\n[310329 rows x 24 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>cosine_to_orig</th>\n      <th>frac_norms_seen</th>\n      <th>from_clipping</th>\n      <th>from_embeddings</th>\n      <th>from_original_token</th>\n      <th>from_spellcheck</th>\n      <th>from_split</th>\n      <th>norms_seen</th>\n      <th>spellcheck_score</th>\n      <th>length</th>\n      <th>...</th>\n      <th>orig_length</th>\n      <th>orig_in_feature_lex</th>\n      <th>raw</th>\n      <th>prev</th>\n      <th>next</th>\n      <th>process</th>\n      <th>tweet</th>\n      <th>tok</th>\n      <th>wiki_uni</th>\n      <th>wiki_uni_orig</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.377180</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>8</td>\n      <td>2</td>\n      <td>...</td>\n      <td>3.0</td>\n      <td>1</td>\n      <td>can</td>\n      <td>#askcamila</td>\n      <td>you</td>\n      <td>35</td>\n      <td>0</td>\n      <td>2</td>\n      <td>9348892.0</td>\n      <td>1938303.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.255824</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>11</td>\n      <td>2</td>\n      <td>...</td>\n      <td>3.0</td>\n      <td>1</td>\n      <td>can</td>\n      <td>#askcamila</td>\n      <td>you</td>\n      <td>35</td>\n      <td>0</td>\n      <td>2</td>\n      <td>13145.0</td>\n      <td>1938303.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.261057</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>8</td>\n      <td>3</td>\n      <td>...</td>\n      <td>3.0</td>\n      <td>1</td>\n      <td>can</td>\n      <td>#askcamila</td>\n      <td>you</td>\n      <td>35</td>\n      <td>0</td>\n      <td>2</td>\n      <td>13063.0</td>\n      <td>1938303.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.245124</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>14</td>\n      <td>4</td>\n      <td>...</td>\n      <td>3.0</td>\n      <td>1</td>\n      <td>can</td>\n      <td>#askcamila</td>\n      <td>you</td>\n      <td>35</td>\n      <td>0</td>\n      <td>2</td>\n      <td>7646.0</td>\n      <td>1938303.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.299136</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>8</td>\n      <td>3</td>\n      <td>...</td>\n      <td>3.0</td>\n      <td>1</td>\n      <td>can</td>\n      <td>#askcamila</td>\n      <td>you</td>\n      <td>35</td>\n      <td>0</td>\n      <td>2</td>\n      <td>10015.0</td>\n      <td>1938303.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>310324</th>\n      <td>0.376790</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>11</td>\n      <td>3</td>\n      <td>...</td>\n      <td>3.0</td>\n      <td>1</td>\n      <td>lol</td>\n      <td>thots</td>\n      <td>\"</td>\n      <td>32</td>\n      <td>9</td>\n      <td>25</td>\n      <td>272700.0</td>\n      <td>762.0</td>\n    </tr>\n    <tr>\n      <th>310325</th>\n      <td>0.296913</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>9</td>\n      <td>3</td>\n      <td>...</td>\n      <td>3.0</td>\n      <td>1</td>\n      <td>lol</td>\n      <td>thots</td>\n      <td>\"</td>\n      <td>32</td>\n      <td>9</td>\n      <td>25</td>\n      <td>1621.0</td>\n      <td>762.0</td>\n    </tr>\n    <tr>\n      <th>310326</th>\n      <td>0.230619</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>9</td>\n      <td>3</td>\n      <td>...</td>\n      <td>3.0</td>\n      <td>1</td>\n      <td>lol</td>\n      <td>thots</td>\n      <td>\"</td>\n      <td>32</td>\n      <td>9</td>\n      <td>25</td>\n      <td>3721.0</td>\n      <td>762.0</td>\n    </tr>\n    <tr>\n      <th>310327</th>\n      <td>0.285978</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>9</td>\n      <td>3</td>\n      <td>...</td>\n      <td>3.0</td>\n      <td>1</td>\n      <td>lol</td>\n      <td>thots</td>\n      <td>\"</td>\n      <td>32</td>\n      <td>9</td>\n      <td>25</td>\n      <td>9104.0</td>\n      <td>762.0</td>\n    </tr>\n    <tr>\n      <th>310328</th>\n      <td>0.235724</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>9</td>\n      <td>3</td>\n      <td>...</td>\n      <td>3.0</td>\n      <td>1</td>\n      <td>lol</td>\n      <td>thots</td>\n      <td>\"</td>\n      <td>32</td>\n      <td>9</td>\n      <td>25</td>\n      <td>2936.0</td>\n      <td>762.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>310329 rows × 24 columns</p>\n</div>"
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candidates.merge(candidates.loc[candidates.from_original_token == 1][[\"wiki_uni\", \"process\", \"tweet\", \"tok\"]], \"left\", on=[\"process\", \"tweet\", \"tok\"], suffixes=(None, \"_orig\"))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "outputs": [
    {
     "data": {
      "text/plain": "                       level_0 index  cosine_to_orig  frac_norms_seen  \\\n#askcamila you  #askcamila can    an        0.377180              NaN   \n#askcamila you  #askcamila can    ca        0.255824              NaN   \n#askcamila you  #askcamila can   cab        0.261057              NaN   \n#askcamila you  #askcamila can  cain        0.245124              NaN   \n#askcamila you  #askcamila can   cam        0.299136              NaN   \n...                        ...   ...             ...              ...   \nthots \"              thots lol   low        0.376790              NaN   \nthots \"              thots lol   mol        0.296913              NaN   \nthots \"              thots lol   pol        0.230619              NaN   \nthots \"              thots lol   sol        0.285978              NaN   \nthots \"              thots lol   vol        0.235724              NaN   \n\n                from_clipping  from_embeddings  from_original_token  \\\n#askcamila you            NaN              NaN                  NaN   \n#askcamila you            NaN              NaN                  NaN   \n#askcamila you            NaN              NaN                  NaN   \n#askcamila you            NaN              NaN                  NaN   \n#askcamila you            NaN              NaN                  NaN   \n...                       ...              ...                  ...   \nthots \"                   NaN              NaN                  NaN   \nthots \"                   NaN              NaN                  NaN   \nthots \"                   NaN              NaN                  NaN   \nthots \"                   NaN              NaN                  NaN   \nthots \"                   NaN              NaN                  NaN   \n\n                from_spellcheck  from_split  norms_seen  ...  \\\n#askcamila you              1.0         NaN         NaN  ...   \n#askcamila you              1.0         NaN         NaN  ...   \n#askcamila you              1.0         NaN         NaN  ...   \n#askcamila you              1.0         NaN         NaN  ...   \n#askcamila you              1.0         NaN         NaN  ...   \n...                         ...         ...         ...  ...   \nthots \"                     1.0         NaN         NaN  ...   \nthots \"                     1.0         NaN         NaN  ...   \nthots \"                     1.0         NaN         NaN  ...   \nthots \"                     1.0         NaN         NaN  ...   \nthots \"                     1.0         NaN         NaN  ...   \n\n                orig_frac_norms_seen  orig_length  orig_in_feature_lex  raw  \\\n#askcamila you                   1.0          3.0                    1  can   \n#askcamila you                   1.0          3.0                    1  can   \n#askcamila you                   1.0          3.0                    1  can   \n#askcamila you                   1.0          3.0                    1  can   \n#askcamila you                   1.0          3.0                    1  can   \n...                              ...          ...                  ...  ...   \nthots \"                          1.0          3.0                    1  lol   \nthots \"                          1.0          3.0                    1  lol   \nthots \"                          1.0          3.0                    1  lol   \nthots \"                          1.0          3.0                    1  lol   \nthots \"                          1.0          3.0                    1  lol   \n\n                      prev  next  process  tweet tok bigram_freqs_twit  \n#askcamila you  #askcamila   you       35      0   2              99.0  \n#askcamila you  #askcamila   you       35      0   2              99.0  \n#askcamila you  #askcamila   you       35      0   2              99.0  \n#askcamila you  #askcamila   you       35      0   2              99.0  \n#askcamila you  #askcamila   you       35      0   2              99.0  \n...                    ...   ...      ...    ...  ..               ...  \nthots \"              thots     \"       32      9  25             775.0  \nthots \"              thots     \"       32      9  25             775.0  \nthots \"              thots     \"       32      9  25             775.0  \nthots \"              thots     \"       32      9  25             775.0  \nthots \"              thots     \"       32      9  25             775.0  \n\n[310329 rows x 25 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>level_0</th>\n      <th>index</th>\n      <th>cosine_to_orig</th>\n      <th>frac_norms_seen</th>\n      <th>from_clipping</th>\n      <th>from_embeddings</th>\n      <th>from_original_token</th>\n      <th>from_spellcheck</th>\n      <th>from_split</th>\n      <th>norms_seen</th>\n      <th>...</th>\n      <th>orig_frac_norms_seen</th>\n      <th>orig_length</th>\n      <th>orig_in_feature_lex</th>\n      <th>raw</th>\n      <th>prev</th>\n      <th>next</th>\n      <th>process</th>\n      <th>tweet</th>\n      <th>tok</th>\n      <th>bigram_freqs_twit</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>#askcamila you</th>\n      <td>#askcamila can</td>\n      <td>an</td>\n      <td>0.377180</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>1.0</td>\n      <td>3.0</td>\n      <td>1</td>\n      <td>can</td>\n      <td>#askcamila</td>\n      <td>you</td>\n      <td>35</td>\n      <td>0</td>\n      <td>2</td>\n      <td>99.0</td>\n    </tr>\n    <tr>\n      <th>#askcamila you</th>\n      <td>#askcamila can</td>\n      <td>ca</td>\n      <td>0.255824</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>1.0</td>\n      <td>3.0</td>\n      <td>1</td>\n      <td>can</td>\n      <td>#askcamila</td>\n      <td>you</td>\n      <td>35</td>\n      <td>0</td>\n      <td>2</td>\n      <td>99.0</td>\n    </tr>\n    <tr>\n      <th>#askcamila you</th>\n      <td>#askcamila can</td>\n      <td>cab</td>\n      <td>0.261057</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>1.0</td>\n      <td>3.0</td>\n      <td>1</td>\n      <td>can</td>\n      <td>#askcamila</td>\n      <td>you</td>\n      <td>35</td>\n      <td>0</td>\n      <td>2</td>\n      <td>99.0</td>\n    </tr>\n    <tr>\n      <th>#askcamila you</th>\n      <td>#askcamila can</td>\n      <td>cain</td>\n      <td>0.245124</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>1.0</td>\n      <td>3.0</td>\n      <td>1</td>\n      <td>can</td>\n      <td>#askcamila</td>\n      <td>you</td>\n      <td>35</td>\n      <td>0</td>\n      <td>2</td>\n      <td>99.0</td>\n    </tr>\n    <tr>\n      <th>#askcamila you</th>\n      <td>#askcamila can</td>\n      <td>cam</td>\n      <td>0.299136</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>1.0</td>\n      <td>3.0</td>\n      <td>1</td>\n      <td>can</td>\n      <td>#askcamila</td>\n      <td>you</td>\n      <td>35</td>\n      <td>0</td>\n      <td>2</td>\n      <td>99.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>thots \"</th>\n      <td>thots lol</td>\n      <td>low</td>\n      <td>0.376790</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>1.0</td>\n      <td>3.0</td>\n      <td>1</td>\n      <td>lol</td>\n      <td>thots</td>\n      <td>\"</td>\n      <td>32</td>\n      <td>9</td>\n      <td>25</td>\n      <td>775.0</td>\n    </tr>\n    <tr>\n      <th>thots \"</th>\n      <td>thots lol</td>\n      <td>mol</td>\n      <td>0.296913</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>1.0</td>\n      <td>3.0</td>\n      <td>1</td>\n      <td>lol</td>\n      <td>thots</td>\n      <td>\"</td>\n      <td>32</td>\n      <td>9</td>\n      <td>25</td>\n      <td>775.0</td>\n    </tr>\n    <tr>\n      <th>thots \"</th>\n      <td>thots lol</td>\n      <td>pol</td>\n      <td>0.230619</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>1.0</td>\n      <td>3.0</td>\n      <td>1</td>\n      <td>lol</td>\n      <td>thots</td>\n      <td>\"</td>\n      <td>32</td>\n      <td>9</td>\n      <td>25</td>\n      <td>775.0</td>\n    </tr>\n    <tr>\n      <th>thots \"</th>\n      <td>thots lol</td>\n      <td>sol</td>\n      <td>0.285978</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>1.0</td>\n      <td>3.0</td>\n      <td>1</td>\n      <td>lol</td>\n      <td>thots</td>\n      <td>\"</td>\n      <td>32</td>\n      <td>9</td>\n      <td>25</td>\n      <td>775.0</td>\n    </tr>\n    <tr>\n      <th>thots \"</th>\n      <td>thots lol</td>\n      <td>vol</td>\n      <td>0.235724</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>1.0</td>\n      <td>3.0</td>\n      <td>1</td>\n      <td>lol</td>\n      <td>thots</td>\n      <td>\"</td>\n      <td>32</td>\n      <td>9</td>\n      <td>25</td>\n      <td>775.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>310329 rows × 25 columns</p>\n</div>"
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candidates.reset_index().set_index(candidates.prev + \" \" + candidates.next)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def original_token(tok):\n",
    "    # MONOISE\n",
    "    # needed if detect step is skipped, as all tokens will be replaced by one from the list of candidates\n",
    "    return {tok}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "from lexnorm.data import word2vec\n",
    "vectors = word2vec.get_vectors(raw)\n",
    "# load embeddings from van der goot. Used params -size 100 -window 5 -cbow 0 -binary 1 -threads 45\n",
    "# there are 400 length embeddings which supposedly give slight performance improvement, but quite slow\n",
    "# unicode incompatibilities present so must ignore when loading"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "outputs": [],
   "source": [
    "from lexnorm.generate_extract.filtering import is_eligible\n",
    "\n",
    "def word_embeddings(tok, vectors, threshold=0):\n",
    "    # uni, bigram freqs?\n",
    "    # implement word2vec with keras. Use newer embeddings. Experiment with different no. of candidates generated. Could even create twitter embeddings myself? Could clean up as VDG did before creating train embeddings. Cosine similarity threshold?\n",
    "    # MONOISE\n",
    "    # can use twitter embeddings from van der Goot - based on distributional hypothesis to find tokens with similar semantics\n",
    "    # could use cosine similarity as a feature for selection? Using here to get most similar candidates.\n",
    "    # ISSUE: antonyms also often present in same contexts.\n",
    "    candidates = set()\n",
    "    if tok in vectors:\n",
    "        candidates = set(vectors.similar_by_vector(tok))\n",
    "    return {c[0].lower() for c in candidates if is_eligible(c[0]) and c[1] >= threshold}\n",
    "\n",
    "# pretty much no tokens in train set not in twitter embeddings from vdg but can't assume this is the case\n",
    "# possible that tok will not be in vectors\n",
    "vectors.similar_by_vector(\"yo\")\n",
    "# pretty terrible performance for train_vectors - to be expected as such a low amount of data could literally just use external set as contains almost every word anyway and will be much more accurate"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "def lookup(tok, dictionary):\n",
    "    # external norm dicts?\n",
    "    # MONOISE\n",
    "    # lookup in list of all replacement pairs found in the training data (and external sources?)\n",
    "    # all norm tokens with raw token tok are included as candidates\n",
    "    return [(k, v) for k, v in dictionary.get(tok, {}).items()]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "def clipping(tok, lex):\n",
    "    # MONOISE\n",
    "    # all words in lexicon that have tok as a prefix (capturing abbreviation). May only consider for tok length above 2?\n",
    "    candidates = set()\n",
    "    if len(tok) < 2:\n",
    "        return set()\n",
    "    # length threshold? prune generated (only some degree of clipping allowed w.r.t. edit distance)?\n",
    "    return [t for t in lex if t.startswith(tok)]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "# number of candidates on average generated by each module and with all modules\n",
    "# which modules contribute the most / most unique correct candidates\n",
    "# for modules and whole, percentage of correct vs incorrect candidates"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "def split(tok, lex):\n",
    "    # MONOISE\n",
    "    # hypothesis splits on (every/some) position and check if both words are in lexicon. May only consider of tok length above 3?\n",
    "    candidates = set()\n",
    "    if len(tok) < 3:\n",
    "        return set()\n",
    "    for pos in range(1, len(tok)):\n",
    "        left = tok[:pos]\n",
    "        right = tok[pos:]\n",
    "        if left in lex and right in lex:\n",
    "            candidates.add(\" \".join([left, right]))\n",
    "    # recursive candidate generation on each left and right? Probably not... More than one split? Probably not either...\n",
    "    # length threshold?\n",
    "    return candidates"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "from spylls.hunspell import Dictionary\n",
    "\n",
    "def spellcheck(tok):\n",
    "    # no control over this - can I change in source code? Try and load in custom lexicon.\n",
    "    dictionary = Dictionary.from_files('en_US')\n",
    "    return {c.lower() for c in dictionary.suggest(tok)}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "# contextual features?\n",
    "# def generate_candidates(tweet):\n",
    "#     for token in tweet:\n",
    "#         ..."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "def generate_candidates(tok):\n",
    "    candidates = set()\n",
    "    candidates = candidates.union(original_token(tok))\n",
    "    candidates = candidates.union(word_embeddings(tok, w2v_vectors))\n",
    "    candidates = candidates.union(spellcheck(tok))\n",
    "    # obviously lookup on the train set will always give the correct answer!\n",
    "    # candidates = candidates.union(lookup(tok, norm_dictionary))\n",
    "    candidates = candidates.union(clipping(tok, lex))\n",
    "    candidates = candidates.union(split(tok, lex))\n",
    "    return candidates"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "from lexnorm.evaluation import condition_normalisation\n",
    "from lexnorm.data import normEval\n",
    "import os\n",
    "from lexnorm.definitions import DATA_PATH\n",
    "from lexnorm.data import norm_dict\n",
    "from lexnorm.models.filtering import is_eligible\n",
    "from random import sample\n",
    "\n",
    "norm_dictionary = norm_dict.construct(os.path.join(DATA_PATH, \"interim/train.txt\"))\n",
    "# keys = sample(list(norm_dictionary.keys()), 100)\n",
    "# assert condition_normalisation.contingency(\n",
    "#     raw, norm, lambda x: x[0] == \"a\", True\n",
    "# ) == condition_normalisation.contingency_from_dict(\n",
    "#     norm_dictionary, lambda x: x[0][0] == \"a\"\n",
    "# )\n",
    "a, b, c, d = condition_normalisation.contingency_from_dict(\n",
    "    norm_dictionary, lambda x: not x[1] in generate_candidates(x[0])\n",
    ")\n",
    "\n",
    "# print(sum(a.values()) + sum(b.values()) + sum(c.values()) + sum(d.values()))\n",
    "\n",
    "# a2, b2, c2, d2 = condition_normalisation.contingency(raw, norm, lambda x: x[0] == \"a\", True)\n",
    "\n",
    "# print(sum(a.values()) + sum(b.values()) + sum(c.values()) + sum(d.values()))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "# merge module that checks some tokens ahead of current token (perhaps only one)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [],
   "source": [
    "import lexnorm.models.candidate_generation as candidate_generation\n",
    "import importlib\n",
    "import numpy as np\n",
    "from spylls.hunspell import Dictionary\n",
    "importlib.reload(candidate_generation)\n",
    "dictionary = Dictionary.from_files(\"en_US\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "train_data = pd.DataFrame()\n",
    "candidates = candidate_generation.candidates_from_token(\"bruh\", w2v_vectors, norm_dictionary, lex, dictionary)\n",
    "# candidates.cosine_to_orig = candidates.index.map(lambda x: w2v_vectors.similarity(x, \"lol\") if x in w2v_vectors else 0)\n",
    "# w2v_vectors.similarity(candidates.index, \"lol\") if indexes in w2v_vectors else 0\n",
    "# candidates.index.to_series()\n",
    "candidates"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "d1 = pd.DataFrame(columns=[\"feature\"])\n",
    "d2 = pd.DataFrame(columns=[\"feature\"])\n",
    "d1.loc[\"key\"] = {\"feature\": np.nan}\n",
    "d2.loc[\"key\"] = {\"feature\": 1}\n",
    "d2.loc[\"key2\"] = {\"feature\": 3}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "d1.combine_first(d2)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(os.path.join(DATA_PATH, \"hpc/dev_pipeline.txt\"), index_col=0)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join(DATA_PATH, \"../hpc/candidates.txt\"), index_col=0)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "outputs": [],
   "source": [
    "# get all tokens where correct normalisation not produced by candidate generation\n",
    "filtered = df.groupby(\"raw_tok_index\").filter(lambda x: x.sum()[\"correct\"] == 0)\n",
    "ungenerated = filtered.loc[filtered[\"from_original_token\"] == 1.0][\"gold\"]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "outputs": [],
   "source": [
    "raw, _ = loadNormData(os.path.join(DATA_PATH, \"raw/dev.norm\"))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "outputs": [
    {
     "data": {
      "text/plain": "6876"
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count = 0\n",
    "for raw_tweet, norm_tweet in zip(raw, norm):\n",
    "    for raw_tok, norm_tok in zip(raw_tweet, norm_tweet):\n",
    "        if is_eligible(raw_tok):\n",
    "            count += 1\n",
    "count\n",
    "# as expected - perhaps can be used to test candidate_generation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "outputs": [
    {
     "data": {
      "text/plain": "[(('v', 'very'), 2),\n (('hapi', 'happy'), 2),\n (('witchu', 'with you'), 2),\n (('yessss', 'yes'), 1),\n (('bestie', 'best friend'), 1),\n (('niggra', 'nigger'), 1),\n (('wada', 'water'), 1),\n (('chu', 'you'), 1),\n (('nows', 'now is'), 1),\n (('nuh', 'know'), 1),\n (('ntn', 'nothing'), 1),\n (('mnl', 'my new love'), 1),\n (('za', 'that'), 1),\n (('skepta', 'sunglasses'), 1),\n (('whatdoiwear', 'what do i wear'), 1),\n (('brutha', 'brother'), 1),\n (('yah', 'you'), 1),\n (('nuff', 'enough'), 1),\n (('shizz', 'shit'), 1),\n (('nuffin', 'nothing'), 1),\n (('diss', 'this'), 1),\n (('vas', 'was'), 1),\n (('redsox', 'red sox'), 1),\n (('nem', 'they'), 1),\n (('fkn', 'fucking'), 1),\n (('sim', 'seems'), 1),\n (('hbu', 'how about you'), 1),\n (('goddamit', 'god damn it'), 1),\n (('satnite', 'saturday night'), 1),\n (('dese', 'these'), 1),\n (('summn', 'something'), 1),\n (('cums', 'comes'), 1),\n (('dnt', \"doesn't\"), 1),\n (('getcha', 'get you'), 1),\n (('trynna', 'trying to'), 1),\n (('ya', 'your'), 1),\n (('lottle', 'lot'), 1),\n (('happylgovo', 'happy'), 1),\n (('gal', 'guy'), 1),\n (('fuccin', 'fucking'), 1),\n (('cuh', 'see you'), 1),\n (('datpiff', 'the piff'), 1),\n (('diy', 'do it yourself'), 1),\n (('da', 'that'), 1),\n (('shat', 'shit'), 1),\n (('slp', 'sleep'), 1),\n (('dia', 'their'), 1),\n (('devs', 'developer'), 1),\n (('yea', 'you'), 1),\n (('cannnot', \"can't\"), 1),\n (('ull', 'you will'), 1),\n (('icant', \"i can't\"), 1),\n (('brotha', 'brother'), 1),\n (('poooollll', 'pool'), 1),\n (('sup', \"what's up\"), 1),\n (('wassup', \"what's up\"), 1),\n (('pre', 'preorder'), 1),\n (('order', nan), 1)]"
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "Counter(zip(ungenerated.index, ungenerated)).most_common()\n",
    "# very few ungenerated correct candidates! So candidate generation module perhaps alright."
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
