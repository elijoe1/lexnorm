{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from lexnorm.data import lexicon\n",
    "from lexnorm.data import norm_dict\n",
    "from lexnorm.data.normEval import loadNormData\n",
    "from lexnorm.definitions import DATA_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "outputs": [],
   "source": [
    "raw, norm = loadNormData(os.path.join(DATA_PATH, \"interim/train.txt\"))\n",
    "norm_dictionary = norm_dict.from_train(os.path.join(DATA_PATH, \"interim/train.txt\"))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "outputs": [],
   "source": [
    "lex = lexicon.build(\n",
    "        {\"english\", \"american\"},\n",
    "        {\"contractions\", \"proper-names\", \"upper\", \"words\"},\n",
    "        50,\n",
    "        1,\n",
    "    )\n",
    "\n",
    "lex = lexicon.refine(lex.union(lexicon.build_abbreviations()))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation of lexicon with normalisation: 0.43\n",
      "Most common un-normalised raw alphanumeric tokens in lexicon: [(('i', 'i'), 648), (('the', 'the'), 631), (('to', 'to'), 534), (('a', 'a'), 479), (('and', 'and'), 411), (('you', 'you'), 340), (('in', 'in'), 326), (('for', 'for'), 320), (('is', 'is'), 318), (('me', 'me'), 281), (('my', 'my'), 278), (('lol', 'lol'), 272), (('on', 'on'), 271), (('of', 'of'), 249), (('it', 'it'), 203), (('with', 'with'), 186), (('that', 'that'), 185), (('this', 'this'), 180), (('so', 'so'), 180), (('be', 'be'), 159)]\n",
      "Most common normalised raw alphanumeric tokens in lexicon: [(('nigga', 'nigger'), 57), (('niggas', 'niggers'), 52), (('ur', 'your'), 33), (('gonna', 'going to'), 29), (('bout', 'about'), 21), (('wit', 'with'), 17), (('tho', 'though'), 17), (('cause', 'because'), 16), (('wanna', 'want to'), 12), (('cant', \"can't\"), 12), (('ur', \"you're\"), 12), (('yo', 'you'), 11), (('nd', 'and'), 11), (('ill', \"i'll\"), 9), (('dis', 'this'), 9), (('yo', 'your'), 8), (('em', 'them'), 8), (('yea', 'yeah'), 7), (('its', \"it's\"), 7), (('rn', 'right now'), 6)]\n",
      "Most common un-normalised raw alphanumeric tokens not in lexicon: [(('haha', 'haha'), 81), (('bae', 'bae'), 45), (('x', 'x'), 44), (('niall', 'niall'), 40), (('2', '2'), 40), (('hahaha', 'hahaha'), 36), (('idk', 'idk'), 36), (('1', '1'), 34), (('rp', 'rp'), 33), (('exo', 'exo'), 31), (('wtf', 'wtf'), 26), (('t', 't'), 25), (('5', '5'), 25), (('zayn', 'zayn'), 23), (('2014', '2014'), 23), (('smh', 'smh'), 21), (('c', 'c'), 21), (('b', 'b'), 21), (('3', '3'), 21), (('4', '4'), 20)]\n",
      "Most common normalised raw alphanumeric tokens not in lexicon: [(('u', 'you'), 328), (('im', \"i'm\"), 181), (('dont', \"don't\"), 92), (('n', 'and'), 47), (('pls', 'please'), 43), (('lil', 'little'), 35), (('thats', \"that's\"), 33), (('bruh', 'brother'), 31), (('aint', \"ain't\"), 30), (('rt', 'retweet'), 26), (('ima', \"i'm going to\"), 25), (('ppl', 'people'), 25), (('yall', \"y'all\"), 23), (('cuz', 'because'), 22), (('r', 'are'), 22), (('d', 'the'), 22), (('didnt', \"didn't\"), 19), (('gon', 'gonna'), 18), (('tryna', 'trying to'), 18), (('imma', \"i'm going to\"), 18)]\n"
     ]
    }
   ],
   "source": [
    "lexicon.evaluate(raw, norm, lex)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "outputs": [],
   "source": [
    "def original_token(tok):\n",
    "    # MONOISE\n",
    "    # needed if detect step is skipped, as all tokens will be replaced by one from the list of candidates\n",
    "    return {tok}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "outputs": [],
   "source": [
    "def word_embeddings(tok):\n",
    "    # TODO implement. word2vec\n",
    "    # MONOISE\n",
    "    # can use twitter embeddings from van der Goot - based on distributional hypothesis to find tokens with similar semantics\n",
    "    return set()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "outputs": [],
   "source": [
    "def aspell(tok):\n",
    "    # TODO implement\n",
    "    # MONOISE\n",
    "    # uses weighted character edit distance, double metaphone alg for similar looking + sounding words\n",
    "    return set()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "outputs": [],
   "source": [
    "def lookup(tok, dictionary):\n",
    "    # TODO: external norm dicts?\n",
    "    # MONOISE\n",
    "    # lookup in list of all replacement pairs found in the training data (and external sources?)\n",
    "    # all norm tokens with raw token tok are included as candidates\n",
    "    return {v for v in dictionary.get(tok, {}).keys()}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "outputs": [],
   "source": [
    "def clipping(tok, lex):\n",
    "    # MONOISE\n",
    "    # all words in lexicon that have tok as a prefix (capturing abbreviation). May only consider for tok length above 2?\n",
    "    candidates = set()\n",
    "    if len(tok) < 2:\n",
    "        return set()\n",
    "    # TODO: length threshold? prune generated (only some degree of clipping allowed w.r.t. edit distance)?\n",
    "    return [t for t in lex if t.startswith(tok)]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "outputs": [],
   "source": [
    "# TODO: number of candidates on average generated by each module and with all modules\n",
    "# TODO: which modules contribute the most / most unique correct candidates\n",
    "# TODO: for modules and whole, percentage of correct vs incorrect candidates"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "outputs": [],
   "source": [
    "def split(tok, lex):\n",
    "    # MONOISE\n",
    "    # hypothesis splits on (every/some) position and check if both words are in lexicon. May only consider of tok length above 3?\n",
    "    candidates = set()\n",
    "    if len(tok) < 3:\n",
    "        return set()\n",
    "    for pos in range(1, len(tok)):\n",
    "        left = tok[:pos]\n",
    "        right = tok[pos:]\n",
    "        if left in lex and right in lex:\n",
    "            candidates.add(\" \".join([left, right]))\n",
    "    # TODO: recursive candidate generation on each left and right? Probably not... More than one split? Probably not either...\n",
    "    # TODO: length threshold?\n",
    "    return candidates"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "outputs": [],
   "source": [
    "# TODO: contextual features?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "outputs": [
    {
     "data": {
      "text/plain": "{'ban gin', 'bang in', 'bangin', 'banging'}"
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# def generate_candidates(tweet):\n",
    "#     for token in tweet:\n",
    "#         ...\n",
    "def generate_candidates(tok):\n",
    "    candidates = set()\n",
    "    candidates = candidates.union(original_token(tok))\n",
    "    candidates = candidates.union(word_embeddings(tok))\n",
    "    candidates = candidates.union(aspell(tok))\n",
    "    candidates = candidates.union(lookup(tok, norm_dictionary))\n",
    "    candidates = candidates.union(clipping(tok, lex))\n",
    "    candidates = candidates.union(split(tok, lex))\n",
    "    return candidates\n",
    "\n",
    "generate_candidates(\"bangin\")"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
